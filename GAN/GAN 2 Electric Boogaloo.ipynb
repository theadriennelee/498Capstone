{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4555cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72a98101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         15420\n",
      "1         15430\n",
      "2         15440\n",
      "3         15450\n",
      "4         15460\n",
      "          ...  \n",
      "19730    212720\n",
      "19731    212730\n",
      "19732    212740\n",
      "19733    212750\n",
      "19734    212760\n",
      "Name: date, Length: 19735, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('scalar.csv')\n",
    "\n",
    "\n",
    "continuous_features = ['date','Appliances', 'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3',\n",
    "       'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8',\n",
    "       'RH_8', 'T9', 'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed',\n",
    "       'Visibility', 'Tdewpoint', 'rv1', 'rv2']\n",
    "print(df['date'])\n",
    "\n",
    "# training configuration\n",
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 32\n",
    "\n",
    "log_step = 100\n",
    "epochs = 5000+1\n",
    "learning_rate = 5e-4\n",
    "models_dir = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56a4ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       date  Appliances  lights    T1  RH_1    T2  RH_2    T3  RH_3    T4  \\\n",
      "0       NaN         2.0    16.0  16.0  28.0  11.0  34.0  10.0  37.0  17.0   \n",
      "1       NaN         2.0    16.0  16.0  27.0  11.0  34.0  10.0  37.0  17.0   \n",
      "2       NaN         1.0    16.0  16.0  26.0  11.0  33.0  10.0  37.0  17.0   \n",
      "3       NaN         1.0    24.0  16.0  26.0  11.0  33.0  10.0  37.0  17.0   \n",
      "4       NaN         2.0    24.0  16.0  26.0  11.0  33.0  10.0  37.0  17.0   \n",
      "...     ...         ...     ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "19730  49.0         4.0     NaN  46.0  26.0  35.0  30.0  41.0  28.0  43.0   \n",
      "19731  49.0         3.0     NaN  45.0  26.0  35.0  30.0  41.0  29.0  43.0   \n",
      "19732  49.0        12.0     NaN  45.0  26.0  34.0  31.0  40.0  30.0  43.0   \n",
      "19733  49.0        19.0     NaN  45.0  27.0  33.0  31.0  40.0  29.0  43.0   \n",
      "19734  49.0        19.0     NaN  45.0  26.0  33.0  31.0  39.0  28.0  43.0   \n",
      "\n",
      "       ...    T9  RH_9  T_out  Press_mm_hg  RH_out  Windspeed  Visibility  \\\n",
      "0      ...  11.0  33.0   18.0          4.0    44.0       24.0        47.0   \n",
      "1      ...  11.0  33.0   18.0          5.0    44.0       23.0        44.0   \n",
      "2      ...  10.0  33.0   18.0          5.0    44.0       22.0        41.0   \n",
      "3      ...  10.0  33.0   18.0          5.0    44.0       21.0        38.0   \n",
      "4      ...  10.0  33.0   17.0          5.0    44.0       20.0        35.0   \n",
      "...    ...   ...   ...    ...          ...     ...        ...         ...   \n",
      "19730  ...  43.0  36.0   44.0         30.0    20.0       11.0        17.0   \n",
      "19731  ...  43.0  36.0   44.0         30.0    21.0       12.0        18.0   \n",
      "19732  ...  43.0  36.0   44.0         30.0    21.0       13.0        18.0   \n",
      "19733  ...  43.0  36.0   43.0         30.0    21.0       13.0        19.0   \n",
      "19734  ...  43.0  36.0   43.0         30.0    21.0       14.0        19.0   \n",
      "\n",
      "       Tdewpoint   rv1   rv2  \n",
      "0           26.0  13.0  13.0  \n",
      "1           26.0  18.0  18.0  \n",
      "2           26.0  28.0  28.0  \n",
      "3           26.0  45.0  45.0  \n",
      "4           26.0  10.0  10.0  \n",
      "...          ...   ...   ...  \n",
      "19730       45.0  43.0  43.0  \n",
      "19731       45.0  49.0  49.0  \n",
      "19732       45.0  29.0  29.0  \n",
      "19733       44.0   6.0   6.0  \n",
      "19734       44.0  34.0  34.0  \n",
      "\n",
      "[19735 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "for column in continuous_features:\n",
    "  min = df[column].min()\n",
    "  max = df[column].max()\n",
    "  feature_bins = pd.cut(df[column], bins=np.linspace(min, max, 51), labels=False)\n",
    "  df.drop([column], axis=1, inplace=True)\n",
    "  df = pd.concat([df, feature_bins], axis=1)\n",
    "print(df)\n",
    "df.to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4807d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  Appliances    lights        T1      RH_1        T2      RH_2  \\\n",
      "0           NaN   -0.066795  1.564867 -1.119740  1.731414 -0.391566  1.147930   \n",
      "1           NaN   -0.066795  1.564867 -1.119740  1.586121 -0.391566  1.147930   \n",
      "2           NaN   -1.164761  1.564867 -1.119740  1.438016 -0.391566  0.958908   \n",
      "3           NaN   -1.164761  1.700779 -1.119740  1.438016 -0.391566  0.958908   \n",
      "4           NaN   -0.066795  1.700779 -1.119740  1.438016 -0.391566  0.958908   \n",
      "...         ...         ...       ...       ...       ...       ...       ...   \n",
      "19730  1.555142    0.829964       NaN  2.370527  1.438016  2.066687  0.403217   \n",
      "19731  1.555142    0.491180       NaN  2.259487  1.438016  2.066687  0.403217   \n",
      "19732  1.555142    1.684086       NaN  2.259487  1.438016  1.989929  0.586513   \n",
      "19733  1.555142    1.877496       NaN  2.259487  1.586121  1.911803  0.586513   \n",
      "19734  1.555142    1.877496       NaN  2.259487  1.438016  1.911803  0.586513   \n",
      "\n",
      "             T3      RH_3        T4  ...        T9      RH_9     T_out  \\\n",
      "0     -1.351830  1.569980 -0.932569  ... -1.244036  0.928842 -0.092151   \n",
      "1     -1.351830  1.569980 -0.932569  ... -1.244036  0.928842 -0.092151   \n",
      "2     -1.351830  1.569980 -0.932569  ... -1.368469  0.928842 -0.092151   \n",
      "3     -1.351830  1.569980 -0.932569  ... -1.368469  0.928842 -0.092151   \n",
      "4     -1.351830  1.569980 -0.932569  ... -1.368469  0.928842 -0.213972   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "19730  2.177369  0.606319  1.842668  ...  1.717190  1.232565  2.462568   \n",
      "19731  2.177369  0.723054  1.842668  ...  1.717190  1.232565  2.462568   \n",
      "19732  2.086027  0.837067  1.842668  ...  1.717190  1.232565  2.462568   \n",
      "19733  2.086027  0.723054  1.842668  ...  1.717190  1.232565  2.378646   \n",
      "19734  1.993773  0.606319  1.842668  ...  1.717190  1.232565  2.378646   \n",
      "\n",
      "       Press_mm_hg    RH_out  Windspeed  Visibility  Tdewpoint       rv1  \\\n",
      "0        -2.541579  0.838496   1.141723    1.937918   0.356006 -0.785922   \n",
      "1        -2.479127  0.838496   1.065633    1.666396   0.356006 -0.404581   \n",
      "2        -2.479127  0.838496   0.987026    1.386995   0.356006  0.284128   \n",
      "3        -2.479127  0.838496   0.905704    1.098898   0.356006  1.319957   \n",
      "4        -2.479127  0.838496   0.821442    0.801130   0.356006 -1.032792   \n",
      "...            ...       ...        ...         ...        ...       ...   \n",
      "19730    -0.058138 -1.580845  -0.127611   -1.279790   2.170972  1.204417   \n",
      "19731    -0.058138 -1.518760   0.001102   -1.145433   2.170972  1.546981   \n",
      "19732    -0.058138 -1.518760   0.122302   -1.145433   2.170972  0.349026   \n",
      "19733    -0.058138 -1.518760   0.122302   -1.014074   2.080226 -1.393892   \n",
      "19734    -0.058138 -1.518760   0.236921   -1.014074   2.080226  0.665075   \n",
      "\n",
      "            rv2  \n",
      "0     -0.785922  \n",
      "1     -0.404581  \n",
      "2      0.284128  \n",
      "3      1.319957  \n",
      "4     -1.032792  \n",
      "...         ...  \n",
      "19730  1.204417  \n",
      "19731  1.546981  \n",
      "19732  0.349026  \n",
      "19733 -1.393892  \n",
      "19734  0.665075  \n",
      "\n",
      "[19735 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "df[df.columns] = PowerTransformer(method='yeo-johnson', standardize=True, copy=True).fit_transform(df[df.columns])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54f7ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  Appliances    lights        T1      RH_1        T2      RH_2  \\\n",
      "0           NaN   -0.066795  1.564867 -1.119740  1.731414 -0.391566  1.147930   \n",
      "1           NaN   -0.066795  1.564867 -1.119740  1.586121 -0.391566  1.147930   \n",
      "2           NaN   -1.164761  1.564867 -1.119740  1.438016 -0.391566  0.958908   \n",
      "3           NaN   -1.164761  1.700779 -1.119740  1.438016 -0.391566  0.958908   \n",
      "4           NaN   -0.066795  1.700779 -1.119740  1.438016 -0.391566  0.958908   \n",
      "...         ...         ...       ...       ...       ...       ...       ...   \n",
      "19730  1.555142    0.829964       NaN  2.370527  1.438016  2.066687  0.403217   \n",
      "19731  1.555142    0.491180       NaN  2.259487  1.438016  2.066687  0.403217   \n",
      "19732  1.555142    1.684086       NaN  2.259487  1.438016  1.989929  0.586513   \n",
      "19733  1.555142    1.877496       NaN  2.259487  1.586121  1.911803  0.586513   \n",
      "19734  1.555142    1.877496       NaN  2.259487  1.438016  1.911803  0.586513   \n",
      "\n",
      "             T3      RH_3        T4  ...        T9      RH_9     T_out  \\\n",
      "0     -1.351830  1.569980 -0.932569  ... -1.244036  0.928842 -0.092151   \n",
      "1     -1.351830  1.569980 -0.932569  ... -1.244036  0.928842 -0.092151   \n",
      "2     -1.351830  1.569980 -0.932569  ... -1.368469  0.928842 -0.092151   \n",
      "3     -1.351830  1.569980 -0.932569  ... -1.368469  0.928842 -0.092151   \n",
      "4     -1.351830  1.569980 -0.932569  ... -1.368469  0.928842 -0.213972   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "19730  2.177369  0.606319  1.842668  ...  1.717190  1.232565  2.462568   \n",
      "19731  2.177369  0.723054  1.842668  ...  1.717190  1.232565  2.462568   \n",
      "19732  2.086027  0.837067  1.842668  ...  1.717190  1.232565  2.462568   \n",
      "19733  2.086027  0.723054  1.842668  ...  1.717190  1.232565  2.378646   \n",
      "19734  1.993773  0.606319  1.842668  ...  1.717190  1.232565  2.378646   \n",
      "\n",
      "       Press_mm_hg    RH_out  Windspeed  Visibility  Tdewpoint       rv1  \\\n",
      "0        -2.541579  0.838496   1.141723    1.937918   0.356006 -0.785922   \n",
      "1        -2.479127  0.838496   1.065633    1.666396   0.356006 -0.404581   \n",
      "2        -2.479127  0.838496   0.987026    1.386995   0.356006  0.284128   \n",
      "3        -2.479127  0.838496   0.905704    1.098898   0.356006  1.319957   \n",
      "4        -2.479127  0.838496   0.821442    0.801130   0.356006 -1.032792   \n",
      "...            ...       ...        ...         ...        ...       ...   \n",
      "19730    -0.058138 -1.580845  -0.127611   -1.279790   2.170972  1.204417   \n",
      "19731    -0.058138 -1.518760   0.001102   -1.145433   2.170972  1.546981   \n",
      "19732    -0.058138 -1.518760   0.122302   -1.145433   2.170972  0.349026   \n",
      "19733    -0.058138 -1.518760   0.122302   -1.014074   2.080226 -1.393892   \n",
      "19734    -0.058138 -1.518760   0.236921   -1.014074   2.080226  0.665075   \n",
      "\n",
      "            rv2  \n",
      "0     -0.785922  \n",
      "1     -0.404581  \n",
      "2      0.284128  \n",
      "3      1.319957  \n",
      "4     -1.032792  \n",
      "...         ...  \n",
      "19730  1.204417  \n",
      "19731  1.546981  \n",
      "19732  0.349026  \n",
      "19733 -1.393892  \n",
      "19734  0.665075  \n",
      "\n",
      "[19735 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "pw= PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "pwt=pw.fit_transform(df[df.columns])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "981f415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns]=pwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abd9549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "263f09d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>1.581403</td>\n",
       "      <td>-1.123375</td>\n",
       "      <td>1.714188</td>\n",
       "      <td>-0.389650</td>\n",
       "      <td>1.146193</td>\n",
       "      <td>-1.358099</td>\n",
       "      <td>1.547744</td>\n",
       "      <td>-0.932422</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.244796</td>\n",
       "      <td>0.929872</td>\n",
       "      <td>-0.088897</td>\n",
       "      <td>-2.476743</td>\n",
       "      <td>0.830316</td>\n",
       "      <td>1.146808</td>\n",
       "      <td>1.898418</td>\n",
       "      <td>0.357172</td>\n",
       "      <td>-0.817298</td>\n",
       "      <td>-0.817298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>1.581403</td>\n",
       "      <td>-1.123375</td>\n",
       "      <td>1.572623</td>\n",
       "      <td>-0.389650</td>\n",
       "      <td>1.146193</td>\n",
       "      <td>-1.358099</td>\n",
       "      <td>1.547744</td>\n",
       "      <td>-0.932422</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.244796</td>\n",
       "      <td>0.929872</td>\n",
       "      <td>-0.088897</td>\n",
       "      <td>-2.417524</td>\n",
       "      <td>0.830316</td>\n",
       "      <td>1.068626</td>\n",
       "      <td>1.639629</td>\n",
       "      <td>0.357172</td>\n",
       "      <td>-0.469613</td>\n",
       "      <td>-0.469613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.209357</td>\n",
       "      <td>1.581403</td>\n",
       "      <td>-1.123375</td>\n",
       "      <td>1.428100</td>\n",
       "      <td>-0.389650</td>\n",
       "      <td>0.958841</td>\n",
       "      <td>-1.358099</td>\n",
       "      <td>1.547744</td>\n",
       "      <td>-0.932422</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.369596</td>\n",
       "      <td>0.929872</td>\n",
       "      <td>-0.088897</td>\n",
       "      <td>-2.417524</td>\n",
       "      <td>0.830316</td>\n",
       "      <td>0.987968</td>\n",
       "      <td>1.372022</td>\n",
       "      <td>0.357172</td>\n",
       "      <td>0.216308</td>\n",
       "      <td>0.216308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.209357</td>\n",
       "      <td>1.602236</td>\n",
       "      <td>-1.123375</td>\n",
       "      <td>1.428100</td>\n",
       "      <td>-0.389650</td>\n",
       "      <td>0.958841</td>\n",
       "      <td>-1.358099</td>\n",
       "      <td>1.547744</td>\n",
       "      <td>-0.932422</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.369596</td>\n",
       "      <td>0.929872</td>\n",
       "      <td>-0.088897</td>\n",
       "      <td>-2.417524</td>\n",
       "      <td>0.830316</td>\n",
       "      <td>0.904645</td>\n",
       "      <td>1.094525</td>\n",
       "      <td>0.357172</td>\n",
       "      <td>1.394775</td>\n",
       "      <td>1.394775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>1.602236</td>\n",
       "      <td>-1.123375</td>\n",
       "      <td>1.428100</td>\n",
       "      <td>-0.389650</td>\n",
       "      <td>0.958841</td>\n",
       "      <td>-1.358099</td>\n",
       "      <td>1.547744</td>\n",
       "      <td>-0.932422</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.369596</td>\n",
       "      <td>0.929872</td>\n",
       "      <td>-0.210884</td>\n",
       "      <td>-2.417524</td>\n",
       "      <td>0.830316</td>\n",
       "      <td>0.818447</td>\n",
       "      <td>0.805805</td>\n",
       "      <td>0.357172</td>\n",
       "      <td>-1.033851</td>\n",
       "      <td>-1.033851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>1.681446</td>\n",
       "      <td>0.856374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.333449</td>\n",
       "      <td>1.428100</td>\n",
       "      <td>2.059780</td>\n",
       "      <td>0.406662</td>\n",
       "      <td>2.154970</td>\n",
       "      <td>0.615643</td>\n",
       "      <td>1.814467</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714906</td>\n",
       "      <td>1.223366</td>\n",
       "      <td>2.448684</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-1.470051</td>\n",
       "      <td>-0.140372</td>\n",
       "      <td>-1.293132</td>\n",
       "      <td>2.166457</td>\n",
       "      <td>1.256494</td>\n",
       "      <td>1.256494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>1.681446</td>\n",
       "      <td>0.549579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.225997</td>\n",
       "      <td>1.428100</td>\n",
       "      <td>2.059780</td>\n",
       "      <td>0.406662</td>\n",
       "      <td>2.154970</td>\n",
       "      <td>0.729687</td>\n",
       "      <td>1.814467</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714906</td>\n",
       "      <td>1.223366</td>\n",
       "      <td>2.448684</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-1.422841</td>\n",
       "      <td>-0.011920</td>\n",
       "      <td>-1.153540</td>\n",
       "      <td>2.166457</td>\n",
       "      <td>1.670718</td>\n",
       "      <td>1.670718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>1.681446</td>\n",
       "      <td>1.582457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.225997</td>\n",
       "      <td>1.428100</td>\n",
       "      <td>1.983599</td>\n",
       "      <td>0.589056</td>\n",
       "      <td>2.065673</td>\n",
       "      <td>0.840730</td>\n",
       "      <td>1.814467</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714906</td>\n",
       "      <td>1.223366</td>\n",
       "      <td>2.448684</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-1.422841</td>\n",
       "      <td>0.109562</td>\n",
       "      <td>-1.153540</td>\n",
       "      <td>2.166457</td>\n",
       "      <td>0.285540</td>\n",
       "      <td>0.285540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19733</th>\n",
       "      <td>1.681446</td>\n",
       "      <td>1.739605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.225997</td>\n",
       "      <td>1.572623</td>\n",
       "      <td>1.906046</td>\n",
       "      <td>0.589056</td>\n",
       "      <td>2.065673</td>\n",
       "      <td>0.729687</td>\n",
       "      <td>1.814467</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714906</td>\n",
       "      <td>1.223366</td>\n",
       "      <td>2.365725</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-1.422841</td>\n",
       "      <td>0.109562</td>\n",
       "      <td>-1.017463</td>\n",
       "      <td>2.076120</td>\n",
       "      <td>-1.340850</td>\n",
       "      <td>-1.340850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>1.681446</td>\n",
       "      <td>1.739605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.225997</td>\n",
       "      <td>1.428100</td>\n",
       "      <td>1.906046</td>\n",
       "      <td>0.589056</td>\n",
       "      <td>1.975431</td>\n",
       "      <td>0.615643</td>\n",
       "      <td>1.814467</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714906</td>\n",
       "      <td>1.223366</td>\n",
       "      <td>2.365725</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-1.422841</td>\n",
       "      <td>0.224887</td>\n",
       "      <td>-1.017463</td>\n",
       "      <td>2.076120</td>\n",
       "      <td>0.632417</td>\n",
       "      <td>0.632417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  Appliances    lights        T1      RH_1        T2      RH_2  \\\n",
       "0      0.000000    0.008865  1.581403 -1.123375  1.714188 -0.389650  1.146193   \n",
       "1      0.000000    0.008865  1.581403 -1.123375  1.572623 -0.389650  1.146193   \n",
       "2      0.000000   -1.209357  1.581403 -1.123375  1.428100 -0.389650  0.958841   \n",
       "3      0.000000   -1.209357  1.602236 -1.123375  1.428100 -0.389650  0.958841   \n",
       "4      0.000000    0.008865  1.602236 -1.123375  1.428100 -0.389650  0.958841   \n",
       "...         ...         ...       ...       ...       ...       ...       ...   \n",
       "19730  1.681446    0.856374  0.000000  2.333449  1.428100  2.059780  0.406662   \n",
       "19731  1.681446    0.549579  0.000000  2.225997  1.428100  2.059780  0.406662   \n",
       "19732  1.681446    1.582457  0.000000  2.225997  1.428100  1.983599  0.589056   \n",
       "19733  1.681446    1.739605  0.000000  2.225997  1.572623  1.906046  0.589056   \n",
       "19734  1.681446    1.739605  0.000000  2.225997  1.428100  1.906046  0.589056   \n",
       "\n",
       "             T3      RH_3        T4  ...        T9      RH_9     T_out  \\\n",
       "0     -1.358099  1.547744 -0.932422  ... -1.244796  0.929872 -0.088897   \n",
       "1     -1.358099  1.547744 -0.932422  ... -1.244796  0.929872 -0.088897   \n",
       "2     -1.358099  1.547744 -0.932422  ... -1.369596  0.929872 -0.088897   \n",
       "3     -1.358099  1.547744 -0.932422  ... -1.369596  0.929872 -0.088897   \n",
       "4     -1.358099  1.547744 -0.932422  ... -1.369596  0.929872 -0.210884   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "19730  2.154970  0.615643  1.814467  ...  1.714906  1.223366  2.448684   \n",
       "19731  2.154970  0.729687  1.814467  ...  1.714906  1.223366  2.448684   \n",
       "19732  2.065673  0.840730  1.814467  ...  1.714906  1.223366  2.448684   \n",
       "19733  2.065673  0.729687  1.814467  ...  1.714906  1.223366  2.365725   \n",
       "19734  1.975431  0.615643  1.814467  ...  1.714906  1.223366  2.365725   \n",
       "\n",
       "       Press_mm_hg    RH_out  Windspeed  Visibility  Tdewpoint       rv1  \\\n",
       "0        -2.476743  0.830316   1.146808    1.898418   0.357172 -0.817298   \n",
       "1        -2.417524  0.830316   1.068626    1.639629   0.357172 -0.469613   \n",
       "2        -2.417524  0.830316   0.987968    1.372022   0.357172  0.216308   \n",
       "3        -2.417524  0.830316   0.904645    1.094525   0.357172  1.394775   \n",
       "4        -2.417524  0.830316   0.818447    0.805805   0.357172 -1.033851   \n",
       "...            ...       ...        ...         ...        ...       ...   \n",
       "19730    -0.073269 -1.470051  -0.140372   -1.293132   2.166457  1.256494   \n",
       "19731    -0.073269 -1.422841  -0.011920   -1.153540   2.166457  1.670718   \n",
       "19732    -0.073269 -1.422841   0.109562   -1.153540   2.166457  0.285540   \n",
       "19733    -0.073269 -1.422841   0.109562   -1.017463   2.076120 -1.340850   \n",
       "19734    -0.073269 -1.422841   0.224887   -1.017463   2.076120  0.632417   \n",
       "\n",
       "            rv2  \n",
       "0     -0.817298  \n",
       "1     -0.469613  \n",
       "2      0.216308  \n",
       "3      1.394775  \n",
       "4     -1.033851  \n",
       "...         ...  \n",
       "19730  1.256494  \n",
       "19731  1.670718  \n",
       "19732  0.285540  \n",
       "19733 -1.340850  \n",
       "19734  0.632417  \n",
       "\n",
       "[19735 rows x 29 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bac668fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class GAN():\n",
    "    \n",
    "    def __init__(self, gan_args):\n",
    "        [self.batch_size, lr, self.noise_dim,\n",
    "         self.data_dim, layers_dim] = gan_args\n",
    "\n",
    "        self.generator = Generator(self.batch_size).\\\n",
    "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
    "\n",
    "        self.discriminator = Discriminator(self.batch_size).\\\n",
    "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
    "\n",
    "        optimizer = Adam(lr, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.noise_dim,))\n",
    "        record = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(record)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def get_data_batch(self, train, batch_size, seed=0):\n",
    "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
    "        # np.random.seed(seed)\n",
    "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
    "        # iterate through shuffled indices, so every sample gets covered evenly\n",
    "\n",
    "        start_i = (batch_size * seed) % len(train)\n",
    "        stop_i = start_i + batch_size\n",
    "        shuffle_seed = (batch_size * seed) // len(train)\n",
    "        np.random.seed(shuffle_seed)\n",
    "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
    "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
    "        x = train.loc[train_ix[start_i: stop_i]].values\n",
    "        return np.reshape(x, (batch_size, -1))\n",
    "        \n",
    "    def train(self, data, train_arguments):\n",
    "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
    "        \n",
    "        data_cols = data.columns\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((self.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):    \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            batch_data = self.get_data_batch(data, self.batch_size)\n",
    "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_data = self.generator.predict(noise)\n",
    "    \n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "    \n",
    "            # Plot the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "    \n",
    "            # If at save interval => save generated events\n",
    "            if epoch % sample_interval == 0:\n",
    "                #Test here data generation step\n",
    "                # save model checkpoints\n",
    "                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.h5'\n",
    "                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))\n",
    "                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))\n",
    "\n",
    "                #Here is generating the data\n",
    "                z = tf.random.normal((432, self.noise_dim))\n",
    "                gen_data = self.generator(z)\n",
    "                print('generated_data')\n",
    "\n",
    "    def save(self, path, name):\n",
    "        assert os.path.isdir(path) == True, \\\n",
    "            \"Please provide a valid path. Path must be a directory.\"\n",
    "        model_path = os.path.join(path, name)\n",
    "        self.generator.save_weights(model_path)  # Load the generator\n",
    "        return\n",
    "    \n",
    "    def load(self, path):\n",
    "        assert os.path.isdir(path) == True, \\\n",
    "            \"Please provide a valid path. Path must be a directory.\"\n",
    "        self.generator = Generator(self.batch_size)\n",
    "        self.generator = self.generator.load_weights(path)\n",
    "        return self.generator\n",
    "    \n",
    "class Generator():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def build_model(self, input_shape, dim, data_dim):\n",
    "        input= Input(shape=input_shape, batch_size=self.batch_size)\n",
    "        x = Dense(dim, activation='relu')(input)\n",
    "        x = Dense(dim * 2, activation='relu')(x)\n",
    "        x = Dense(dim * 4, activation='relu')(x)\n",
    "        x = Dense(data_dim)(x)\n",
    "        return Model(inputs=input, outputs=x)\n",
    "\n",
    "class Discriminator():\n",
    "    def __init__(self,batch_size):\n",
    "        self.batch_size=batch_size\n",
    "    \n",
    "    def build_model(self, input_shape, dim):\n",
    "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
    "        x = Dense(dim * 4, activation='relu')(input)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(dim * 2, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(dim, activation='relu')(x)\n",
    "        x = Dense(1, activation='sigmoid')(x)\n",
    "        return Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87160647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "data_cols = df.columns\n",
    "#Define the GAN and training parameters\n",
    "df[data_cols] = df[data_cols]\n",
    "\n",
    "print(df.shape[1])\n",
    "\n",
    "gan_args = [batch_size, learning_rate, noise_dim, df.shape[1], dim]\n",
    "train_args = ['', epochs, log_step]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "152848ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.782627, acc.: 7.81%] [G loss: 0.683976]\n",
      "generated_data\n",
      "1 [D loss: 0.682558, acc.: 42.19%] [G loss: 0.648666]\n",
      "2 [D loss: 0.642441, acc.: 50.00%] [G loss: 0.612257]\n",
      "3 [D loss: 0.624083, acc.: 50.00%] [G loss: 0.589604]\n",
      "4 [D loss: 0.611463, acc.: 50.00%] [G loss: 0.613978]\n",
      "5 [D loss: 0.559198, acc.: 50.00%] [G loss: 0.659792]\n",
      "6 [D loss: 0.535366, acc.: 50.00%] [G loss: 0.733170]\n",
      "7 [D loss: 0.517634, acc.: 53.12%] [G loss: 0.852032]\n",
      "8 [D loss: 0.456951, acc.: 79.69%] [G loss: 1.018465]\n",
      "9 [D loss: 0.416088, acc.: 95.31%] [G loss: 1.137799]\n",
      "10 [D loss: 0.410095, acc.: 95.31%] [G loss: 1.168597]\n",
      "11 [D loss: 0.440834, acc.: 92.19%] [G loss: 1.079895]\n",
      "12 [D loss: 0.496026, acc.: 60.94%] [G loss: 0.974061]\n",
      "13 [D loss: 0.591724, acc.: 50.00%] [G loss: 0.890705]\n",
      "14 [D loss: 0.502854, acc.: 54.69%] [G loss: 0.924970]\n",
      "15 [D loss: 0.452692, acc.: 81.25%] [G loss: 1.166035]\n",
      "16 [D loss: 0.378790, acc.: 95.31%] [G loss: 1.284756]\n",
      "17 [D loss: 0.378399, acc.: 98.44%] [G loss: 1.330337]\n",
      "18 [D loss: 0.372119, acc.: 95.31%] [G loss: 1.170037]\n",
      "19 [D loss: 0.459105, acc.: 67.19%] [G loss: 1.029644]\n",
      "20 [D loss: 0.443336, acc.: 70.31%] [G loss: 0.941266]\n",
      "21 [D loss: 0.425591, acc.: 81.25%] [G loss: 0.992179]\n",
      "22 [D loss: 0.379998, acc.: 96.88%] [G loss: 1.005580]\n",
      "23 [D loss: 0.355727, acc.: 98.44%] [G loss: 1.141134]\n",
      "24 [D loss: 0.342554, acc.: 100.00%] [G loss: 1.175371]\n",
      "25 [D loss: 0.313618, acc.: 100.00%] [G loss: 1.198513]\n",
      "26 [D loss: 0.304606, acc.: 100.00%] [G loss: 1.230455]\n",
      "27 [D loss: 0.291617, acc.: 100.00%] [G loss: 1.248742]\n",
      "28 [D loss: 0.307963, acc.: 100.00%] [G loss: 1.226658]\n",
      "29 [D loss: 0.305712, acc.: 98.44%] [G loss: 1.179584]\n",
      "30 [D loss: 0.329923, acc.: 98.44%] [G loss: 1.204026]\n",
      "31 [D loss: 0.310017, acc.: 98.44%] [G loss: 1.240290]\n",
      "32 [D loss: 0.305017, acc.: 95.31%] [G loss: 1.252883]\n",
      "33 [D loss: 0.265017, acc.: 100.00%] [G loss: 1.333032]\n",
      "34 [D loss: 0.261746, acc.: 100.00%] [G loss: 1.333950]\n",
      "35 [D loss: 0.237119, acc.: 100.00%] [G loss: 1.512152]\n",
      "36 [D loss: 0.225797, acc.: 98.44%] [G loss: 1.565536]\n",
      "37 [D loss: 0.239731, acc.: 98.44%] [G loss: 1.486755]\n",
      "38 [D loss: 0.265392, acc.: 98.44%] [G loss: 1.362073]\n",
      "39 [D loss: 0.271572, acc.: 93.75%] [G loss: 1.442582]\n",
      "40 [D loss: 0.249188, acc.: 98.44%] [G loss: 1.599611]\n",
      "41 [D loss: 0.221921, acc.: 98.44%] [G loss: 1.709703]\n",
      "42 [D loss: 0.180955, acc.: 98.44%] [G loss: 1.813948]\n",
      "43 [D loss: 0.174564, acc.: 100.00%] [G loss: 1.888272]\n",
      "44 [D loss: 0.141517, acc.: 100.00%] [G loss: 1.931511]\n",
      "45 [D loss: 0.150174, acc.: 100.00%] [G loss: 2.037494]\n",
      "46 [D loss: 0.156552, acc.: 100.00%] [G loss: 2.026386]\n",
      "47 [D loss: 0.137843, acc.: 100.00%] [G loss: 2.125511]\n",
      "48 [D loss: 0.146774, acc.: 98.44%] [G loss: 2.067772]\n",
      "49 [D loss: 0.149663, acc.: 100.00%] [G loss: 1.970754]\n",
      "50 [D loss: 0.186822, acc.: 98.44%] [G loss: 2.530266]\n",
      "51 [D loss: 0.166431, acc.: 98.44%] [G loss: 2.720160]\n",
      "52 [D loss: 0.241118, acc.: 89.06%] [G loss: 2.875998]\n",
      "53 [D loss: 0.216845, acc.: 93.75%] [G loss: 2.663271]\n",
      "54 [D loss: 0.189766, acc.: 95.31%] [G loss: 2.599510]\n",
      "55 [D loss: 0.278553, acc.: 89.06%] [G loss: 2.908650]\n",
      "56 [D loss: 0.209195, acc.: 95.31%] [G loss: 3.009418]\n",
      "57 [D loss: 0.210598, acc.: 95.31%] [G loss: 2.625205]\n",
      "58 [D loss: 0.117335, acc.: 100.00%] [G loss: 2.935279]\n",
      "59 [D loss: 0.115877, acc.: 100.00%] [G loss: 3.192712]\n",
      "60 [D loss: 0.108316, acc.: 98.44%] [G loss: 3.002715]\n",
      "61 [D loss: 0.104683, acc.: 98.44%] [G loss: 2.690269]\n",
      "62 [D loss: 0.163221, acc.: 95.31%] [G loss: 3.213406]\n",
      "63 [D loss: 0.160705, acc.: 96.88%] [G loss: 3.015035]\n",
      "64 [D loss: 0.112127, acc.: 98.44%] [G loss: 3.133031]\n",
      "65 [D loss: 0.059681, acc.: 98.44%] [G loss: 3.399454]\n",
      "66 [D loss: 0.151955, acc.: 95.31%] [G loss: 3.498234]\n",
      "67 [D loss: 0.139969, acc.: 96.88%] [G loss: 4.073886]\n",
      "68 [D loss: 0.294045, acc.: 87.50%] [G loss: 3.179238]\n",
      "69 [D loss: 0.113178, acc.: 96.88%] [G loss: 4.036741]\n",
      "70 [D loss: 0.353079, acc.: 90.62%] [G loss: 3.034122]\n",
      "71 [D loss: 0.129888, acc.: 96.88%] [G loss: 3.208808]\n",
      "72 [D loss: 0.159413, acc.: 95.31%] [G loss: 2.994703]\n",
      "73 [D loss: 0.212909, acc.: 93.75%] [G loss: 4.400342]\n",
      "74 [D loss: 0.305627, acc.: 87.50%] [G loss: 3.671338]\n",
      "75 [D loss: 0.263756, acc.: 90.62%] [G loss: 4.917507]\n",
      "76 [D loss: 0.169013, acc.: 92.19%] [G loss: 3.940166]\n",
      "77 [D loss: 0.138549, acc.: 95.31%] [G loss: 3.194947]\n",
      "78 [D loss: 0.062858, acc.: 100.00%] [G loss: 3.580416]\n",
      "79 [D loss: 0.079129, acc.: 96.88%] [G loss: 3.971045]\n",
      "80 [D loss: 0.316321, acc.: 89.06%] [G loss: 4.622603]\n",
      "81 [D loss: 0.168384, acc.: 92.19%] [G loss: 4.556847]\n",
      "82 [D loss: 0.096179, acc.: 96.88%] [G loss: 3.688045]\n",
      "83 [D loss: 0.167395, acc.: 95.31%] [G loss: 3.861556]\n",
      "84 [D loss: 0.203092, acc.: 95.31%] [G loss: 3.750058]\n",
      "85 [D loss: 0.332771, acc.: 87.50%] [G loss: 3.455705]\n",
      "86 [D loss: 0.224068, acc.: 89.06%] [G loss: 3.335207]\n",
      "87 [D loss: 0.334689, acc.: 90.62%] [G loss: 3.819641]\n",
      "88 [D loss: 0.418537, acc.: 78.12%] [G loss: 2.460874]\n",
      "89 [D loss: 0.368208, acc.: 85.94%] [G loss: 3.974252]\n",
      "90 [D loss: 0.260698, acc.: 85.94%] [G loss: 4.123175]\n",
      "91 [D loss: 0.490540, acc.: 81.25%] [G loss: 3.887681]\n",
      "92 [D loss: 0.166068, acc.: 95.31%] [G loss: 4.458278]\n",
      "93 [D loss: 0.250526, acc.: 92.19%] [G loss: 4.861362]\n",
      "94 [D loss: 0.136191, acc.: 95.31%] [G loss: 4.029889]\n",
      "95 [D loss: 0.283954, acc.: 93.75%] [G loss: 4.428641]\n",
      "96 [D loss: 0.183393, acc.: 90.62%] [G loss: 5.066984]\n",
      "97 [D loss: 0.261604, acc.: 87.50%] [G loss: 4.027922]\n",
      "98 [D loss: 0.199613, acc.: 96.88%] [G loss: 4.731696]\n",
      "99 [D loss: 0.182611, acc.: 93.75%] [G loss: 3.789324]\n",
      "100 [D loss: 0.691083, acc.: 70.31%] [G loss: 5.447542]\n",
      "generated_data\n",
      "101 [D loss: 0.329328, acc.: 87.50%] [G loss: 3.961226]\n",
      "102 [D loss: 0.289620, acc.: 89.06%] [G loss: 4.259029]\n",
      "103 [D loss: 0.200469, acc.: 92.19%] [G loss: 4.219857]\n",
      "104 [D loss: 0.284671, acc.: 90.62%] [G loss: 4.646620]\n",
      "105 [D loss: 0.107892, acc.: 96.88%] [G loss: 5.136468]\n",
      "106 [D loss: 0.135888, acc.: 96.88%] [G loss: 3.580702]\n",
      "107 [D loss: 0.096725, acc.: 98.44%] [G loss: 3.493187]\n",
      "108 [D loss: 0.158665, acc.: 95.31%] [G loss: 3.019279]\n",
      "109 [D loss: 0.192998, acc.: 92.19%] [G loss: 5.103543]\n",
      "110 [D loss: 0.105001, acc.: 96.88%] [G loss: 4.845075]\n",
      "111 [D loss: 0.225519, acc.: 93.75%] [G loss: 3.981469]\n",
      "112 [D loss: 0.285721, acc.: 90.62%] [G loss: 4.265243]\n",
      "113 [D loss: 0.200779, acc.: 92.19%] [G loss: 4.990293]\n",
      "114 [D loss: 0.233259, acc.: 95.31%] [G loss: 4.130961]\n",
      "115 [D loss: 0.327057, acc.: 85.94%] [G loss: 4.191227]\n",
      "116 [D loss: 0.261105, acc.: 85.94%] [G loss: 3.666152]\n",
      "117 [D loss: 0.252058, acc.: 90.62%] [G loss: 3.027552]\n",
      "118 [D loss: 0.927528, acc.: 64.06%] [G loss: 5.343751]\n",
      "119 [D loss: 0.384557, acc.: 84.38%] [G loss: 4.649994]\n",
      "120 [D loss: 0.340615, acc.: 81.25%] [G loss: 3.381157]\n",
      "121 [D loss: 0.469548, acc.: 81.25%] [G loss: 4.975539]\n",
      "122 [D loss: 0.360508, acc.: 82.81%] [G loss: 4.530175]\n",
      "123 [D loss: 0.500497, acc.: 75.00%] [G loss: 3.957196]\n",
      "124 [D loss: 0.280239, acc.: 89.06%] [G loss: 4.737108]\n",
      "125 [D loss: 0.340625, acc.: 87.50%] [G loss: 3.998900]\n",
      "126 [D loss: 0.380088, acc.: 84.38%] [G loss: 4.867050]\n",
      "127 [D loss: 0.238695, acc.: 92.19%] [G loss: 4.333497]\n",
      "128 [D loss: 0.336237, acc.: 84.38%] [G loss: 5.154841]\n",
      "129 [D loss: 0.270654, acc.: 92.19%] [G loss: 4.105267]\n",
      "130 [D loss: 0.343000, acc.: 87.50%] [G loss: 4.042036]\n",
      "131 [D loss: 0.239634, acc.: 93.75%] [G loss: 4.085310]\n",
      "132 [D loss: 0.246697, acc.: 90.62%] [G loss: 3.507121]\n",
      "133 [D loss: 0.286757, acc.: 89.06%] [G loss: 3.826839]\n",
      "134 [D loss: 0.284154, acc.: 90.62%] [G loss: 3.802049]\n",
      "135 [D loss: 0.327956, acc.: 84.38%] [G loss: 4.177404]\n",
      "136 [D loss: 0.312091, acc.: 85.94%] [G loss: 4.531411]\n",
      "137 [D loss: 0.276994, acc.: 90.62%] [G loss: 3.550530]\n",
      "138 [D loss: 0.319361, acc.: 87.50%] [G loss: 4.058298]\n",
      "139 [D loss: 0.210951, acc.: 92.19%] [G loss: 3.647254]\n",
      "140 [D loss: 0.386417, acc.: 82.81%] [G loss: 3.824712]\n",
      "141 [D loss: 0.159042, acc.: 96.88%] [G loss: 3.318295]\n",
      "142 [D loss: 0.361725, acc.: 87.50%] [G loss: 3.269271]\n",
      "143 [D loss: 0.277466, acc.: 90.62%] [G loss: 3.314066]\n",
      "144 [D loss: 0.224750, acc.: 92.19%] [G loss: 2.844317]\n",
      "145 [D loss: 0.415804, acc.: 82.81%] [G loss: 2.442713]\n",
      "146 [D loss: 0.259276, acc.: 90.62%] [G loss: 2.781200]\n",
      "147 [D loss: 0.385871, acc.: 82.81%] [G loss: 2.931540]\n",
      "148 [D loss: 0.232420, acc.: 93.75%] [G loss: 2.654241]\n",
      "149 [D loss: 0.300740, acc.: 87.50%] [G loss: 2.710625]\n",
      "150 [D loss: 0.328444, acc.: 82.81%] [G loss: 3.851418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 [D loss: 0.272441, acc.: 90.62%] [G loss: 4.143234]\n",
      "152 [D loss: 0.154560, acc.: 93.75%] [G loss: 3.693939]\n",
      "153 [D loss: 0.251131, acc.: 89.06%] [G loss: 4.026524]\n",
      "154 [D loss: 0.383162, acc.: 81.25%] [G loss: 4.170980]\n",
      "155 [D loss: 0.272463, acc.: 87.50%] [G loss: 4.435507]\n",
      "156 [D loss: 0.269395, acc.: 89.06%] [G loss: 3.237028]\n",
      "157 [D loss: 0.273821, acc.: 89.06%] [G loss: 3.358710]\n",
      "158 [D loss: 0.164456, acc.: 96.88%] [G loss: 3.529437]\n",
      "159 [D loss: 0.301233, acc.: 85.94%] [G loss: 3.996846]\n",
      "160 [D loss: 0.337905, acc.: 87.50%] [G loss: 3.536718]\n",
      "161 [D loss: 0.344974, acc.: 89.06%] [G loss: 3.620883]\n",
      "162 [D loss: 0.258239, acc.: 90.62%] [G loss: 3.054293]\n",
      "163 [D loss: 0.401326, acc.: 82.81%] [G loss: 3.917975]\n",
      "164 [D loss: 0.272904, acc.: 93.75%] [G loss: 4.047588]\n",
      "165 [D loss: 0.352477, acc.: 89.06%] [G loss: 2.904821]\n",
      "166 [D loss: 0.302592, acc.: 89.06%] [G loss: 3.038870]\n",
      "167 [D loss: 0.336035, acc.: 90.62%] [G loss: 3.304512]\n",
      "168 [D loss: 0.277600, acc.: 90.62%] [G loss: 2.963062]\n",
      "169 [D loss: 0.288272, acc.: 90.62%] [G loss: 3.097793]\n",
      "170 [D loss: 0.204118, acc.: 95.31%] [G loss: 3.435294]\n",
      "171 [D loss: 0.259259, acc.: 93.75%] [G loss: 3.249006]\n",
      "172 [D loss: 0.150683, acc.: 95.31%] [G loss: 3.165815]\n",
      "173 [D loss: 0.252650, acc.: 93.75%] [G loss: 3.147449]\n",
      "174 [D loss: 0.164497, acc.: 96.88%] [G loss: 3.652208]\n",
      "175 [D loss: 0.169301, acc.: 93.75%] [G loss: 3.342919]\n",
      "176 [D loss: 0.133999, acc.: 96.88%] [G loss: 3.110347]\n",
      "177 [D loss: 0.217667, acc.: 93.75%] [G loss: 4.099763]\n",
      "178 [D loss: 0.126872, acc.: 95.31%] [G loss: 4.323139]\n",
      "179 [D loss: 0.139360, acc.: 95.31%] [G loss: 3.541589]\n",
      "180 [D loss: 0.173919, acc.: 93.75%] [G loss: 3.918143]\n",
      "181 [D loss: 0.073563, acc.: 98.44%] [G loss: 3.562702]\n",
      "182 [D loss: 0.359255, acc.: 79.69%] [G loss: 4.162469]\n",
      "183 [D loss: 0.190998, acc.: 90.62%] [G loss: 4.136645]\n",
      "184 [D loss: 0.346616, acc.: 87.50%] [G loss: 4.855065]\n",
      "185 [D loss: 0.262907, acc.: 93.75%] [G loss: 4.701830]\n",
      "186 [D loss: 0.581458, acc.: 75.00%] [G loss: 5.243225]\n",
      "187 [D loss: 0.271549, acc.: 93.75%] [G loss: 4.170513]\n",
      "188 [D loss: 0.506518, acc.: 84.38%] [G loss: 4.090857]\n",
      "189 [D loss: 0.412653, acc.: 90.62%] [G loss: 3.421848]\n",
      "190 [D loss: 0.476265, acc.: 89.06%] [G loss: 3.187921]\n",
      "191 [D loss: 0.422151, acc.: 79.69%] [G loss: 3.940000]\n",
      "192 [D loss: 0.238163, acc.: 90.62%] [G loss: 3.823667]\n",
      "193 [D loss: 0.327517, acc.: 87.50%] [G loss: 3.679770]\n",
      "194 [D loss: 0.160892, acc.: 90.62%] [G loss: 4.150800]\n",
      "195 [D loss: 0.235221, acc.: 90.62%] [G loss: 3.523211]\n",
      "196 [D loss: 0.237882, acc.: 87.50%] [G loss: 3.886426]\n",
      "197 [D loss: 0.318621, acc.: 85.94%] [G loss: 4.275402]\n",
      "198 [D loss: 0.205977, acc.: 93.75%] [G loss: 3.596193]\n",
      "199 [D loss: 0.393473, acc.: 79.69%] [G loss: 4.836019]\n",
      "200 [D loss: 0.254315, acc.: 87.50%] [G loss: 4.179194]\n",
      "generated_data\n",
      "201 [D loss: 0.297410, acc.: 89.06%] [G loss: 3.301563]\n",
      "202 [D loss: 0.297863, acc.: 89.06%] [G loss: 4.128832]\n",
      "203 [D loss: 0.234548, acc.: 92.19%] [G loss: 3.321857]\n",
      "204 [D loss: 0.367642, acc.: 82.81%] [G loss: 4.730919]\n",
      "205 [D loss: 0.364819, acc.: 85.94%] [G loss: 3.429778]\n",
      "206 [D loss: 0.441298, acc.: 84.38%] [G loss: 4.350509]\n",
      "207 [D loss: 0.161658, acc.: 95.31%] [G loss: 3.974277]\n",
      "208 [D loss: 0.195287, acc.: 93.75%] [G loss: 2.663232]\n",
      "209 [D loss: 0.148803, acc.: 98.44%] [G loss: 2.969371]\n",
      "210 [D loss: 0.138240, acc.: 96.88%] [G loss: 2.915668]\n",
      "211 [D loss: 0.286522, acc.: 89.06%] [G loss: 3.554115]\n",
      "212 [D loss: 0.167137, acc.: 92.19%] [G loss: 3.889108]\n",
      "213 [D loss: 0.112571, acc.: 95.31%] [G loss: 3.084819]\n",
      "214 [D loss: 0.188933, acc.: 95.31%] [G loss: 3.655209]\n",
      "215 [D loss: 0.103001, acc.: 96.88%] [G loss: 3.744015]\n",
      "216 [D loss: 0.108170, acc.: 98.44%] [G loss: 3.238974]\n",
      "217 [D loss: 0.114144, acc.: 100.00%] [G loss: 3.211077]\n",
      "218 [D loss: 0.146098, acc.: 95.31%] [G loss: 3.322271]\n",
      "219 [D loss: 0.126406, acc.: 95.31%] [G loss: 3.214097]\n",
      "220 [D loss: 0.205091, acc.: 92.19%] [G loss: 4.185017]\n",
      "221 [D loss: 0.195359, acc.: 93.75%] [G loss: 3.978013]\n",
      "222 [D loss: 0.213131, acc.: 93.75%] [G loss: 3.423908]\n",
      "223 [D loss: 0.303355, acc.: 89.06%] [G loss: 3.979762]\n",
      "224 [D loss: 0.253527, acc.: 90.62%] [G loss: 4.136075]\n",
      "225 [D loss: 0.315971, acc.: 87.50%] [G loss: 2.990131]\n",
      "226 [D loss: 0.439025, acc.: 82.81%] [G loss: 4.198596]\n",
      "227 [D loss: 0.261763, acc.: 92.19%] [G loss: 4.333738]\n",
      "228 [D loss: 0.304951, acc.: 92.19%] [G loss: 3.324441]\n",
      "229 [D loss: 0.298887, acc.: 89.06%] [G loss: 3.934947]\n",
      "230 [D loss: 0.208147, acc.: 93.75%] [G loss: 4.708575]\n",
      "231 [D loss: 0.147325, acc.: 93.75%] [G loss: 4.021516]\n",
      "232 [D loss: 0.419677, acc.: 81.25%] [G loss: 5.238018]\n",
      "233 [D loss: 0.101445, acc.: 96.88%] [G loss: 5.632467]\n",
      "234 [D loss: 0.183319, acc.: 95.31%] [G loss: 3.378890]\n",
      "235 [D loss: 0.579825, acc.: 70.31%] [G loss: 6.101962]\n",
      "236 [D loss: 0.261109, acc.: 90.62%] [G loss: 6.121570]\n",
      "237 [D loss: 0.443115, acc.: 81.25%] [G loss: 4.644817]\n",
      "238 [D loss: 0.245247, acc.: 90.62%] [G loss: 4.273474]\n",
      "239 [D loss: 0.193823, acc.: 92.19%] [G loss: 4.664770]\n",
      "240 [D loss: 0.182452, acc.: 95.31%] [G loss: 3.680864]\n",
      "241 [D loss: 0.281120, acc.: 92.19%] [G loss: 4.551678]\n",
      "242 [D loss: 0.171927, acc.: 92.19%] [G loss: 3.637537]\n",
      "243 [D loss: 0.358582, acc.: 84.38%] [G loss: 5.736290]\n",
      "244 [D loss: 0.183609, acc.: 93.75%] [G loss: 4.506612]\n",
      "245 [D loss: 0.201247, acc.: 93.75%] [G loss: 3.192658]\n",
      "246 [D loss: 0.315986, acc.: 85.94%] [G loss: 4.246764]\n",
      "247 [D loss: 0.144953, acc.: 93.75%] [G loss: 3.731310]\n",
      "248 [D loss: 0.321155, acc.: 84.38%] [G loss: 3.651653]\n",
      "249 [D loss: 0.160483, acc.: 93.75%] [G loss: 3.940696]\n",
      "250 [D loss: 0.250034, acc.: 90.62%] [G loss: 3.601467]\n",
      "251 [D loss: 0.192617, acc.: 92.19%] [G loss: 3.880714]\n",
      "252 [D loss: 0.194335, acc.: 92.19%] [G loss: 4.415615]\n",
      "253 [D loss: 0.252575, acc.: 85.94%] [G loss: 4.266306]\n",
      "254 [D loss: 0.179917, acc.: 92.19%] [G loss: 3.998813]\n",
      "255 [D loss: 0.276600, acc.: 90.62%] [G loss: 3.796538]\n",
      "256 [D loss: 0.175403, acc.: 92.19%] [G loss: 4.092899]\n",
      "257 [D loss: 0.300956, acc.: 90.62%] [G loss: 3.660563]\n",
      "258 [D loss: 0.249131, acc.: 90.62%] [G loss: 3.077083]\n",
      "259 [D loss: 0.341164, acc.: 81.25%] [G loss: 2.996140]\n",
      "260 [D loss: 0.362837, acc.: 82.81%] [G loss: 3.321035]\n",
      "261 [D loss: 0.244432, acc.: 92.19%] [G loss: 3.032001]\n",
      "262 [D loss: 0.462760, acc.: 75.00%] [G loss: 5.172978]\n",
      "263 [D loss: 0.290905, acc.: 87.50%] [G loss: 4.539233]\n",
      "264 [D loss: 0.212394, acc.: 92.19%] [G loss: 3.557970]\n",
      "265 [D loss: 0.156152, acc.: 95.31%] [G loss: 3.234337]\n",
      "266 [D loss: 0.113885, acc.: 98.44%] [G loss: 3.528109]\n",
      "267 [D loss: 0.196393, acc.: 93.75%] [G loss: 3.912761]\n",
      "268 [D loss: 0.161167, acc.: 93.75%] [G loss: 4.148304]\n",
      "269 [D loss: 0.149000, acc.: 95.31%] [G loss: 3.432480]\n",
      "270 [D loss: 0.113755, acc.: 96.88%] [G loss: 3.764796]\n",
      "271 [D loss: 0.111205, acc.: 96.88%] [G loss: 3.381260]\n",
      "272 [D loss: 0.146256, acc.: 98.44%] [G loss: 4.107906]\n",
      "273 [D loss: 0.101196, acc.: 96.88%] [G loss: 3.696458]\n",
      "274 [D loss: 0.147980, acc.: 93.75%] [G loss: 4.014653]\n",
      "275 [D loss: 0.081986, acc.: 100.00%] [G loss: 4.238905]\n",
      "276 [D loss: 0.161539, acc.: 93.75%] [G loss: 4.431005]\n",
      "277 [D loss: 0.125147, acc.: 96.88%] [G loss: 3.831710]\n",
      "278 [D loss: 0.182601, acc.: 93.75%] [G loss: 4.157217]\n",
      "279 [D loss: 0.163481, acc.: 95.31%] [G loss: 4.024883]\n",
      "280 [D loss: 0.175592, acc.: 98.44%] [G loss: 3.905381]\n",
      "281 [D loss: 0.208719, acc.: 95.31%] [G loss: 3.622269]\n",
      "282 [D loss: 0.204680, acc.: 93.75%] [G loss: 3.323650]\n",
      "283 [D loss: 0.208776, acc.: 93.75%] [G loss: 3.810646]\n",
      "284 [D loss: 0.153160, acc.: 95.31%] [G loss: 3.693901]\n",
      "285 [D loss: 0.126263, acc.: 95.31%] [G loss: 3.538508]\n",
      "286 [D loss: 0.168019, acc.: 92.19%] [G loss: 3.447785]\n",
      "287 [D loss: 0.102416, acc.: 96.88%] [G loss: 3.558487]\n",
      "288 [D loss: 0.137740, acc.: 95.31%] [G loss: 3.348898]\n",
      "289 [D loss: 0.152399, acc.: 93.75%] [G loss: 4.217783]\n",
      "290 [D loss: 0.142871, acc.: 95.31%] [G loss: 4.043036]\n",
      "291 [D loss: 0.216629, acc.: 92.19%] [G loss: 4.542305]\n",
      "292 [D loss: 0.155643, acc.: 93.75%] [G loss: 4.688521]\n",
      "293 [D loss: 0.170841, acc.: 93.75%] [G loss: 3.564625]\n",
      "294 [D loss: 0.416493, acc.: 84.38%] [G loss: 6.701875]\n",
      "295 [D loss: 0.189324, acc.: 93.75%] [G loss: 7.944478]\n",
      "296 [D loss: 0.258517, acc.: 93.75%] [G loss: 6.127960]\n",
      "297 [D loss: 0.231924, acc.: 93.75%] [G loss: 4.736649]\n",
      "298 [D loss: 0.109449, acc.: 96.88%] [G loss: 3.823783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299 [D loss: 0.102638, acc.: 98.44%] [G loss: 3.044908]\n",
      "300 [D loss: 0.085674, acc.: 98.44%] [G loss: 3.040296]\n",
      "generated_data\n",
      "301 [D loss: 0.067262, acc.: 100.00%] [G loss: 2.814802]\n",
      "302 [D loss: 0.244221, acc.: 89.06%] [G loss: 3.181583]\n",
      "303 [D loss: 0.211022, acc.: 90.62%] [G loss: 4.077623]\n",
      "304 [D loss: 0.170276, acc.: 90.62%] [G loss: 3.478795]\n",
      "305 [D loss: 0.207821, acc.: 95.31%] [G loss: 3.830217]\n",
      "306 [D loss: 0.138729, acc.: 96.88%] [G loss: 3.711621]\n",
      "307 [D loss: 0.176127, acc.: 93.75%] [G loss: 3.360379]\n",
      "308 [D loss: 0.220842, acc.: 90.62%] [G loss: 4.691565]\n",
      "309 [D loss: 0.275575, acc.: 89.06%] [G loss: 3.399424]\n",
      "310 [D loss: 0.450088, acc.: 81.25%] [G loss: 4.014516]\n",
      "311 [D loss: 0.261385, acc.: 92.19%] [G loss: 3.600817]\n",
      "312 [D loss: 0.268182, acc.: 93.75%] [G loss: 2.294655]\n",
      "313 [D loss: 0.269369, acc.: 89.06%] [G loss: 3.962083]\n",
      "314 [D loss: 0.296738, acc.: 92.19%] [G loss: 3.171247]\n",
      "315 [D loss: 0.435651, acc.: 81.25%] [G loss: 3.660337]\n",
      "316 [D loss: 0.254765, acc.: 90.62%] [G loss: 3.199419]\n",
      "317 [D loss: 0.310682, acc.: 90.62%] [G loss: 2.351898]\n",
      "318 [D loss: 0.273310, acc.: 92.19%] [G loss: 3.611899]\n",
      "319 [D loss: 0.259646, acc.: 92.19%] [G loss: 3.277275]\n",
      "320 [D loss: 0.265271, acc.: 92.19%] [G loss: 2.560571]\n",
      "321 [D loss: 0.186877, acc.: 95.31%] [G loss: 2.818931]\n",
      "322 [D loss: 0.160846, acc.: 96.88%] [G loss: 3.060215]\n",
      "323 [D loss: 0.157199, acc.: 93.75%] [G loss: 3.093632]\n",
      "324 [D loss: 0.137575, acc.: 95.31%] [G loss: 3.279053]\n",
      "325 [D loss: 0.165832, acc.: 93.75%] [G loss: 3.030234]\n",
      "326 [D loss: 0.172891, acc.: 93.75%] [G loss: 3.672352]\n",
      "327 [D loss: 0.142084, acc.: 96.88%] [G loss: 3.563441]\n",
      "328 [D loss: 0.119217, acc.: 96.88%] [G loss: 3.499618]\n",
      "329 [D loss: 0.145193, acc.: 95.31%] [G loss: 3.066433]\n",
      "330 [D loss: 0.108893, acc.: 95.31%] [G loss: 2.876855]\n",
      "331 [D loss: 0.147498, acc.: 93.75%] [G loss: 4.044316]\n",
      "332 [D loss: 0.127457, acc.: 95.31%] [G loss: 4.337432]\n",
      "333 [D loss: 0.205118, acc.: 93.75%] [G loss: 3.279811]\n",
      "334 [D loss: 0.153863, acc.: 92.19%] [G loss: 2.766582]\n",
      "335 [D loss: 0.264798, acc.: 93.75%] [G loss: 4.261333]\n",
      "336 [D loss: 0.120346, acc.: 93.75%] [G loss: 4.290827]\n",
      "337 [D loss: 0.269396, acc.: 89.06%] [G loss: 3.000526]\n",
      "338 [D loss: 0.226048, acc.: 89.06%] [G loss: 3.155619]\n",
      "339 [D loss: 0.102309, acc.: 96.88%] [G loss: 3.330202]\n",
      "340 [D loss: 0.128531, acc.: 95.31%] [G loss: 3.784199]\n",
      "341 [D loss: 0.116584, acc.: 95.31%] [G loss: 3.743710]\n",
      "342 [D loss: 0.211129, acc.: 92.19%] [G loss: 3.822233]\n",
      "343 [D loss: 0.312503, acc.: 85.94%] [G loss: 5.925659]\n",
      "344 [D loss: 0.253519, acc.: 93.75%] [G loss: 5.628854]\n",
      "345 [D loss: 0.407259, acc.: 85.94%] [G loss: 3.545322]\n",
      "346 [D loss: 0.294808, acc.: 87.50%] [G loss: 4.264481]\n",
      "347 [D loss: 0.213922, acc.: 93.75%] [G loss: 4.104282]\n",
      "348 [D loss: 0.351777, acc.: 87.50%] [G loss: 3.994351]\n",
      "349 [D loss: 0.409755, acc.: 85.94%] [G loss: 6.474034]\n",
      "350 [D loss: 0.456380, acc.: 87.50%] [G loss: 4.046232]\n",
      "351 [D loss: 0.426574, acc.: 85.94%] [G loss: 3.888117]\n",
      "352 [D loss: 0.329491, acc.: 85.94%] [G loss: 3.694509]\n",
      "353 [D loss: 0.344810, acc.: 84.38%] [G loss: 3.485959]\n",
      "354 [D loss: 0.286690, acc.: 87.50%] [G loss: 3.091730]\n",
      "355 [D loss: 0.456005, acc.: 81.25%] [G loss: 3.246766]\n",
      "356 [D loss: 0.226039, acc.: 93.75%] [G loss: 2.628540]\n",
      "357 [D loss: 0.223428, acc.: 89.06%] [G loss: 2.965380]\n",
      "358 [D loss: 0.198467, acc.: 93.75%] [G loss: 3.410535]\n",
      "359 [D loss: 0.190975, acc.: 89.06%] [G loss: 3.995791]\n",
      "360 [D loss: 0.188276, acc.: 90.62%] [G loss: 2.959206]\n",
      "361 [D loss: 0.201898, acc.: 93.75%] [G loss: 3.093245]\n",
      "362 [D loss: 0.150344, acc.: 96.88%] [G loss: 3.426518]\n",
      "363 [D loss: 0.204235, acc.: 89.06%] [G loss: 3.119368]\n",
      "364 [D loss: 0.230150, acc.: 93.75%] [G loss: 3.440727]\n",
      "365 [D loss: 0.196238, acc.: 92.19%] [G loss: 3.342556]\n",
      "366 [D loss: 0.234291, acc.: 92.19%] [G loss: 3.882725]\n",
      "367 [D loss: 0.231203, acc.: 90.62%] [G loss: 3.767605]\n",
      "368 [D loss: 0.205599, acc.: 90.62%] [G loss: 3.903915]\n",
      "369 [D loss: 0.193543, acc.: 90.62%] [G loss: 3.979573]\n",
      "370 [D loss: 0.136935, acc.: 95.31%] [G loss: 4.345463]\n",
      "371 [D loss: 0.111923, acc.: 95.31%] [G loss: 4.021172]\n",
      "372 [D loss: 0.119407, acc.: 95.31%] [G loss: 3.701304]\n",
      "373 [D loss: 0.199333, acc.: 93.75%] [G loss: 3.568433]\n",
      "374 [D loss: 0.194022, acc.: 92.19%] [G loss: 4.331717]\n",
      "375 [D loss: 0.251427, acc.: 89.06%] [G loss: 6.148775]\n",
      "376 [D loss: 0.335587, acc.: 87.50%] [G loss: 4.309924]\n",
      "377 [D loss: 0.400228, acc.: 85.94%] [G loss: 5.558153]\n",
      "378 [D loss: 0.188075, acc.: 90.62%] [G loss: 5.273679]\n",
      "379 [D loss: 0.169705, acc.: 95.31%] [G loss: 3.540311]\n",
      "380 [D loss: 0.299387, acc.: 89.06%] [G loss: 5.529365]\n",
      "381 [D loss: 0.168594, acc.: 93.75%] [G loss: 4.851327]\n",
      "382 [D loss: 0.208642, acc.: 90.62%] [G loss: 3.513642]\n",
      "383 [D loss: 0.512612, acc.: 84.38%] [G loss: 4.893976]\n",
      "384 [D loss: 0.189882, acc.: 92.19%] [G loss: 4.253583]\n",
      "385 [D loss: 0.764428, acc.: 76.56%] [G loss: 7.072071]\n",
      "386 [D loss: 0.509598, acc.: 87.50%] [G loss: 6.930643]\n",
      "387 [D loss: 0.553068, acc.: 90.62%] [G loss: 3.499537]\n",
      "388 [D loss: 0.420416, acc.: 85.94%] [G loss: 3.016974]\n",
      "389 [D loss: 0.450041, acc.: 82.81%] [G loss: 3.648481]\n",
      "390 [D loss: 0.217711, acc.: 92.19%] [G loss: 3.723250]\n",
      "391 [D loss: 0.242089, acc.: 92.19%] [G loss: 2.838086]\n",
      "392 [D loss: 0.151066, acc.: 96.88%] [G loss: 3.008991]\n",
      "393 [D loss: 0.136614, acc.: 98.44%] [G loss: 3.551981]\n",
      "394 [D loss: 0.119407, acc.: 96.88%] [G loss: 3.584552]\n",
      "395 [D loss: 0.124146, acc.: 96.88%] [G loss: 3.306451]\n",
      "396 [D loss: 0.091840, acc.: 98.44%] [G loss: 3.748634]\n",
      "397 [D loss: 0.086855, acc.: 96.88%] [G loss: 3.627084]\n",
      "398 [D loss: 0.097870, acc.: 96.88%] [G loss: 3.424932]\n",
      "399 [D loss: 0.134871, acc.: 95.31%] [G loss: 3.382664]\n",
      "400 [D loss: 0.092474, acc.: 96.88%] [G loss: 3.565341]\n",
      "generated_data\n",
      "401 [D loss: 0.126684, acc.: 96.88%] [G loss: 3.016993]\n",
      "402 [D loss: 0.149650, acc.: 96.88%] [G loss: 3.205801]\n",
      "403 [D loss: 0.138720, acc.: 96.88%] [G loss: 3.223983]\n",
      "404 [D loss: 0.141423, acc.: 96.88%] [G loss: 3.395203]\n",
      "405 [D loss: 0.206866, acc.: 93.75%] [G loss: 4.209878]\n",
      "406 [D loss: 0.174695, acc.: 93.75%] [G loss: 4.179907]\n",
      "407 [D loss: 0.199547, acc.: 95.31%] [G loss: 3.352802]\n",
      "408 [D loss: 0.264510, acc.: 89.06%] [G loss: 3.762275]\n",
      "409 [D loss: 0.197595, acc.: 95.31%] [G loss: 3.992639]\n",
      "410 [D loss: 0.242928, acc.: 92.19%] [G loss: 3.740205]\n",
      "411 [D loss: 0.224314, acc.: 96.88%] [G loss: 4.106330]\n",
      "412 [D loss: 0.237361, acc.: 92.19%] [G loss: 3.377127]\n",
      "413 [D loss: 0.360644, acc.: 87.50%] [G loss: 4.290788]\n",
      "414 [D loss: 0.268138, acc.: 89.06%] [G loss: 3.720731]\n",
      "415 [D loss: 0.339369, acc.: 85.94%] [G loss: 3.294299]\n",
      "416 [D loss: 0.182670, acc.: 93.75%] [G loss: 3.845085]\n",
      "417 [D loss: 0.168477, acc.: 95.31%] [G loss: 3.232981]\n",
      "418 [D loss: 0.112909, acc.: 100.00%] [G loss: 4.411122]\n",
      "419 [D loss: 0.076753, acc.: 96.88%] [G loss: 4.096426]\n",
      "420 [D loss: 0.084072, acc.: 100.00%] [G loss: 3.908434]\n",
      "421 [D loss: 0.059872, acc.: 100.00%] [G loss: 3.834697]\n",
      "422 [D loss: 0.068166, acc.: 98.44%] [G loss: 3.563193]\n",
      "423 [D loss: 0.118042, acc.: 96.88%] [G loss: 4.203097]\n",
      "424 [D loss: 0.139409, acc.: 93.75%] [G loss: 4.056037]\n",
      "425 [D loss: 0.077657, acc.: 96.88%] [G loss: 4.231286]\n",
      "426 [D loss: 0.075557, acc.: 98.44%] [G loss: 4.802807]\n",
      "427 [D loss: 0.058694, acc.: 96.88%] [G loss: 3.984197]\n",
      "428 [D loss: 0.116257, acc.: 93.75%] [G loss: 4.240823]\n",
      "429 [D loss: 0.117800, acc.: 96.88%] [G loss: 4.757849]\n",
      "430 [D loss: 0.131055, acc.: 96.88%] [G loss: 4.232378]\n",
      "431 [D loss: 0.191487, acc.: 92.19%] [G loss: 3.572696]\n",
      "432 [D loss: 0.186784, acc.: 95.31%] [G loss: 4.738040]\n",
      "433 [D loss: 0.219118, acc.: 90.62%] [G loss: 5.184727]\n",
      "434 [D loss: 0.131971, acc.: 95.31%] [G loss: 3.632807]\n",
      "435 [D loss: 0.308379, acc.: 85.94%] [G loss: 4.279973]\n",
      "436 [D loss: 0.158093, acc.: 93.75%] [G loss: 3.582208]\n",
      "437 [D loss: 0.270458, acc.: 89.06%] [G loss: 3.614315]\n",
      "438 [D loss: 0.191388, acc.: 90.62%] [G loss: 3.372304]\n",
      "439 [D loss: 0.149559, acc.: 93.75%] [G loss: 3.675892]\n",
      "440 [D loss: 0.195409, acc.: 92.19%] [G loss: 3.409870]\n",
      "441 [D loss: 0.130698, acc.: 95.31%] [G loss: 3.700884]\n",
      "442 [D loss: 0.104103, acc.: 98.44%] [G loss: 3.858046]\n",
      "443 [D loss: 0.096538, acc.: 98.44%] [G loss: 3.595726]\n",
      "444 [D loss: 0.118007, acc.: 98.44%] [G loss: 3.283904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 [D loss: 0.108196, acc.: 96.88%] [G loss: 3.377026]\n",
      "446 [D loss: 0.095882, acc.: 95.31%] [G loss: 3.587135]\n",
      "447 [D loss: 0.067631, acc.: 98.44%] [G loss: 3.835010]\n",
      "448 [D loss: 0.102765, acc.: 96.88%] [G loss: 3.967553]\n",
      "449 [D loss: 0.108938, acc.: 98.44%] [G loss: 3.829249]\n",
      "450 [D loss: 0.084895, acc.: 98.44%] [G loss: 3.588107]\n",
      "451 [D loss: 0.131594, acc.: 98.44%] [G loss: 4.001212]\n",
      "452 [D loss: 0.131842, acc.: 96.88%] [G loss: 3.949997]\n",
      "453 [D loss: 0.102971, acc.: 96.88%] [G loss: 4.286574]\n",
      "454 [D loss: 0.158487, acc.: 95.31%] [G loss: 3.911625]\n",
      "455 [D loss: 0.092190, acc.: 96.88%] [G loss: 3.938680]\n",
      "456 [D loss: 0.134508, acc.: 95.31%] [G loss: 4.384017]\n",
      "457 [D loss: 0.103445, acc.: 96.88%] [G loss: 4.365121]\n",
      "458 [D loss: 0.082901, acc.: 98.44%] [G loss: 4.418756]\n",
      "459 [D loss: 0.123103, acc.: 93.75%] [G loss: 3.998669]\n",
      "460 [D loss: 0.058258, acc.: 98.44%] [G loss: 4.126666]\n",
      "461 [D loss: 0.113030, acc.: 96.88%] [G loss: 4.381958]\n",
      "462 [D loss: 0.207473, acc.: 93.75%] [G loss: 5.532909]\n",
      "463 [D loss: 0.102101, acc.: 95.31%] [G loss: 4.376808]\n",
      "464 [D loss: 0.149623, acc.: 96.88%] [G loss: 4.263405]\n",
      "465 [D loss: 0.113995, acc.: 95.31%] [G loss: 4.229434]\n",
      "466 [D loss: 0.132834, acc.: 96.88%] [G loss: 4.083382]\n",
      "467 [D loss: 0.103843, acc.: 98.44%] [G loss: 3.461519]\n",
      "468 [D loss: 0.111193, acc.: 98.44%] [G loss: 3.771476]\n",
      "469 [D loss: 0.156137, acc.: 95.31%] [G loss: 4.087049]\n",
      "470 [D loss: 0.114553, acc.: 98.44%] [G loss: 4.004972]\n",
      "471 [D loss: 0.162219, acc.: 96.88%] [G loss: 3.901674]\n",
      "472 [D loss: 0.258231, acc.: 89.06%] [G loss: 4.447967]\n",
      "473 [D loss: 0.183098, acc.: 93.75%] [G loss: 4.757064]\n",
      "474 [D loss: 0.133935, acc.: 95.31%] [G loss: 4.026125]\n",
      "475 [D loss: 0.112454, acc.: 98.44%] [G loss: 3.900071]\n",
      "476 [D loss: 0.101692, acc.: 96.88%] [G loss: 4.419387]\n",
      "477 [D loss: 0.161501, acc.: 93.75%] [G loss: 4.955319]\n",
      "478 [D loss: 0.172023, acc.: 93.75%] [G loss: 4.826511]\n",
      "479 [D loss: 0.092793, acc.: 96.88%] [G loss: 4.110774]\n",
      "480 [D loss: 0.180697, acc.: 95.31%] [G loss: 4.434597]\n",
      "481 [D loss: 0.140106, acc.: 96.88%] [G loss: 4.530256]\n",
      "482 [D loss: 0.150060, acc.: 95.31%] [G loss: 3.882064]\n",
      "483 [D loss: 0.190525, acc.: 92.19%] [G loss: 5.306546]\n",
      "484 [D loss: 0.134723, acc.: 96.88%] [G loss: 4.787263]\n",
      "485 [D loss: 0.214279, acc.: 95.31%] [G loss: 3.778135]\n",
      "486 [D loss: 0.150761, acc.: 95.31%] [G loss: 3.924908]\n",
      "487 [D loss: 0.188495, acc.: 95.31%] [G loss: 3.430577]\n",
      "488 [D loss: 0.173431, acc.: 93.75%] [G loss: 3.851432]\n",
      "489 [D loss: 0.141896, acc.: 95.31%] [G loss: 3.634099]\n",
      "490 [D loss: 0.168995, acc.: 95.31%] [G loss: 3.283602]\n",
      "491 [D loss: 0.129169, acc.: 95.31%] [G loss: 3.303480]\n",
      "492 [D loss: 0.130687, acc.: 95.31%] [G loss: 3.731463]\n",
      "493 [D loss: 0.085018, acc.: 96.88%] [G loss: 3.807868]\n",
      "494 [D loss: 0.084188, acc.: 96.88%] [G loss: 3.696889]\n",
      "495 [D loss: 0.099466, acc.: 95.31%] [G loss: 3.307073]\n",
      "496 [D loss: 0.092157, acc.: 96.88%] [G loss: 3.224687]\n",
      "497 [D loss: 0.109236, acc.: 96.88%] [G loss: 3.696531]\n",
      "498 [D loss: 0.119607, acc.: 96.88%] [G loss: 3.785042]\n",
      "499 [D loss: 0.122638, acc.: 95.31%] [G loss: 4.359408]\n",
      "500 [D loss: 0.118343, acc.: 96.88%] [G loss: 4.098867]\n",
      "generated_data\n",
      "501 [D loss: 0.099387, acc.: 96.88%] [G loss: 3.479736]\n",
      "502 [D loss: 0.110425, acc.: 96.88%] [G loss: 3.464705]\n",
      "503 [D loss: 0.118533, acc.: 96.88%] [G loss: 3.302164]\n",
      "504 [D loss: 0.136113, acc.: 95.31%] [G loss: 3.436933]\n",
      "505 [D loss: 0.124811, acc.: 96.88%] [G loss: 3.495006]\n",
      "506 [D loss: 0.120604, acc.: 96.88%] [G loss: 3.399401]\n",
      "507 [D loss: 0.103383, acc.: 96.88%] [G loss: 3.154860]\n",
      "508 [D loss: 0.116194, acc.: 95.31%] [G loss: 3.378285]\n",
      "509 [D loss: 0.114881, acc.: 95.31%] [G loss: 3.409155]\n",
      "510 [D loss: 0.121969, acc.: 96.88%] [G loss: 3.423865]\n",
      "511 [D loss: 0.132243, acc.: 96.88%] [G loss: 3.508538]\n",
      "512 [D loss: 0.113923, acc.: 96.88%] [G loss: 3.320936]\n",
      "513 [D loss: 0.116768, acc.: 96.88%] [G loss: 3.028852]\n",
      "514 [D loss: 0.205870, acc.: 92.19%] [G loss: 3.757196]\n",
      "515 [D loss: 0.128067, acc.: 96.88%] [G loss: 4.200126]\n",
      "516 [D loss: 0.134160, acc.: 96.88%] [G loss: 3.711492]\n",
      "517 [D loss: 0.111960, acc.: 96.88%] [G loss: 3.619767]\n",
      "518 [D loss: 0.122174, acc.: 96.88%] [G loss: 3.471515]\n",
      "519 [D loss: 0.117438, acc.: 96.88%] [G loss: 3.185594]\n",
      "520 [D loss: 0.131626, acc.: 96.88%] [G loss: 3.276703]\n",
      "521 [D loss: 0.098472, acc.: 96.88%] [G loss: 3.363304]\n",
      "522 [D loss: 0.110900, acc.: 96.88%] [G loss: 2.951547]\n",
      "523 [D loss: 0.108402, acc.: 96.88%] [G loss: 3.156875]\n",
      "524 [D loss: 0.118629, acc.: 96.88%] [G loss: 3.764837]\n",
      "525 [D loss: 0.104435, acc.: 96.88%] [G loss: 3.291805]\n",
      "526 [D loss: 0.145036, acc.: 96.88%] [G loss: 3.587005]\n",
      "527 [D loss: 0.101454, acc.: 96.88%] [G loss: 3.561326]\n",
      "528 [D loss: 0.110029, acc.: 96.88%] [G loss: 3.463653]\n",
      "529 [D loss: 0.110146, acc.: 96.88%] [G loss: 3.232257]\n",
      "530 [D loss: 0.081085, acc.: 96.88%] [G loss: 3.371937]\n",
      "531 [D loss: 0.105959, acc.: 96.88%] [G loss: 2.988245]\n",
      "532 [D loss: 0.106560, acc.: 96.88%] [G loss: 3.117872]\n",
      "533 [D loss: 0.157387, acc.: 96.88%] [G loss: 3.515109]\n",
      "534 [D loss: 0.090564, acc.: 96.88%] [G loss: 3.617605]\n",
      "535 [D loss: 0.128856, acc.: 96.88%] [G loss: 3.492220]\n",
      "536 [D loss: 0.103696, acc.: 96.88%] [G loss: 3.599848]\n",
      "537 [D loss: 0.137326, acc.: 95.31%] [G loss: 3.622656]\n",
      "538 [D loss: 0.125386, acc.: 96.88%] [G loss: 3.681777]\n",
      "539 [D loss: 0.116254, acc.: 96.88%] [G loss: 3.249734]\n",
      "540 [D loss: 0.102881, acc.: 96.88%] [G loss: 3.467260]\n",
      "541 [D loss: 0.110424, acc.: 96.88%] [G loss: 3.347465]\n",
      "542 [D loss: 0.124375, acc.: 96.88%] [G loss: 3.467702]\n",
      "543 [D loss: 0.103925, acc.: 96.88%] [G loss: 3.407205]\n",
      "544 [D loss: 0.152000, acc.: 96.88%] [G loss: 3.645414]\n",
      "545 [D loss: 0.098810, acc.: 96.88%] [G loss: 3.708307]\n",
      "546 [D loss: 0.129078, acc.: 96.88%] [G loss: 3.594477]\n",
      "547 [D loss: 0.123881, acc.: 96.88%] [G loss: 3.273087]\n",
      "548 [D loss: 0.096549, acc.: 96.88%] [G loss: 3.343779]\n",
      "549 [D loss: 0.112718, acc.: 96.88%] [G loss: 3.409257]\n",
      "550 [D loss: 0.103628, acc.: 96.88%] [G loss: 3.422969]\n",
      "551 [D loss: 0.117526, acc.: 96.88%] [G loss: 3.354994]\n",
      "552 [D loss: 0.109684, acc.: 96.88%] [G loss: 3.534175]\n",
      "553 [D loss: 0.095908, acc.: 96.88%] [G loss: 3.336729]\n",
      "554 [D loss: 0.130213, acc.: 96.88%] [G loss: 3.367028]\n",
      "555 [D loss: 0.112631, acc.: 96.88%] [G loss: 3.143939]\n",
      "556 [D loss: 0.113712, acc.: 96.88%] [G loss: 3.140462]\n",
      "557 [D loss: 0.110897, acc.: 96.88%] [G loss: 3.372752]\n",
      "558 [D loss: 0.103307, acc.: 96.88%] [G loss: 3.263151]\n",
      "559 [D loss: 0.105007, acc.: 96.88%] [G loss: 3.565212]\n",
      "560 [D loss: 0.109852, acc.: 96.88%] [G loss: 3.512354]\n",
      "561 [D loss: 0.130014, acc.: 96.88%] [G loss: 3.466916]\n",
      "562 [D loss: 0.117599, acc.: 96.88%] [G loss: 3.447098]\n",
      "563 [D loss: 0.118517, acc.: 96.88%] [G loss: 3.358560]\n",
      "564 [D loss: 0.118196, acc.: 96.88%] [G loss: 3.385441]\n",
      "565 [D loss: 0.096618, acc.: 96.88%] [G loss: 3.393491]\n",
      "566 [D loss: 0.113194, acc.: 96.88%] [G loss: 3.478360]\n",
      "567 [D loss: 0.123621, acc.: 96.88%] [G loss: 3.016907]\n",
      "568 [D loss: 0.103930, acc.: 96.88%] [G loss: 3.426166]\n",
      "569 [D loss: 0.117254, acc.: 96.88%] [G loss: 2.707000]\n",
      "570 [D loss: 0.126643, acc.: 96.88%] [G loss: 3.252882]\n",
      "571 [D loss: 0.125379, acc.: 96.88%] [G loss: 3.405201]\n",
      "572 [D loss: 0.158072, acc.: 95.31%] [G loss: 3.954959]\n",
      "573 [D loss: 0.119989, acc.: 96.88%] [G loss: 3.824799]\n",
      "574 [D loss: 0.124128, acc.: 96.88%] [G loss: 3.517737]\n",
      "575 [D loss: 0.127701, acc.: 96.88%] [G loss: 3.385271]\n",
      "576 [D loss: 0.117159, acc.: 96.88%] [G loss: 3.429566]\n",
      "577 [D loss: 0.123424, acc.: 96.88%] [G loss: 3.361257]\n",
      "578 [D loss: 0.113974, acc.: 96.88%] [G loss: 3.437061]\n",
      "579 [D loss: 0.117616, acc.: 96.88%] [G loss: 3.427069]\n",
      "580 [D loss: 0.111588, acc.: 96.88%] [G loss: 3.050196]\n",
      "581 [D loss: 0.137117, acc.: 96.88%] [G loss: 3.168549]\n",
      "582 [D loss: 0.095146, acc.: 96.88%] [G loss: 3.671629]\n",
      "583 [D loss: 0.106636, acc.: 96.88%] [G loss: 3.646567]\n",
      "584 [D loss: 0.112781, acc.: 96.88%] [G loss: 3.176188]\n",
      "585 [D loss: 0.106677, acc.: 96.88%] [G loss: 3.019009]\n",
      "586 [D loss: 0.128432, acc.: 96.88%] [G loss: 3.149622]\n",
      "587 [D loss: 0.115174, acc.: 96.88%] [G loss: 3.049618]\n",
      "588 [D loss: 0.109865, acc.: 96.88%] [G loss: 3.251978]\n",
      "589 [D loss: 0.131122, acc.: 96.88%] [G loss: 2.952010]\n",
      "590 [D loss: 0.101379, acc.: 96.88%] [G loss: 3.241883]\n",
      "591 [D loss: 0.114024, acc.: 96.88%] [G loss: 3.283107]\n",
      "592 [D loss: 0.119761, acc.: 96.88%] [G loss: 3.252832]\n",
      "593 [D loss: 0.106295, acc.: 96.88%] [G loss: 3.282004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594 [D loss: 0.114001, acc.: 96.88%] [G loss: 3.457108]\n",
      "595 [D loss: 0.106316, acc.: 96.88%] [G loss: 3.341619]\n",
      "596 [D loss: 0.110886, acc.: 96.88%] [G loss: 3.359463]\n",
      "597 [D loss: 0.128372, acc.: 96.88%] [G loss: 3.153768]\n",
      "598 [D loss: 0.125065, acc.: 96.88%] [G loss: 3.365054]\n",
      "599 [D loss: 0.122472, acc.: 96.88%] [G loss: 3.280362]\n",
      "600 [D loss: 0.123699, acc.: 96.88%] [G loss: 3.121958]\n",
      "generated_data\n",
      "601 [D loss: 0.114213, acc.: 96.88%] [G loss: 3.167660]\n",
      "602 [D loss: 0.113153, acc.: 96.88%] [G loss: 3.141538]\n",
      "603 [D loss: 0.101218, acc.: 96.88%] [G loss: 2.979659]\n",
      "604 [D loss: 0.100668, acc.: 96.88%] [G loss: 3.174911]\n",
      "605 [D loss: 0.116165, acc.: 96.88%] [G loss: 3.167973]\n",
      "606 [D loss: 0.115667, acc.: 96.88%] [G loss: 3.249844]\n",
      "607 [D loss: 0.102787, acc.: 96.88%] [G loss: 3.407658]\n",
      "608 [D loss: 0.118797, acc.: 96.88%] [G loss: 3.208567]\n",
      "609 [D loss: 0.110410, acc.: 96.88%] [G loss: 3.325999]\n",
      "610 [D loss: 0.124209, acc.: 96.88%] [G loss: 3.455759]\n",
      "611 [D loss: 0.120959, acc.: 96.88%] [G loss: 3.256658]\n",
      "612 [D loss: 0.121107, acc.: 95.31%] [G loss: 3.454094]\n",
      "613 [D loss: 0.124186, acc.: 96.88%] [G loss: 3.583529]\n",
      "614 [D loss: 0.118636, acc.: 96.88%] [G loss: 3.277277]\n",
      "615 [D loss: 0.126466, acc.: 96.88%] [G loss: 3.266322]\n",
      "616 [D loss: 0.121333, acc.: 96.88%] [G loss: 3.213031]\n",
      "617 [D loss: 0.108583, acc.: 96.88%] [G loss: 3.132838]\n",
      "618 [D loss: 0.102202, acc.: 96.88%] [G loss: 3.038380]\n",
      "619 [D loss: 0.125751, acc.: 96.88%] [G loss: 3.206025]\n",
      "620 [D loss: 0.134643, acc.: 96.88%] [G loss: 3.179996]\n",
      "621 [D loss: 0.115956, acc.: 96.88%] [G loss: 3.265172]\n",
      "622 [D loss: 0.127120, acc.: 96.88%] [G loss: 3.094708]\n",
      "623 [D loss: 0.106257, acc.: 96.88%] [G loss: 3.145867]\n",
      "624 [D loss: 0.100889, acc.: 96.88%] [G loss: 3.246187]\n",
      "625 [D loss: 0.106590, acc.: 96.88%] [G loss: 3.247528]\n",
      "626 [D loss: 0.109583, acc.: 96.88%] [G loss: 3.303696]\n",
      "627 [D loss: 0.110865, acc.: 96.88%] [G loss: 3.125740]\n",
      "628 [D loss: 0.118490, acc.: 96.88%] [G loss: 3.094161]\n",
      "629 [D loss: 0.110559, acc.: 96.88%] [G loss: 3.072908]\n",
      "630 [D loss: 0.108652, acc.: 96.88%] [G loss: 3.314342]\n",
      "631 [D loss: 0.125600, acc.: 96.88%] [G loss: 3.087552]\n",
      "632 [D loss: 0.116889, acc.: 96.88%] [G loss: 3.016534]\n",
      "633 [D loss: 0.117515, acc.: 96.88%] [G loss: 3.372409]\n",
      "634 [D loss: 0.114150, acc.: 96.88%] [G loss: 3.309652]\n",
      "635 [D loss: 0.118154, acc.: 96.88%] [G loss: 3.379768]\n",
      "636 [D loss: 0.118458, acc.: 96.88%] [G loss: 3.253171]\n",
      "637 [D loss: 0.125222, acc.: 96.88%] [G loss: 3.219507]\n",
      "638 [D loss: 0.119488, acc.: 96.88%] [G loss: 3.486426]\n",
      "639 [D loss: 0.115874, acc.: 96.88%] [G loss: 3.200447]\n",
      "640 [D loss: 0.130726, acc.: 96.88%] [G loss: 3.473555]\n",
      "641 [D loss: 0.110819, acc.: 96.88%] [G loss: 3.055343]\n",
      "642 [D loss: 0.099813, acc.: 96.88%] [G loss: 3.357519]\n",
      "643 [D loss: 0.111351, acc.: 96.88%] [G loss: 3.294580]\n",
      "644 [D loss: 0.132638, acc.: 96.88%] [G loss: 3.241474]\n",
      "645 [D loss: 0.101625, acc.: 96.88%] [G loss: 3.537363]\n",
      "646 [D loss: 0.100918, acc.: 96.88%] [G loss: 3.269378]\n",
      "647 [D loss: 0.120211, acc.: 96.88%] [G loss: 3.315680]\n",
      "648 [D loss: 0.112955, acc.: 96.88%] [G loss: 3.183141]\n",
      "649 [D loss: 0.118343, acc.: 96.88%] [G loss: 3.219129]\n",
      "650 [D loss: 0.121071, acc.: 96.88%] [G loss: 3.194302]\n",
      "651 [D loss: 0.105535, acc.: 96.88%] [G loss: 3.579718]\n",
      "652 [D loss: 0.110340, acc.: 96.88%] [G loss: 3.344172]\n",
      "653 [D loss: 0.116813, acc.: 96.88%] [G loss: 3.326002]\n",
      "654 [D loss: 0.108030, acc.: 96.88%] [G loss: 3.155557]\n",
      "655 [D loss: 0.127925, acc.: 96.88%] [G loss: 3.179410]\n",
      "656 [D loss: 0.125965, acc.: 96.88%] [G loss: 3.323363]\n",
      "657 [D loss: 0.111613, acc.: 96.88%] [G loss: 3.369901]\n",
      "658 [D loss: 0.102842, acc.: 96.88%] [G loss: 3.366996]\n",
      "659 [D loss: 0.103874, acc.: 96.88%] [G loss: 3.393076]\n",
      "660 [D loss: 0.098244, acc.: 96.88%] [G loss: 3.180509]\n",
      "661 [D loss: 0.113961, acc.: 96.88%] [G loss: 3.209213]\n",
      "662 [D loss: 0.115015, acc.: 96.88%] [G loss: 3.328156]\n",
      "663 [D loss: 0.108182, acc.: 96.88%] [G loss: 2.993824]\n",
      "664 [D loss: 0.116070, acc.: 96.88%] [G loss: 3.250173]\n",
      "665 [D loss: 0.123264, acc.: 96.88%] [G loss: 3.342453]\n",
      "666 [D loss: 0.139137, acc.: 96.88%] [G loss: 3.431184]\n",
      "667 [D loss: 0.121391, acc.: 96.88%] [G loss: 3.456731]\n",
      "668 [D loss: 0.126594, acc.: 96.88%] [G loss: 3.266567]\n",
      "669 [D loss: 0.127184, acc.: 96.88%] [G loss: 3.135581]\n",
      "670 [D loss: 0.116727, acc.: 96.88%] [G loss: 2.992119]\n",
      "671 [D loss: 0.122559, acc.: 96.88%] [G loss: 3.243338]\n",
      "672 [D loss: 0.115199, acc.: 96.88%] [G loss: 3.233886]\n",
      "673 [D loss: 0.124362, acc.: 96.88%] [G loss: 3.146807]\n",
      "674 [D loss: 0.122462, acc.: 96.88%] [G loss: 3.440338]\n",
      "675 [D loss: 0.105746, acc.: 96.88%] [G loss: 3.259117]\n",
      "676 [D loss: 0.101319, acc.: 96.88%] [G loss: 3.301899]\n",
      "677 [D loss: 0.109250, acc.: 96.88%] [G loss: 3.371044]\n",
      "678 [D loss: 0.129808, acc.: 96.88%] [G loss: 3.299239]\n",
      "679 [D loss: 0.116241, acc.: 96.88%] [G loss: 3.505165]\n",
      "680 [D loss: 0.124844, acc.: 96.88%] [G loss: 3.231475]\n",
      "681 [D loss: 0.108309, acc.: 96.88%] [G loss: 3.199954]\n",
      "682 [D loss: 0.134315, acc.: 96.88%] [G loss: 3.294464]\n",
      "683 [D loss: 0.129930, acc.: 96.88%] [G loss: 3.283689]\n",
      "684 [D loss: 0.103607, acc.: 96.88%] [G loss: 3.303255]\n",
      "685 [D loss: 0.113205, acc.: 96.88%] [G loss: 3.153007]\n",
      "686 [D loss: 0.108707, acc.: 96.88%] [G loss: 3.268070]\n",
      "687 [D loss: 0.115787, acc.: 96.88%] [G loss: 3.167278]\n",
      "688 [D loss: 0.110401, acc.: 96.88%] [G loss: 3.171475]\n",
      "689 [D loss: 0.095904, acc.: 96.88%] [G loss: 3.493912]\n",
      "690 [D loss: 0.109387, acc.: 96.88%] [G loss: 3.479299]\n",
      "691 [D loss: 0.111352, acc.: 96.88%] [G loss: 3.284371]\n",
      "692 [D loss: 0.112630, acc.: 96.88%] [G loss: 3.421516]\n",
      "693 [D loss: 0.123353, acc.: 96.88%] [G loss: 3.267467]\n",
      "694 [D loss: 0.132581, acc.: 96.88%] [G loss: 3.222939]\n",
      "695 [D loss: 0.133458, acc.: 96.88%] [G loss: 3.306421]\n",
      "696 [D loss: 0.094983, acc.: 96.88%] [G loss: 3.342491]\n",
      "697 [D loss: 0.097528, acc.: 96.88%] [G loss: 3.245595]\n",
      "698 [D loss: 0.125189, acc.: 96.88%] [G loss: 3.259237]\n",
      "699 [D loss: 0.119148, acc.: 96.88%] [G loss: 3.268463]\n",
      "700 [D loss: 0.104525, acc.: 96.88%] [G loss: 3.077159]\n",
      "generated_data\n",
      "701 [D loss: 0.117767, acc.: 96.88%] [G loss: 3.219175]\n",
      "702 [D loss: 0.133434, acc.: 96.88%] [G loss: 3.512458]\n",
      "703 [D loss: 0.105283, acc.: 96.88%] [G loss: 3.441396]\n",
      "704 [D loss: 0.129430, acc.: 96.88%] [G loss: 3.198474]\n",
      "705 [D loss: 0.125308, acc.: 96.88%] [G loss: 3.157631]\n",
      "706 [D loss: 0.121044, acc.: 96.88%] [G loss: 3.364853]\n",
      "707 [D loss: 0.103787, acc.: 96.88%] [G loss: 3.171128]\n",
      "708 [D loss: 0.131442, acc.: 96.88%] [G loss: 3.393072]\n",
      "709 [D loss: 0.127167, acc.: 96.88%] [G loss: 3.233790]\n",
      "710 [D loss: 0.118619, acc.: 96.88%] [G loss: 3.393839]\n",
      "711 [D loss: 0.115758, acc.: 96.88%] [G loss: 3.148415]\n",
      "712 [D loss: 0.109811, acc.: 96.88%] [G loss: 3.244090]\n",
      "713 [D loss: 0.132594, acc.: 96.88%] [G loss: 3.117519]\n",
      "714 [D loss: 0.114278, acc.: 96.88%] [G loss: 3.239699]\n",
      "715 [D loss: 0.114848, acc.: 96.88%] [G loss: 3.116961]\n",
      "716 [D loss: 0.112278, acc.: 96.88%] [G loss: 3.238247]\n",
      "717 [D loss: 0.110708, acc.: 96.88%] [G loss: 3.244756]\n",
      "718 [D loss: 0.099670, acc.: 96.88%] [G loss: 3.413227]\n",
      "719 [D loss: 0.123189, acc.: 96.88%] [G loss: 3.335333]\n",
      "720 [D loss: 0.116002, acc.: 96.88%] [G loss: 3.339499]\n",
      "721 [D loss: 0.114604, acc.: 96.88%] [G loss: 3.317584]\n",
      "722 [D loss: 0.124928, acc.: 96.88%] [G loss: 3.105279]\n",
      "723 [D loss: 0.127360, acc.: 96.88%] [G loss: 3.345149]\n",
      "724 [D loss: 0.103782, acc.: 96.88%] [G loss: 3.539747]\n",
      "725 [D loss: 0.111467, acc.: 96.88%] [G loss: 3.276167]\n",
      "726 [D loss: 0.124551, acc.: 96.88%] [G loss: 3.076701]\n",
      "727 [D loss: 0.117946, acc.: 96.88%] [G loss: 3.092706]\n",
      "728 [D loss: 0.107020, acc.: 96.88%] [G loss: 3.270643]\n",
      "729 [D loss: 0.110925, acc.: 96.88%] [G loss: 3.029413]\n",
      "730 [D loss: 0.113471, acc.: 96.88%] [G loss: 3.249331]\n",
      "731 [D loss: 0.115199, acc.: 96.88%] [G loss: 3.336156]\n",
      "732 [D loss: 0.112244, acc.: 96.88%] [G loss: 3.078557]\n",
      "733 [D loss: 0.122107, acc.: 96.88%] [G loss: 3.113702]\n",
      "734 [D loss: 0.118899, acc.: 96.88%] [G loss: 3.364697]\n",
      "735 [D loss: 0.115793, acc.: 96.88%] [G loss: 3.307974]\n",
      "736 [D loss: 0.101152, acc.: 96.88%] [G loss: 3.454333]\n",
      "737 [D loss: 0.120219, acc.: 96.88%] [G loss: 3.263339]\n",
      "738 [D loss: 0.124476, acc.: 96.88%] [G loss: 3.058575]\n",
      "739 [D loss: 0.104531, acc.: 96.88%] [G loss: 3.332134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740 [D loss: 0.133404, acc.: 96.88%] [G loss: 3.476997]\n",
      "741 [D loss: 0.118479, acc.: 96.88%] [G loss: 3.146390]\n",
      "742 [D loss: 0.112338, acc.: 96.88%] [G loss: 3.128078]\n",
      "743 [D loss: 0.135748, acc.: 96.88%] [G loss: 3.148916]\n",
      "744 [D loss: 0.119280, acc.: 96.88%] [G loss: 3.324473]\n",
      "745 [D loss: 0.120790, acc.: 96.88%] [G loss: 3.333827]\n",
      "746 [D loss: 0.117440, acc.: 96.88%] [G loss: 3.480515]\n",
      "747 [D loss: 0.115876, acc.: 96.88%] [G loss: 3.243768]\n",
      "748 [D loss: 0.129379, acc.: 96.88%] [G loss: 3.472745]\n",
      "749 [D loss: 0.120144, acc.: 96.88%] [G loss: 3.174971]\n",
      "750 [D loss: 0.112702, acc.: 96.88%] [G loss: 3.220477]\n",
      "751 [D loss: 0.111202, acc.: 96.88%] [G loss: 3.432573]\n",
      "752 [D loss: 0.124556, acc.: 96.88%] [G loss: 3.232582]\n",
      "753 [D loss: 0.117198, acc.: 96.88%] [G loss: 3.164085]\n",
      "754 [D loss: 0.110961, acc.: 96.88%] [G loss: 3.173877]\n",
      "755 [D loss: 0.114824, acc.: 96.88%] [G loss: 2.899291]\n",
      "756 [D loss: 0.104341, acc.: 96.88%] [G loss: 3.126162]\n",
      "757 [D loss: 0.108650, acc.: 96.88%] [G loss: 3.259165]\n",
      "758 [D loss: 0.111151, acc.: 96.88%] [G loss: 3.352760]\n",
      "759 [D loss: 0.117171, acc.: 96.88%] [G loss: 3.254502]\n",
      "760 [D loss: 0.109597, acc.: 96.88%] [G loss: 3.253386]\n",
      "761 [D loss: 0.144218, acc.: 96.88%] [G loss: 3.337113]\n",
      "762 [D loss: 0.112774, acc.: 96.88%] [G loss: 3.091266]\n",
      "763 [D loss: 0.115582, acc.: 96.88%] [G loss: 3.082107]\n",
      "764 [D loss: 0.120922, acc.: 96.88%] [G loss: 3.256795]\n",
      "765 [D loss: 0.113883, acc.: 96.88%] [G loss: 3.180959]\n",
      "766 [D loss: 0.107892, acc.: 96.88%] [G loss: 3.388503]\n",
      "767 [D loss: 0.118468, acc.: 96.88%] [G loss: 3.475482]\n",
      "768 [D loss: 0.112017, acc.: 96.88%] [G loss: 3.308963]\n",
      "769 [D loss: 0.128193, acc.: 96.88%] [G loss: 3.384936]\n",
      "770 [D loss: 0.111266, acc.: 96.88%] [G loss: 3.278995]\n",
      "771 [D loss: 0.107014, acc.: 96.88%] [G loss: 3.437216]\n",
      "772 [D loss: 0.114860, acc.: 96.88%] [G loss: 3.002203]\n",
      "773 [D loss: 0.103473, acc.: 96.88%] [G loss: 3.304921]\n",
      "774 [D loss: 0.122204, acc.: 96.88%] [G loss: 3.349935]\n",
      "775 [D loss: 0.116174, acc.: 96.88%] [G loss: 3.301793]\n",
      "776 [D loss: 0.137390, acc.: 96.88%] [G loss: 3.260738]\n",
      "777 [D loss: 0.118012, acc.: 96.88%] [G loss: 3.346488]\n",
      "778 [D loss: 0.105805, acc.: 96.88%] [G loss: 3.270324]\n",
      "779 [D loss: 0.119414, acc.: 96.88%] [G loss: 3.310195]\n",
      "780 [D loss: 0.116951, acc.: 96.88%] [G loss: 3.287114]\n",
      "781 [D loss: 0.124555, acc.: 96.88%] [G loss: 3.228304]\n",
      "782 [D loss: 0.118986, acc.: 96.88%] [G loss: 3.215049]\n",
      "783 [D loss: 0.128895, acc.: 96.88%] [G loss: 3.175776]\n",
      "784 [D loss: 0.109454, acc.: 96.88%] [G loss: 3.392321]\n",
      "785 [D loss: 0.135668, acc.: 96.88%] [G loss: 3.275476]\n",
      "786 [D loss: 0.109831, acc.: 96.88%] [G loss: 3.280846]\n",
      "787 [D loss: 0.106400, acc.: 96.88%] [G loss: 3.346937]\n",
      "788 [D loss: 0.101865, acc.: 96.88%] [G loss: 3.321006]\n",
      "789 [D loss: 0.117364, acc.: 96.88%] [G loss: 3.355546]\n",
      "790 [D loss: 0.113321, acc.: 96.88%] [G loss: 3.304659]\n",
      "791 [D loss: 0.136977, acc.: 96.88%] [G loss: 3.212623]\n",
      "792 [D loss: 0.109098, acc.: 96.88%] [G loss: 3.243382]\n",
      "793 [D loss: 0.108710, acc.: 96.88%] [G loss: 3.076709]\n",
      "794 [D loss: 0.117182, acc.: 96.88%] [G loss: 3.229092]\n",
      "795 [D loss: 0.117498, acc.: 96.88%] [G loss: 3.246694]\n",
      "796 [D loss: 0.119618, acc.: 96.88%] [G loss: 3.121275]\n",
      "797 [D loss: 0.117419, acc.: 96.88%] [G loss: 3.236813]\n",
      "798 [D loss: 0.113712, acc.: 96.88%] [G loss: 3.358692]\n",
      "799 [D loss: 0.115178, acc.: 96.88%] [G loss: 2.999224]\n",
      "800 [D loss: 0.109829, acc.: 96.88%] [G loss: 3.285440]\n",
      "generated_data\n",
      "801 [D loss: 0.116249, acc.: 96.88%] [G loss: 3.091803]\n",
      "802 [D loss: 0.107536, acc.: 96.88%] [G loss: 3.137907]\n",
      "803 [D loss: 0.113089, acc.: 96.88%] [G loss: 3.120046]\n",
      "804 [D loss: 0.111563, acc.: 96.88%] [G loss: 3.128788]\n",
      "805 [D loss: 0.115568, acc.: 96.88%] [G loss: 3.484289]\n",
      "806 [D loss: 0.116765, acc.: 96.88%] [G loss: 3.128462]\n",
      "807 [D loss: 0.103365, acc.: 96.88%] [G loss: 3.306734]\n",
      "808 [D loss: 0.125006, acc.: 96.88%] [G loss: 3.215251]\n",
      "809 [D loss: 0.109932, acc.: 96.88%] [G loss: 3.456818]\n",
      "810 [D loss: 0.103755, acc.: 96.88%] [G loss: 3.353915]\n",
      "811 [D loss: 0.108412, acc.: 96.88%] [G loss: 3.199847]\n",
      "812 [D loss: 0.139568, acc.: 96.88%] [G loss: 3.394669]\n",
      "813 [D loss: 0.098693, acc.: 96.88%] [G loss: 3.245934]\n",
      "814 [D loss: 0.141640, acc.: 96.88%] [G loss: 3.214473]\n",
      "815 [D loss: 0.129068, acc.: 96.88%] [G loss: 3.302723]\n",
      "816 [D loss: 0.109401, acc.: 96.88%] [G loss: 3.161407]\n",
      "817 [D loss: 0.129126, acc.: 96.88%] [G loss: 3.242369]\n",
      "818 [D loss: 0.098447, acc.: 96.88%] [G loss: 3.253894]\n",
      "819 [D loss: 0.082748, acc.: 96.88%] [G loss: 3.290095]\n",
      "820 [D loss: 0.118778, acc.: 96.88%] [G loss: 3.225831]\n",
      "821 [D loss: 0.116592, acc.: 96.88%] [G loss: 3.378575]\n",
      "822 [D loss: 0.127044, acc.: 96.88%] [G loss: 3.147699]\n",
      "823 [D loss: 0.113538, acc.: 96.88%] [G loss: 3.152892]\n",
      "824 [D loss: 0.114881, acc.: 96.88%] [G loss: 3.338582]\n",
      "825 [D loss: 0.094313, acc.: 96.88%] [G loss: 3.468270]\n",
      "826 [D loss: 0.135452, acc.: 96.88%] [G loss: 3.172292]\n",
      "827 [D loss: 0.118373, acc.: 96.88%] [G loss: 3.033538]\n",
      "828 [D loss: 0.107971, acc.: 96.88%] [G loss: 3.540071]\n",
      "829 [D loss: 0.127534, acc.: 96.88%] [G loss: 3.328555]\n",
      "830 [D loss: 0.113060, acc.: 96.88%] [G loss: 3.504813]\n",
      "831 [D loss: 0.106136, acc.: 96.88%] [G loss: 3.237785]\n",
      "832 [D loss: 0.125891, acc.: 96.88%] [G loss: 3.100500]\n",
      "833 [D loss: 0.120523, acc.: 96.88%] [G loss: 3.165858]\n",
      "834 [D loss: 0.132214, acc.: 96.88%] [G loss: 3.141680]\n",
      "835 [D loss: 0.113592, acc.: 96.88%] [G loss: 3.458231]\n",
      "836 [D loss: 0.111122, acc.: 96.88%] [G loss: 3.401354]\n",
      "837 [D loss: 0.118111, acc.: 96.88%] [G loss: 3.382748]\n",
      "838 [D loss: 0.117853, acc.: 96.88%] [G loss: 3.309747]\n",
      "839 [D loss: 0.116022, acc.: 96.88%] [G loss: 3.394619]\n",
      "840 [D loss: 0.120832, acc.: 96.88%] [G loss: 3.292888]\n",
      "841 [D loss: 0.127304, acc.: 96.88%] [G loss: 3.136240]\n",
      "842 [D loss: 0.108921, acc.: 96.88%] [G loss: 3.166336]\n",
      "843 [D loss: 0.112609, acc.: 96.88%] [G loss: 3.267705]\n",
      "844 [D loss: 0.102908, acc.: 96.88%] [G loss: 3.509956]\n",
      "845 [D loss: 0.114549, acc.: 96.88%] [G loss: 3.263596]\n",
      "846 [D loss: 0.113740, acc.: 96.88%] [G loss: 3.278546]\n",
      "847 [D loss: 0.123169, acc.: 96.88%] [G loss: 3.427555]\n",
      "848 [D loss: 0.110727, acc.: 96.88%] [G loss: 3.376224]\n",
      "849 [D loss: 0.116686, acc.: 96.88%] [G loss: 3.415083]\n",
      "850 [D loss: 0.115851, acc.: 96.88%] [G loss: 3.358084]\n",
      "851 [D loss: 0.115830, acc.: 96.88%] [G loss: 3.012984]\n",
      "852 [D loss: 0.125945, acc.: 96.88%] [G loss: 3.245367]\n",
      "853 [D loss: 0.113703, acc.: 96.88%] [G loss: 3.017447]\n",
      "854 [D loss: 0.117270, acc.: 96.88%] [G loss: 3.124258]\n",
      "855 [D loss: 0.121672, acc.: 96.88%] [G loss: 3.315417]\n",
      "856 [D loss: 0.130341, acc.: 96.88%] [G loss: 3.432881]\n",
      "857 [D loss: 0.114285, acc.: 96.88%] [G loss: 3.266063]\n",
      "858 [D loss: 0.116632, acc.: 96.88%] [G loss: 3.244494]\n",
      "859 [D loss: 0.115559, acc.: 96.88%] [G loss: 3.471243]\n",
      "860 [D loss: 0.101095, acc.: 96.88%] [G loss: 3.400724]\n",
      "861 [D loss: 0.111163, acc.: 96.88%] [G loss: 3.118653]\n",
      "862 [D loss: 0.121332, acc.: 96.88%] [G loss: 3.152794]\n",
      "863 [D loss: 0.119559, acc.: 96.88%] [G loss: 3.302089]\n",
      "864 [D loss: 0.117628, acc.: 96.88%] [G loss: 3.428857]\n",
      "865 [D loss: 0.136485, acc.: 96.88%] [G loss: 3.301940]\n",
      "866 [D loss: 0.118081, acc.: 96.88%] [G loss: 3.166110]\n",
      "867 [D loss: 0.118674, acc.: 96.88%] [G loss: 3.145256]\n",
      "868 [D loss: 0.108621, acc.: 96.88%] [G loss: 3.461375]\n",
      "869 [D loss: 0.135354, acc.: 96.88%] [G loss: 3.149396]\n",
      "870 [D loss: 0.125628, acc.: 96.88%] [G loss: 3.399737]\n",
      "871 [D loss: 0.127206, acc.: 96.88%] [G loss: 3.331823]\n",
      "872 [D loss: 0.120723, acc.: 96.88%] [G loss: 3.068361]\n",
      "873 [D loss: 0.112580, acc.: 96.88%] [G loss: 3.279804]\n",
      "874 [D loss: 0.124089, acc.: 96.88%] [G loss: 3.042842]\n",
      "875 [D loss: 0.117698, acc.: 96.88%] [G loss: 3.237938]\n",
      "876 [D loss: 0.119866, acc.: 96.88%] [G loss: 3.103023]\n",
      "877 [D loss: 0.115087, acc.: 96.88%] [G loss: 2.923460]\n",
      "878 [D loss: 0.131709, acc.: 96.88%] [G loss: 3.281730]\n",
      "879 [D loss: 0.104373, acc.: 96.88%] [G loss: 3.211245]\n",
      "880 [D loss: 0.117780, acc.: 96.88%] [G loss: 3.233046]\n",
      "881 [D loss: 0.117138, acc.: 96.88%] [G loss: 3.289409]\n",
      "882 [D loss: 0.116722, acc.: 96.88%] [G loss: 3.443313]\n",
      "883 [D loss: 0.114716, acc.: 96.88%] [G loss: 3.576741]\n",
      "884 [D loss: 0.111256, acc.: 96.88%] [G loss: 3.171980]\n",
      "885 [D loss: 0.123278, acc.: 96.88%] [G loss: 3.053176]\n",
      "886 [D loss: 0.120864, acc.: 96.88%] [G loss: 3.356947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887 [D loss: 0.137764, acc.: 96.88%] [G loss: 3.040077]\n",
      "888 [D loss: 0.120485, acc.: 96.88%] [G loss: 3.081816]\n",
      "889 [D loss: 0.111428, acc.: 96.88%] [G loss: 3.043374]\n",
      "890 [D loss: 0.115101, acc.: 96.88%] [G loss: 3.140947]\n",
      "891 [D loss: 0.119027, acc.: 96.88%] [G loss: 3.345231]\n",
      "892 [D loss: 0.108736, acc.: 96.88%] [G loss: 3.114159]\n",
      "893 [D loss: 0.123674, acc.: 96.88%] [G loss: 3.301318]\n",
      "894 [D loss: 0.121204, acc.: 96.88%] [G loss: 3.180639]\n",
      "895 [D loss: 0.111214, acc.: 96.88%] [G loss: 3.298668]\n",
      "896 [D loss: 0.119931, acc.: 96.88%] [G loss: 2.969998]\n",
      "897 [D loss: 0.123618, acc.: 96.88%] [G loss: 3.119318]\n",
      "898 [D loss: 0.103248, acc.: 96.88%] [G loss: 3.042046]\n",
      "899 [D loss: 0.111341, acc.: 96.88%] [G loss: 3.126441]\n",
      "900 [D loss: 0.125868, acc.: 96.88%] [G loss: 3.113451]\n",
      "generated_data\n",
      "901 [D loss: 0.104994, acc.: 96.88%] [G loss: 3.169192]\n",
      "902 [D loss: 0.112583, acc.: 96.88%] [G loss: 3.242418]\n",
      "903 [D loss: 0.114289, acc.: 96.88%] [G loss: 3.114257]\n",
      "904 [D loss: 0.124385, acc.: 96.88%] [G loss: 3.446840]\n",
      "905 [D loss: 0.114913, acc.: 96.88%] [G loss: 3.129490]\n",
      "906 [D loss: 0.111985, acc.: 96.88%] [G loss: 3.136030]\n",
      "907 [D loss: 0.132136, acc.: 96.88%] [G loss: 3.191505]\n",
      "908 [D loss: 0.125790, acc.: 96.88%] [G loss: 3.069477]\n",
      "909 [D loss: 0.115099, acc.: 96.88%] [G loss: 3.221447]\n",
      "910 [D loss: 0.117290, acc.: 96.88%] [G loss: 3.290876]\n",
      "911 [D loss: 0.112860, acc.: 96.88%] [G loss: 3.347670]\n",
      "912 [D loss: 0.125180, acc.: 96.88%] [G loss: 3.107407]\n",
      "913 [D loss: 0.126635, acc.: 96.88%] [G loss: 3.094027]\n",
      "914 [D loss: 0.137420, acc.: 96.88%] [G loss: 3.159780]\n",
      "915 [D loss: 0.128979, acc.: 96.88%] [G loss: 3.299439]\n",
      "916 [D loss: 0.118870, acc.: 96.88%] [G loss: 3.116886]\n",
      "917 [D loss: 0.133705, acc.: 96.88%] [G loss: 3.498823]\n",
      "918 [D loss: 0.120880, acc.: 96.88%] [G loss: 3.134507]\n",
      "919 [D loss: 0.121592, acc.: 96.88%] [G loss: 3.116429]\n",
      "920 [D loss: 0.116731, acc.: 96.88%] [G loss: 3.092773]\n",
      "921 [D loss: 0.123051, acc.: 96.88%] [G loss: 2.921632]\n",
      "922 [D loss: 0.112598, acc.: 96.88%] [G loss: 3.300291]\n",
      "923 [D loss: 0.123442, acc.: 96.88%] [G loss: 3.153301]\n",
      "924 [D loss: 0.126268, acc.: 96.88%] [G loss: 3.295390]\n",
      "925 [D loss: 0.116575, acc.: 96.88%] [G loss: 3.162828]\n",
      "926 [D loss: 0.124736, acc.: 96.88%] [G loss: 3.245708]\n",
      "927 [D loss: 0.118520, acc.: 96.88%] [G loss: 3.257328]\n",
      "928 [D loss: 0.118370, acc.: 96.88%] [G loss: 3.143349]\n",
      "929 [D loss: 0.125768, acc.: 96.88%] [G loss: 3.058990]\n",
      "930 [D loss: 0.133101, acc.: 96.88%] [G loss: 3.170634]\n",
      "931 [D loss: 0.102833, acc.: 96.88%] [G loss: 3.428921]\n",
      "932 [D loss: 0.116300, acc.: 96.88%] [G loss: 3.289399]\n",
      "933 [D loss: 0.110002, acc.: 96.88%] [G loss: 3.263974]\n",
      "934 [D loss: 0.110691, acc.: 96.88%] [G loss: 3.279914]\n",
      "935 [D loss: 0.125714, acc.: 96.88%] [G loss: 3.090645]\n",
      "936 [D loss: 0.133384, acc.: 96.88%] [G loss: 2.945534]\n",
      "937 [D loss: 0.122653, acc.: 96.88%] [G loss: 2.957208]\n",
      "938 [D loss: 0.127045, acc.: 96.88%] [G loss: 3.204489]\n",
      "939 [D loss: 0.116816, acc.: 96.88%] [G loss: 3.103163]\n",
      "940 [D loss: 0.121632, acc.: 96.88%] [G loss: 3.073183]\n",
      "941 [D loss: 0.118427, acc.: 96.88%] [G loss: 3.216613]\n",
      "942 [D loss: 0.108443, acc.: 96.88%] [G loss: 3.047136]\n",
      "943 [D loss: 0.121825, acc.: 96.88%] [G loss: 3.078918]\n",
      "944 [D loss: 0.128437, acc.: 96.88%] [G loss: 3.242555]\n",
      "945 [D loss: 0.107517, acc.: 96.88%] [G loss: 3.091997]\n",
      "946 [D loss: 0.124612, acc.: 96.88%] [G loss: 3.276469]\n",
      "947 [D loss: 0.108732, acc.: 96.88%] [G loss: 3.187031]\n",
      "948 [D loss: 0.131512, acc.: 96.88%] [G loss: 3.116833]\n",
      "949 [D loss: 0.122228, acc.: 96.88%] [G loss: 3.002240]\n",
      "950 [D loss: 0.130187, acc.: 96.88%] [G loss: 2.948485]\n",
      "951 [D loss: 0.121852, acc.: 96.88%] [G loss: 2.965878]\n",
      "952 [D loss: 0.116804, acc.: 96.88%] [G loss: 3.037090]\n",
      "953 [D loss: 0.115147, acc.: 96.88%] [G loss: 2.962753]\n",
      "954 [D loss: 0.109427, acc.: 96.88%] [G loss: 2.997600]\n",
      "955 [D loss: 0.129621, acc.: 96.88%] [G loss: 3.089482]\n",
      "956 [D loss: 0.111874, acc.: 96.88%] [G loss: 3.304838]\n",
      "957 [D loss: 0.110975, acc.: 96.88%] [G loss: 3.207750]\n",
      "958 [D loss: 0.123337, acc.: 96.88%] [G loss: 3.140954]\n",
      "959 [D loss: 0.109114, acc.: 96.88%] [G loss: 2.957754]\n",
      "960 [D loss: 0.115643, acc.: 96.88%] [G loss: 3.105007]\n",
      "961 [D loss: 0.122354, acc.: 96.88%] [G loss: 3.156243]\n",
      "962 [D loss: 0.116441, acc.: 96.88%] [G loss: 3.266816]\n",
      "963 [D loss: 0.128752, acc.: 96.88%] [G loss: 3.496465]\n",
      "964 [D loss: 0.129800, acc.: 96.88%] [G loss: 3.430019]\n",
      "965 [D loss: 0.134972, acc.: 96.88%] [G loss: 3.221814]\n",
      "966 [D loss: 0.106259, acc.: 96.88%] [G loss: 2.961065]\n",
      "967 [D loss: 0.120435, acc.: 96.88%] [G loss: 3.297590]\n",
      "968 [D loss: 0.128050, acc.: 96.88%] [G loss: 3.103805]\n",
      "969 [D loss: 0.123227, acc.: 96.88%] [G loss: 3.323571]\n",
      "970 [D loss: 0.108571, acc.: 96.88%] [G loss: 3.205927]\n",
      "971 [D loss: 0.131598, acc.: 96.88%] [G loss: 3.048437]\n",
      "972 [D loss: 0.118540, acc.: 96.88%] [G loss: 3.038373]\n",
      "973 [D loss: 0.123166, acc.: 96.88%] [G loss: 3.319587]\n",
      "974 [D loss: 0.122333, acc.: 96.88%] [G loss: 3.128708]\n",
      "975 [D loss: 0.131810, acc.: 96.88%] [G loss: 3.198340]\n",
      "976 [D loss: 0.124922, acc.: 96.88%] [G loss: 2.827611]\n",
      "977 [D loss: 0.112164, acc.: 96.88%] [G loss: 3.302046]\n",
      "978 [D loss: 0.129015, acc.: 96.88%] [G loss: 3.137148]\n",
      "979 [D loss: 0.117339, acc.: 96.88%] [G loss: 3.212470]\n",
      "980 [D loss: 0.108063, acc.: 96.88%] [G loss: 3.200204]\n",
      "981 [D loss: 0.135218, acc.: 96.88%] [G loss: 2.980531]\n",
      "982 [D loss: 0.138208, acc.: 96.88%] [G loss: 3.177957]\n",
      "983 [D loss: 0.121438, acc.: 96.88%] [G loss: 3.234256]\n",
      "984 [D loss: 0.126810, acc.: 96.88%] [G loss: 3.139541]\n",
      "985 [D loss: 0.125546, acc.: 96.88%] [G loss: 2.925266]\n",
      "986 [D loss: 0.108921, acc.: 96.88%] [G loss: 3.294202]\n",
      "987 [D loss: 0.130180, acc.: 96.88%] [G loss: 3.079798]\n",
      "988 [D loss: 0.116047, acc.: 96.88%] [G loss: 3.078807]\n",
      "989 [D loss: 0.127498, acc.: 96.88%] [G loss: 3.162521]\n",
      "990 [D loss: 0.115643, acc.: 96.88%] [G loss: 3.146255]\n",
      "991 [D loss: 0.131931, acc.: 96.88%] [G loss: 3.162951]\n",
      "992 [D loss: 0.113089, acc.: 96.88%] [G loss: 3.016628]\n",
      "993 [D loss: 0.123835, acc.: 96.88%] [G loss: 3.026203]\n",
      "994 [D loss: 0.111500, acc.: 96.88%] [G loss: 3.078412]\n",
      "995 [D loss: 0.129590, acc.: 96.88%] [G loss: 3.069109]\n",
      "996 [D loss: 0.107470, acc.: 96.88%] [G loss: 3.060820]\n",
      "997 [D loss: 0.126472, acc.: 96.88%] [G loss: 3.092984]\n",
      "998 [D loss: 0.122636, acc.: 96.88%] [G loss: 3.325042]\n",
      "999 [D loss: 0.127968, acc.: 96.88%] [G loss: 3.129690]\n",
      "1000 [D loss: 0.123082, acc.: 96.88%] [G loss: 3.126888]\n",
      "generated_data\n",
      "1001 [D loss: 0.117303, acc.: 96.88%] [G loss: 3.076163]\n",
      "1002 [D loss: 0.128880, acc.: 96.88%] [G loss: 3.143613]\n",
      "1003 [D loss: 0.113572, acc.: 96.88%] [G loss: 3.018946]\n",
      "1004 [D loss: 0.121996, acc.: 96.88%] [G loss: 3.050243]\n",
      "1005 [D loss: 0.131140, acc.: 96.88%] [G loss: 3.153148]\n",
      "1006 [D loss: 0.115555, acc.: 96.88%] [G loss: 3.156422]\n",
      "1007 [D loss: 0.123468, acc.: 96.88%] [G loss: 3.235403]\n",
      "1008 [D loss: 0.118058, acc.: 96.88%] [G loss: 3.211160]\n",
      "1009 [D loss: 0.117201, acc.: 96.88%] [G loss: 3.177820]\n",
      "1010 [D loss: 0.117789, acc.: 96.88%] [G loss: 3.182309]\n",
      "1011 [D loss: 0.121438, acc.: 96.88%] [G loss: 3.142354]\n",
      "1012 [D loss: 0.124816, acc.: 96.88%] [G loss: 3.132419]\n",
      "1013 [D loss: 0.111240, acc.: 96.88%] [G loss: 3.190868]\n",
      "1014 [D loss: 0.141035, acc.: 96.88%] [G loss: 2.997294]\n",
      "1015 [D loss: 0.119411, acc.: 96.88%] [G loss: 3.018884]\n",
      "1016 [D loss: 0.117911, acc.: 96.88%] [G loss: 3.040112]\n",
      "1017 [D loss: 0.135340, acc.: 96.88%] [G loss: 3.101206]\n",
      "1018 [D loss: 0.124244, acc.: 96.88%] [G loss: 3.119355]\n",
      "1019 [D loss: 0.119999, acc.: 96.88%] [G loss: 3.209434]\n",
      "1020 [D loss: 0.123979, acc.: 96.88%] [G loss: 3.338773]\n",
      "1021 [D loss: 0.104504, acc.: 96.88%] [G loss: 3.405949]\n",
      "1022 [D loss: 0.113566, acc.: 96.88%] [G loss: 3.312332]\n",
      "1023 [D loss: 0.103225, acc.: 96.88%] [G loss: 3.112840]\n",
      "1024 [D loss: 0.112984, acc.: 96.88%] [G loss: 3.103079]\n",
      "1025 [D loss: 0.119396, acc.: 96.88%] [G loss: 3.299408]\n",
      "1026 [D loss: 0.126282, acc.: 96.88%] [G loss: 3.307594]\n",
      "1027 [D loss: 0.113220, acc.: 96.88%] [G loss: 3.386319]\n",
      "1028 [D loss: 0.119501, acc.: 96.88%] [G loss: 3.252843]\n",
      "1029 [D loss: 0.113933, acc.: 96.88%] [G loss: 3.172808]\n",
      "1030 [D loss: 0.126863, acc.: 96.88%] [G loss: 3.209570]\n",
      "1031 [D loss: 0.119467, acc.: 96.88%] [G loss: 3.239932]\n",
      "1032 [D loss: 0.120670, acc.: 96.88%] [G loss: 3.369091]\n",
      "1033 [D loss: 0.109718, acc.: 96.88%] [G loss: 3.439563]\n",
      "1034 [D loss: 0.104628, acc.: 96.88%] [G loss: 3.268342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035 [D loss: 0.132604, acc.: 96.88%] [G loss: 3.151080]\n",
      "1036 [D loss: 0.110937, acc.: 96.88%] [G loss: 3.252217]\n",
      "1037 [D loss: 0.120231, acc.: 96.88%] [G loss: 3.283926]\n",
      "1038 [D loss: 0.124210, acc.: 96.88%] [G loss: 3.019502]\n",
      "1039 [D loss: 0.110234, acc.: 96.88%] [G loss: 3.094932]\n",
      "1040 [D loss: 0.131687, acc.: 96.88%] [G loss: 3.413404]\n",
      "1041 [D loss: 0.109129, acc.: 96.88%] [G loss: 3.097010]\n",
      "1042 [D loss: 0.123044, acc.: 96.88%] [G loss: 3.145256]\n",
      "1043 [D loss: 0.110144, acc.: 96.88%] [G loss: 3.287959]\n",
      "1044 [D loss: 0.125915, acc.: 96.88%] [G loss: 3.051069]\n",
      "1045 [D loss: 0.124228, acc.: 96.88%] [G loss: 3.216193]\n",
      "1046 [D loss: 0.134021, acc.: 96.88%] [G loss: 3.002704]\n",
      "1047 [D loss: 0.118019, acc.: 96.88%] [G loss: 2.980660]\n",
      "1048 [D loss: 0.123779, acc.: 96.88%] [G loss: 2.888381]\n",
      "1049 [D loss: 0.126511, acc.: 96.88%] [G loss: 3.372101]\n",
      "1050 [D loss: 0.117269, acc.: 96.88%] [G loss: 3.035359]\n",
      "1051 [D loss: 0.125295, acc.: 96.88%] [G loss: 3.100919]\n",
      "1052 [D loss: 0.119857, acc.: 96.88%] [G loss: 3.137207]\n",
      "1053 [D loss: 0.120506, acc.: 96.88%] [G loss: 3.180490]\n",
      "1054 [D loss: 0.124920, acc.: 96.88%] [G loss: 3.161132]\n",
      "1055 [D loss: 0.118109, acc.: 96.88%] [G loss: 3.145993]\n",
      "1056 [D loss: 0.115718, acc.: 96.88%] [G loss: 3.261517]\n",
      "1057 [D loss: 0.118441, acc.: 96.88%] [G loss: 2.948970]\n",
      "1058 [D loss: 0.116059, acc.: 96.88%] [G loss: 3.301438]\n",
      "1059 [D loss: 0.117482, acc.: 96.88%] [G loss: 3.254089]\n",
      "1060 [D loss: 0.116175, acc.: 96.88%] [G loss: 3.300647]\n",
      "1061 [D loss: 0.121737, acc.: 96.88%] [G loss: 3.079950]\n",
      "1062 [D loss: 0.121778, acc.: 96.88%] [G loss: 3.147529]\n",
      "1063 [D loss: 0.103729, acc.: 96.88%] [G loss: 3.240811]\n",
      "1064 [D loss: 0.111959, acc.: 96.88%] [G loss: 3.273338]\n",
      "1065 [D loss: 0.123564, acc.: 96.88%] [G loss: 3.318944]\n",
      "1066 [D loss: 0.097959, acc.: 96.88%] [G loss: 3.371675]\n",
      "1067 [D loss: 0.126557, acc.: 96.88%] [G loss: 3.186379]\n",
      "1068 [D loss: 0.118089, acc.: 96.88%] [G loss: 3.255094]\n",
      "1069 [D loss: 0.125502, acc.: 96.88%] [G loss: 3.045477]\n",
      "1070 [D loss: 0.130725, acc.: 96.88%] [G loss: 3.107998]\n",
      "1071 [D loss: 0.120821, acc.: 96.88%] [G loss: 2.884604]\n",
      "1072 [D loss: 0.113217, acc.: 96.88%] [G loss: 3.087263]\n",
      "1073 [D loss: 0.133546, acc.: 96.88%] [G loss: 3.094898]\n",
      "1074 [D loss: 0.130368, acc.: 96.88%] [G loss: 3.059612]\n",
      "1075 [D loss: 0.114562, acc.: 96.88%] [G loss: 3.040216]\n",
      "1076 [D loss: 0.125801, acc.: 96.88%] [G loss: 3.243198]\n",
      "1077 [D loss: 0.119236, acc.: 96.88%] [G loss: 3.568821]\n",
      "1078 [D loss: 0.118154, acc.: 96.88%] [G loss: 3.003964]\n",
      "1079 [D loss: 0.113938, acc.: 96.88%] [G loss: 3.122643]\n",
      "1080 [D loss: 0.122507, acc.: 96.88%] [G loss: 3.122808]\n",
      "1081 [D loss: 0.110751, acc.: 96.88%] [G loss: 3.299509]\n",
      "1082 [D loss: 0.118885, acc.: 96.88%] [G loss: 3.132333]\n",
      "1083 [D loss: 0.122899, acc.: 96.88%] [G loss: 3.000020]\n",
      "1084 [D loss: 0.129515, acc.: 96.88%] [G loss: 3.198553]\n",
      "1085 [D loss: 0.125674, acc.: 96.88%] [G loss: 3.465728]\n",
      "1086 [D loss: 0.102451, acc.: 96.88%] [G loss: 3.261252]\n",
      "1087 [D loss: 0.131953, acc.: 96.88%] [G loss: 3.178916]\n",
      "1088 [D loss: 0.123652, acc.: 96.88%] [G loss: 3.060679]\n",
      "1089 [D loss: 0.125377, acc.: 96.88%] [G loss: 3.058735]\n",
      "1090 [D loss: 0.126999, acc.: 96.88%] [G loss: 3.130796]\n",
      "1091 [D loss: 0.119861, acc.: 96.88%] [G loss: 3.117923]\n",
      "1092 [D loss: 0.133604, acc.: 96.88%] [G loss: 2.978628]\n",
      "1093 [D loss: 0.123548, acc.: 96.88%] [G loss: 3.153086]\n",
      "1094 [D loss: 0.124929, acc.: 96.88%] [G loss: 3.175712]\n",
      "1095 [D loss: 0.121855, acc.: 96.88%] [G loss: 3.070243]\n",
      "1096 [D loss: 0.127838, acc.: 96.88%] [G loss: 3.044753]\n",
      "1097 [D loss: 0.129647, acc.: 96.88%] [G loss: 3.274278]\n",
      "1098 [D loss: 0.120412, acc.: 96.88%] [G loss: 3.219157]\n",
      "1099 [D loss: 0.123600, acc.: 96.88%] [G loss: 3.027363]\n",
      "1100 [D loss: 0.120715, acc.: 96.88%] [G loss: 3.023414]\n",
      "generated_data\n",
      "1101 [D loss: 0.119406, acc.: 96.88%] [G loss: 3.065036]\n",
      "1102 [D loss: 0.111532, acc.: 96.88%] [G loss: 3.241216]\n",
      "1103 [D loss: 0.119992, acc.: 96.88%] [G loss: 3.128374]\n",
      "1104 [D loss: 0.119618, acc.: 96.88%] [G loss: 3.143510]\n",
      "1105 [D loss: 0.133062, acc.: 96.88%] [G loss: 3.021336]\n",
      "1106 [D loss: 0.133568, acc.: 96.88%] [G loss: 3.124346]\n",
      "1107 [D loss: 0.138429, acc.: 96.88%] [G loss: 3.187854]\n",
      "1108 [D loss: 0.114591, acc.: 96.88%] [G loss: 3.145547]\n",
      "1109 [D loss: 0.128317, acc.: 96.88%] [G loss: 3.310372]\n",
      "1110 [D loss: 0.122531, acc.: 96.88%] [G loss: 3.010509]\n",
      "1111 [D loss: 0.125155, acc.: 96.88%] [G loss: 3.051339]\n",
      "1112 [D loss: 0.121840, acc.: 96.88%] [G loss: 3.132878]\n",
      "1113 [D loss: 0.127034, acc.: 96.88%] [G loss: 3.105515]\n",
      "1114 [D loss: 0.113108, acc.: 96.88%] [G loss: 3.248359]\n",
      "1115 [D loss: 0.125740, acc.: 96.88%] [G loss: 3.123330]\n",
      "1116 [D loss: 0.104977, acc.: 96.88%] [G loss: 3.181850]\n",
      "1117 [D loss: 0.118722, acc.: 96.88%] [G loss: 3.123920]\n",
      "1118 [D loss: 0.131484, acc.: 96.88%] [G loss: 3.138812]\n",
      "1119 [D loss: 0.125556, acc.: 96.88%] [G loss: 3.212390]\n",
      "1120 [D loss: 0.124314, acc.: 96.88%] [G loss: 3.137750]\n",
      "1121 [D loss: 0.112203, acc.: 96.88%] [G loss: 3.088136]\n",
      "1122 [D loss: 0.115371, acc.: 96.88%] [G loss: 2.886549]\n",
      "1123 [D loss: 0.118146, acc.: 96.88%] [G loss: 3.201382]\n",
      "1124 [D loss: 0.125695, acc.: 96.88%] [G loss: 3.053198]\n",
      "1125 [D loss: 0.106710, acc.: 96.88%] [G loss: 3.128356]\n",
      "1126 [D loss: 0.123527, acc.: 96.88%] [G loss: 3.212757]\n",
      "1127 [D loss: 0.132271, acc.: 96.88%] [G loss: 3.233292]\n",
      "1128 [D loss: 0.121171, acc.: 96.88%] [G loss: 3.205486]\n",
      "1129 [D loss: 0.117513, acc.: 96.88%] [G loss: 3.223157]\n",
      "1130 [D loss: 0.132920, acc.: 96.88%] [G loss: 3.179041]\n",
      "1131 [D loss: 0.124760, acc.: 96.88%] [G loss: 3.358440]\n",
      "1132 [D loss: 0.107188, acc.: 96.88%] [G loss: 3.302419]\n",
      "1133 [D loss: 0.111824, acc.: 96.88%] [G loss: 3.239795]\n",
      "1134 [D loss: 0.120207, acc.: 96.88%] [G loss: 3.046414]\n",
      "1135 [D loss: 0.119943, acc.: 96.88%] [G loss: 3.121475]\n",
      "1136 [D loss: 0.109472, acc.: 96.88%] [G loss: 3.251157]\n",
      "1137 [D loss: 0.110834, acc.: 96.88%] [G loss: 3.056767]\n",
      "1138 [D loss: 0.124288, acc.: 96.88%] [G loss: 3.256023]\n",
      "1139 [D loss: 0.126621, acc.: 96.88%] [G loss: 2.925889]\n",
      "1140 [D loss: 0.130605, acc.: 96.88%] [G loss: 3.050306]\n",
      "1141 [D loss: 0.130190, acc.: 96.88%] [G loss: 3.151588]\n",
      "1142 [D loss: 0.115744, acc.: 96.88%] [G loss: 3.072589]\n",
      "1143 [D loss: 0.116694, acc.: 96.88%] [G loss: 3.123075]\n",
      "1144 [D loss: 0.117853, acc.: 96.88%] [G loss: 3.147857]\n",
      "1145 [D loss: 0.112072, acc.: 96.88%] [G loss: 3.154593]\n",
      "1146 [D loss: 0.137060, acc.: 96.88%] [G loss: 3.030709]\n",
      "1147 [D loss: 0.116964, acc.: 96.88%] [G loss: 2.997343]\n",
      "1148 [D loss: 0.131873, acc.: 96.88%] [G loss: 3.014004]\n",
      "1149 [D loss: 0.116920, acc.: 96.88%] [G loss: 3.081749]\n",
      "1150 [D loss: 0.118247, acc.: 96.88%] [G loss: 2.985887]\n",
      "1151 [D loss: 0.120086, acc.: 96.88%] [G loss: 2.956120]\n",
      "1152 [D loss: 0.127854, acc.: 96.88%] [G loss: 3.027995]\n",
      "1153 [D loss: 0.130946, acc.: 96.88%] [G loss: 3.060177]\n",
      "1154 [D loss: 0.114034, acc.: 96.88%] [G loss: 3.065512]\n",
      "1155 [D loss: 0.125430, acc.: 96.88%] [G loss: 3.112997]\n",
      "1156 [D loss: 0.111567, acc.: 96.88%] [G loss: 3.046691]\n",
      "1157 [D loss: 0.118319, acc.: 96.88%] [G loss: 3.094858]\n",
      "1158 [D loss: 0.135480, acc.: 96.88%] [G loss: 3.083124]\n",
      "1159 [D loss: 0.120883, acc.: 96.88%] [G loss: 2.991656]\n",
      "1160 [D loss: 0.117800, acc.: 96.88%] [G loss: 3.108837]\n",
      "1161 [D loss: 0.131302, acc.: 96.88%] [G loss: 3.108465]\n",
      "1162 [D loss: 0.119436, acc.: 96.88%] [G loss: 3.155144]\n",
      "1163 [D loss: 0.123579, acc.: 96.88%] [G loss: 3.166429]\n",
      "1164 [D loss: 0.116581, acc.: 96.88%] [G loss: 3.094711]\n",
      "1165 [D loss: 0.121339, acc.: 96.88%] [G loss: 3.073051]\n",
      "1166 [D loss: 0.121181, acc.: 96.88%] [G loss: 3.133837]\n",
      "1167 [D loss: 0.130790, acc.: 96.88%] [G loss: 2.912309]\n",
      "1168 [D loss: 0.128330, acc.: 96.88%] [G loss: 2.992479]\n",
      "1169 [D loss: 0.127975, acc.: 96.88%] [G loss: 3.164201]\n",
      "1170 [D loss: 0.111968, acc.: 96.88%] [G loss: 3.024174]\n",
      "1171 [D loss: 0.119374, acc.: 96.88%] [G loss: 3.058498]\n",
      "1172 [D loss: 0.119958, acc.: 96.88%] [G loss: 3.082092]\n",
      "1173 [D loss: 0.127267, acc.: 96.88%] [G loss: 3.091604]\n",
      "1174 [D loss: 0.131959, acc.: 96.88%] [G loss: 3.223108]\n",
      "1175 [D loss: 0.119932, acc.: 96.88%] [G loss: 3.150126]\n",
      "1176 [D loss: 0.128758, acc.: 96.88%] [G loss: 3.123290]\n",
      "1177 [D loss: 0.120285, acc.: 96.88%] [G loss: 3.090834]\n",
      "1178 [D loss: 0.114041, acc.: 96.88%] [G loss: 3.260058]\n",
      "1179 [D loss: 0.114112, acc.: 96.88%] [G loss: 3.007574]\n",
      "1180 [D loss: 0.125014, acc.: 96.88%] [G loss: 3.134909]\n",
      "1181 [D loss: 0.117200, acc.: 96.88%] [G loss: 3.188296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182 [D loss: 0.120944, acc.: 96.88%] [G loss: 3.104501]\n",
      "1183 [D loss: 0.128807, acc.: 96.88%] [G loss: 2.977248]\n",
      "1184 [D loss: 0.113654, acc.: 96.88%] [G loss: 3.189396]\n",
      "1185 [D loss: 0.122581, acc.: 96.88%] [G loss: 3.000370]\n",
      "1186 [D loss: 0.125372, acc.: 96.88%] [G loss: 3.164428]\n",
      "1187 [D loss: 0.148959, acc.: 96.88%] [G loss: 2.991182]\n",
      "1188 [D loss: 0.117328, acc.: 96.88%] [G loss: 2.961967]\n",
      "1189 [D loss: 0.131737, acc.: 96.88%] [G loss: 3.093035]\n",
      "1190 [D loss: 0.116342, acc.: 96.88%] [G loss: 3.142863]\n",
      "1191 [D loss: 0.116044, acc.: 96.88%] [G loss: 3.073631]\n",
      "1192 [D loss: 0.120271, acc.: 96.88%] [G loss: 3.029041]\n",
      "1193 [D loss: 0.120614, acc.: 96.88%] [G loss: 3.073665]\n",
      "1194 [D loss: 0.118699, acc.: 96.88%] [G loss: 3.070960]\n",
      "1195 [D loss: 0.126546, acc.: 96.88%] [G loss: 3.048575]\n",
      "1196 [D loss: 0.126122, acc.: 96.88%] [G loss: 3.119235]\n",
      "1197 [D loss: 0.126139, acc.: 96.88%] [G loss: 3.068768]\n",
      "1198 [D loss: 0.122790, acc.: 96.88%] [G loss: 3.093731]\n",
      "1199 [D loss: 0.121183, acc.: 96.88%] [G loss: 3.248776]\n",
      "1200 [D loss: 0.128939, acc.: 96.88%] [G loss: 3.343196]\n",
      "generated_data\n",
      "1201 [D loss: 0.115215, acc.: 96.88%] [G loss: 3.252958]\n",
      "1202 [D loss: 0.112400, acc.: 96.88%] [G loss: 3.420375]\n",
      "1203 [D loss: 0.119708, acc.: 96.88%] [G loss: 3.099000]\n",
      "1204 [D loss: 0.125836, acc.: 96.88%] [G loss: 3.087362]\n",
      "1205 [D loss: 0.119917, acc.: 96.88%] [G loss: 3.021221]\n",
      "1206 [D loss: 0.126775, acc.: 96.88%] [G loss: 3.020323]\n",
      "1207 [D loss: 0.118982, acc.: 96.88%] [G loss: 3.179842]\n",
      "1208 [D loss: 0.123619, acc.: 96.88%] [G loss: 3.016021]\n",
      "1209 [D loss: 0.119686, acc.: 96.88%] [G loss: 2.931749]\n",
      "1210 [D loss: 0.127964, acc.: 96.88%] [G loss: 2.958771]\n",
      "1211 [D loss: 0.117803, acc.: 96.88%] [G loss: 2.823953]\n",
      "1212 [D loss: 0.121061, acc.: 96.88%] [G loss: 3.134665]\n",
      "1213 [D loss: 0.138397, acc.: 96.88%] [G loss: 3.116581]\n",
      "1214 [D loss: 0.116306, acc.: 96.88%] [G loss: 3.224499]\n",
      "1215 [D loss: 0.124098, acc.: 96.88%] [G loss: 3.188601]\n",
      "1216 [D loss: 0.124042, acc.: 96.88%] [G loss: 3.194280]\n",
      "1217 [D loss: 0.119083, acc.: 96.88%] [G loss: 2.942676]\n",
      "1218 [D loss: 0.123314, acc.: 96.88%] [G loss: 2.964707]\n",
      "1219 [D loss: 0.113370, acc.: 96.88%] [G loss: 3.084028]\n",
      "1220 [D loss: 0.130645, acc.: 96.88%] [G loss: 3.040617]\n",
      "1221 [D loss: 0.123402, acc.: 96.88%] [G loss: 3.144291]\n",
      "1222 [D loss: 0.111631, acc.: 96.88%] [G loss: 3.082540]\n",
      "1223 [D loss: 0.122322, acc.: 96.88%] [G loss: 3.187432]\n",
      "1224 [D loss: 0.125238, acc.: 96.88%] [G loss: 3.190817]\n",
      "1225 [D loss: 0.127141, acc.: 96.88%] [G loss: 3.054309]\n",
      "1226 [D loss: 0.131169, acc.: 96.88%] [G loss: 2.955960]\n",
      "1227 [D loss: 0.111472, acc.: 96.88%] [G loss: 3.099953]\n",
      "1228 [D loss: 0.124658, acc.: 96.88%] [G loss: 3.101818]\n",
      "1229 [D loss: 0.126549, acc.: 96.88%] [G loss: 3.094578]\n",
      "1230 [D loss: 0.125070, acc.: 96.88%] [G loss: 3.070150]\n",
      "1231 [D loss: 0.120425, acc.: 96.88%] [G loss: 3.077645]\n",
      "1232 [D loss: 0.117966, acc.: 96.88%] [G loss: 3.129142]\n",
      "1233 [D loss: 0.117030, acc.: 96.88%] [G loss: 2.973891]\n",
      "1234 [D loss: 0.116835, acc.: 96.88%] [G loss: 3.082620]\n",
      "1235 [D loss: 0.126009, acc.: 96.88%] [G loss: 3.105596]\n",
      "1236 [D loss: 0.121330, acc.: 96.88%] [G loss: 3.102307]\n",
      "1237 [D loss: 0.122418, acc.: 96.88%] [G loss: 3.184301]\n",
      "1238 [D loss: 0.122280, acc.: 96.88%] [G loss: 2.991513]\n",
      "1239 [D loss: 0.109244, acc.: 96.88%] [G loss: 3.128418]\n",
      "1240 [D loss: 0.114442, acc.: 96.88%] [G loss: 2.991374]\n",
      "1241 [D loss: 0.122918, acc.: 96.88%] [G loss: 2.958041]\n",
      "1242 [D loss: 0.125916, acc.: 96.88%] [G loss: 3.057198]\n",
      "1243 [D loss: 0.124509, acc.: 96.88%] [G loss: 3.179614]\n",
      "1244 [D loss: 0.115646, acc.: 96.88%] [G loss: 3.016011]\n",
      "1245 [D loss: 0.130091, acc.: 96.88%] [G loss: 3.190501]\n",
      "1246 [D loss: 0.115505, acc.: 96.88%] [G loss: 3.085309]\n",
      "1247 [D loss: 0.123485, acc.: 96.88%] [G loss: 3.100507]\n",
      "1248 [D loss: 0.119409, acc.: 96.88%] [G loss: 3.032043]\n",
      "1249 [D loss: 0.117854, acc.: 96.88%] [G loss: 3.109017]\n",
      "1250 [D loss: 0.113714, acc.: 96.88%] [G loss: 3.015030]\n",
      "1251 [D loss: 0.114115, acc.: 96.88%] [G loss: 3.165004]\n",
      "1252 [D loss: 0.121328, acc.: 96.88%] [G loss: 3.095478]\n",
      "1253 [D loss: 0.112158, acc.: 96.88%] [G loss: 3.174861]\n",
      "1254 [D loss: 0.123112, acc.: 96.88%] [G loss: 3.139272]\n",
      "1255 [D loss: 0.122623, acc.: 96.88%] [G loss: 3.259880]\n",
      "1256 [D loss: 0.115895, acc.: 96.88%] [G loss: 3.087418]\n",
      "1257 [D loss: 0.119243, acc.: 96.88%] [G loss: 3.057396]\n",
      "1258 [D loss: 0.127152, acc.: 96.88%] [G loss: 3.010582]\n",
      "1259 [D loss: 0.108212, acc.: 96.88%] [G loss: 2.921932]\n",
      "1260 [D loss: 0.124404, acc.: 96.88%] [G loss: 3.060052]\n",
      "1261 [D loss: 0.126074, acc.: 96.88%] [G loss: 3.168865]\n",
      "1262 [D loss: 0.116582, acc.: 96.88%] [G loss: 3.020366]\n",
      "1263 [D loss: 0.126226, acc.: 96.88%] [G loss: 3.178136]\n",
      "1264 [D loss: 0.127365, acc.: 96.88%] [G loss: 3.123725]\n",
      "1265 [D loss: 0.130156, acc.: 96.88%] [G loss: 3.040977]\n",
      "1266 [D loss: 0.124055, acc.: 96.88%] [G loss: 3.027851]\n",
      "1267 [D loss: 0.115555, acc.: 96.88%] [G loss: 3.072511]\n",
      "1268 [D loss: 0.126786, acc.: 96.88%] [G loss: 2.965990]\n",
      "1269 [D loss: 0.131669, acc.: 96.88%] [G loss: 3.031377]\n",
      "1270 [D loss: 0.105970, acc.: 96.88%] [G loss: 3.047087]\n",
      "1271 [D loss: 0.113863, acc.: 96.88%] [G loss: 3.116693]\n",
      "1272 [D loss: 0.124149, acc.: 96.88%] [G loss: 3.246910]\n",
      "1273 [D loss: 0.131534, acc.: 96.88%] [G loss: 3.093508]\n",
      "1274 [D loss: 0.118752, acc.: 96.88%] [G loss: 3.076679]\n",
      "1275 [D loss: 0.120988, acc.: 96.88%] [G loss: 3.228792]\n",
      "1276 [D loss: 0.122251, acc.: 96.88%] [G loss: 3.134358]\n",
      "1277 [D loss: 0.123631, acc.: 96.88%] [G loss: 2.969572]\n",
      "1278 [D loss: 0.118012, acc.: 96.88%] [G loss: 3.104753]\n",
      "1279 [D loss: 0.121296, acc.: 96.88%] [G loss: 2.980144]\n",
      "1280 [D loss: 0.129927, acc.: 96.88%] [G loss: 3.065940]\n",
      "1281 [D loss: 0.113818, acc.: 96.88%] [G loss: 3.185837]\n",
      "1282 [D loss: 0.126283, acc.: 96.88%] [G loss: 3.024197]\n",
      "1283 [D loss: 0.114584, acc.: 96.88%] [G loss: 3.078519]\n",
      "1284 [D loss: 0.135989, acc.: 96.88%] [G loss: 3.007777]\n",
      "1285 [D loss: 0.119869, acc.: 96.88%] [G loss: 3.077838]\n",
      "1286 [D loss: 0.114661, acc.: 96.88%] [G loss: 3.236169]\n",
      "1287 [D loss: 0.120426, acc.: 96.88%] [G loss: 3.153123]\n",
      "1288 [D loss: 0.115764, acc.: 96.88%] [G loss: 3.036126]\n",
      "1289 [D loss: 0.122011, acc.: 96.88%] [G loss: 3.217129]\n",
      "1290 [D loss: 0.126841, acc.: 96.88%] [G loss: 3.066939]\n",
      "1291 [D loss: 0.125337, acc.: 96.88%] [G loss: 3.169620]\n",
      "1292 [D loss: 0.140845, acc.: 96.88%] [G loss: 2.932834]\n",
      "1293 [D loss: 0.112569, acc.: 96.88%] [G loss: 3.137617]\n",
      "1294 [D loss: 0.116056, acc.: 96.88%] [G loss: 3.188527]\n",
      "1295 [D loss: 0.125809, acc.: 96.88%] [G loss: 3.152329]\n",
      "1296 [D loss: 0.127855, acc.: 96.88%] [G loss: 3.281483]\n",
      "1297 [D loss: 0.119224, acc.: 96.88%] [G loss: 3.150796]\n",
      "1298 [D loss: 0.116334, acc.: 96.88%] [G loss: 3.110213]\n",
      "1299 [D loss: 0.123292, acc.: 96.88%] [G loss: 3.097294]\n",
      "1300 [D loss: 0.119966, acc.: 96.88%] [G loss: 3.060685]\n",
      "generated_data\n",
      "1301 [D loss: 0.135660, acc.: 96.88%] [G loss: 2.912108]\n",
      "1302 [D loss: 0.116536, acc.: 96.88%] [G loss: 3.012765]\n",
      "1303 [D loss: 0.129077, acc.: 96.88%] [G loss: 3.004206]\n",
      "1304 [D loss: 0.119181, acc.: 96.88%] [G loss: 2.948864]\n",
      "1305 [D loss: 0.121142, acc.: 96.88%] [G loss: 3.095182]\n",
      "1306 [D loss: 0.120951, acc.: 96.88%] [G loss: 3.173017]\n",
      "1307 [D loss: 0.115026, acc.: 96.88%] [G loss: 3.105891]\n",
      "1308 [D loss: 0.127138, acc.: 96.88%] [G loss: 3.125681]\n",
      "1309 [D loss: 0.118795, acc.: 96.88%] [G loss: 3.136889]\n",
      "1310 [D loss: 0.124009, acc.: 96.88%] [G loss: 3.182429]\n",
      "1311 [D loss: 0.115755, acc.: 96.88%] [G loss: 3.262102]\n",
      "1312 [D loss: 0.120389, acc.: 96.88%] [G loss: 3.016146]\n",
      "1313 [D loss: 0.122180, acc.: 96.88%] [G loss: 3.009578]\n",
      "1314 [D loss: 0.128739, acc.: 96.88%] [G loss: 3.121913]\n",
      "1315 [D loss: 0.115891, acc.: 96.88%] [G loss: 3.088407]\n",
      "1316 [D loss: 0.120426, acc.: 96.88%] [G loss: 3.040189]\n",
      "1317 [D loss: 0.122090, acc.: 96.88%] [G loss: 3.063758]\n",
      "1318 [D loss: 0.120007, acc.: 96.88%] [G loss: 3.015618]\n",
      "1319 [D loss: 0.117801, acc.: 96.88%] [G loss: 2.987185]\n",
      "1320 [D loss: 0.124195, acc.: 96.88%] [G loss: 3.023944]\n",
      "1321 [D loss: 0.118368, acc.: 96.88%] [G loss: 2.981472]\n",
      "1322 [D loss: 0.116918, acc.: 96.88%] [G loss: 3.075781]\n",
      "1323 [D loss: 0.128315, acc.: 96.88%] [G loss: 3.062845]\n",
      "1324 [D loss: 0.130493, acc.: 96.88%] [G loss: 2.991016]\n",
      "1325 [D loss: 0.122282, acc.: 96.88%] [G loss: 2.947766]\n",
      "1326 [D loss: 0.118638, acc.: 96.88%] [G loss: 3.143581]\n",
      "1327 [D loss: 0.118946, acc.: 96.88%] [G loss: 3.029119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328 [D loss: 0.124206, acc.: 96.88%] [G loss: 3.065410]\n",
      "1329 [D loss: 0.121304, acc.: 96.88%] [G loss: 3.093221]\n",
      "1330 [D loss: 0.127930, acc.: 96.88%] [G loss: 3.003982]\n",
      "1331 [D loss: 0.128147, acc.: 96.88%] [G loss: 2.980987]\n",
      "1332 [D loss: 0.123803, acc.: 96.88%] [G loss: 2.978892]\n",
      "1333 [D loss: 0.126408, acc.: 96.88%] [G loss: 3.081456]\n",
      "1334 [D loss: 0.119410, acc.: 96.88%] [G loss: 3.002504]\n",
      "1335 [D loss: 0.111087, acc.: 96.88%] [G loss: 3.082932]\n",
      "1336 [D loss: 0.128220, acc.: 96.88%] [G loss: 2.977286]\n",
      "1337 [D loss: 0.118925, acc.: 96.88%] [G loss: 2.917094]\n",
      "1338 [D loss: 0.118070, acc.: 96.88%] [G loss: 3.092198]\n",
      "1339 [D loss: 0.116769, acc.: 96.88%] [G loss: 2.951088]\n",
      "1340 [D loss: 0.109918, acc.: 96.88%] [G loss: 3.035975]\n",
      "1341 [D loss: 0.117786, acc.: 96.88%] [G loss: 3.174753]\n",
      "1342 [D loss: 0.118827, acc.: 96.88%] [G loss: 3.219580]\n",
      "1343 [D loss: 0.127198, acc.: 96.88%] [G loss: 3.103705]\n",
      "1344 [D loss: 0.122775, acc.: 96.88%] [G loss: 2.961123]\n",
      "1345 [D loss: 0.129242, acc.: 96.88%] [G loss: 3.033718]\n",
      "1346 [D loss: 0.128190, acc.: 96.88%] [G loss: 3.151953]\n",
      "1347 [D loss: 0.122056, acc.: 96.88%] [G loss: 3.049003]\n",
      "1348 [D loss: 0.113385, acc.: 96.88%] [G loss: 3.304216]\n",
      "1349 [D loss: 0.122339, acc.: 96.88%] [G loss: 3.336223]\n",
      "1350 [D loss: 0.123843, acc.: 96.88%] [G loss: 3.023784]\n",
      "1351 [D loss: 0.129189, acc.: 96.88%] [G loss: 2.973133]\n",
      "1352 [D loss: 0.119323, acc.: 96.88%] [G loss: 3.135981]\n",
      "1353 [D loss: 0.107662, acc.: 96.88%] [G loss: 3.140779]\n",
      "1354 [D loss: 0.124631, acc.: 96.88%] [G loss: 2.947746]\n",
      "1355 [D loss: 0.125099, acc.: 96.88%] [G loss: 3.053696]\n",
      "1356 [D loss: 0.118368, acc.: 96.88%] [G loss: 2.983792]\n",
      "1357 [D loss: 0.115894, acc.: 96.88%] [G loss: 3.107778]\n",
      "1358 [D loss: 0.120629, acc.: 96.88%] [G loss: 3.098738]\n",
      "1359 [D loss: 0.123173, acc.: 96.88%] [G loss: 3.036222]\n",
      "1360 [D loss: 0.132656, acc.: 96.88%] [G loss: 3.040586]\n",
      "1361 [D loss: 0.107903, acc.: 96.88%] [G loss: 3.071680]\n",
      "1362 [D loss: 0.128088, acc.: 96.88%] [G loss: 3.094864]\n",
      "1363 [D loss: 0.121526, acc.: 96.88%] [G loss: 2.938798]\n",
      "1364 [D loss: 0.123395, acc.: 96.88%] [G loss: 2.906796]\n",
      "1365 [D loss: 0.123544, acc.: 96.88%] [G loss: 3.101529]\n",
      "1366 [D loss: 0.124894, acc.: 96.88%] [G loss: 2.979035]\n",
      "1367 [D loss: 0.118367, acc.: 96.88%] [G loss: 3.036958]\n",
      "1368 [D loss: 0.130942, acc.: 96.88%] [G loss: 3.060079]\n",
      "1369 [D loss: 0.120280, acc.: 96.88%] [G loss: 3.106044]\n",
      "1370 [D loss: 0.109300, acc.: 96.88%] [G loss: 3.046220]\n",
      "1371 [D loss: 0.115540, acc.: 96.88%] [G loss: 3.254431]\n",
      "1372 [D loss: 0.119757, acc.: 96.88%] [G loss: 3.057968]\n",
      "1373 [D loss: 0.113501, acc.: 96.88%] [G loss: 3.053781]\n",
      "1374 [D loss: 0.121057, acc.: 96.88%] [G loss: 3.023253]\n",
      "1375 [D loss: 0.127402, acc.: 96.88%] [G loss: 3.003901]\n",
      "1376 [D loss: 0.129518, acc.: 96.88%] [G loss: 3.021601]\n",
      "1377 [D loss: 0.124752, acc.: 96.88%] [G loss: 2.997645]\n",
      "1378 [D loss: 0.132538, acc.: 96.88%] [G loss: 2.903679]\n",
      "1379 [D loss: 0.118392, acc.: 96.88%] [G loss: 3.125192]\n",
      "1380 [D loss: 0.120509, acc.: 96.88%] [G loss: 3.152196]\n",
      "1381 [D loss: 0.121257, acc.: 96.88%] [G loss: 3.031275]\n",
      "1382 [D loss: 0.121020, acc.: 96.88%] [G loss: 3.003908]\n",
      "1383 [D loss: 0.125007, acc.: 96.88%] [G loss: 3.321362]\n",
      "1384 [D loss: 0.122986, acc.: 96.88%] [G loss: 3.064754]\n",
      "1385 [D loss: 0.118791, acc.: 96.88%] [G loss: 3.189122]\n",
      "1386 [D loss: 0.120450, acc.: 96.88%] [G loss: 3.134373]\n",
      "1387 [D loss: 0.122487, acc.: 96.88%] [G loss: 3.104214]\n",
      "1388 [D loss: 0.130833, acc.: 96.88%] [G loss: 3.218375]\n",
      "1389 [D loss: 0.114190, acc.: 96.88%] [G loss: 3.061191]\n",
      "1390 [D loss: 0.120526, acc.: 96.88%] [G loss: 2.973325]\n",
      "1391 [D loss: 0.125034, acc.: 96.88%] [G loss: 3.247335]\n",
      "1392 [D loss: 0.111364, acc.: 96.88%] [G loss: 3.052228]\n",
      "1393 [D loss: 0.131740, acc.: 96.88%] [G loss: 2.927135]\n",
      "1394 [D loss: 0.124542, acc.: 96.88%] [G loss: 3.013947]\n",
      "1395 [D loss: 0.121482, acc.: 96.88%] [G loss: 3.048374]\n",
      "1396 [D loss: 0.114153, acc.: 96.88%] [G loss: 3.231334]\n",
      "1397 [D loss: 0.121261, acc.: 96.88%] [G loss: 3.003237]\n",
      "1398 [D loss: 0.114404, acc.: 96.88%] [G loss: 3.086807]\n",
      "1399 [D loss: 0.132352, acc.: 96.88%] [G loss: 3.019528]\n",
      "1400 [D loss: 0.111837, acc.: 96.88%] [G loss: 3.103277]\n",
      "generated_data\n",
      "1401 [D loss: 0.124785, acc.: 96.88%] [G loss: 2.970010]\n",
      "1402 [D loss: 0.122120, acc.: 96.88%] [G loss: 3.063006]\n",
      "1403 [D loss: 0.134773, acc.: 96.88%] [G loss: 2.988753]\n",
      "1404 [D loss: 0.112529, acc.: 96.88%] [G loss: 3.141732]\n",
      "1405 [D loss: 0.115163, acc.: 96.88%] [G loss: 3.098120]\n",
      "1406 [D loss: 0.122354, acc.: 96.88%] [G loss: 3.170617]\n",
      "1407 [D loss: 0.131060, acc.: 96.88%] [G loss: 3.019270]\n",
      "1408 [D loss: 0.118153, acc.: 96.88%] [G loss: 3.033509]\n",
      "1409 [D loss: 0.126413, acc.: 96.88%] [G loss: 3.140966]\n",
      "1410 [D loss: 0.110769, acc.: 96.88%] [G loss: 3.136779]\n",
      "1411 [D loss: 0.118517, acc.: 96.88%] [G loss: 3.099500]\n",
      "1412 [D loss: 0.121530, acc.: 96.88%] [G loss: 3.176053]\n",
      "1413 [D loss: 0.118533, acc.: 96.88%] [G loss: 3.164593]\n",
      "1414 [D loss: 0.129224, acc.: 96.88%] [G loss: 3.178559]\n",
      "1415 [D loss: 0.128262, acc.: 96.88%] [G loss: 3.002941]\n",
      "1416 [D loss: 0.118457, acc.: 96.88%] [G loss: 3.015664]\n",
      "1417 [D loss: 0.119465, acc.: 96.88%] [G loss: 3.094293]\n",
      "1418 [D loss: 0.120762, acc.: 96.88%] [G loss: 2.942954]\n",
      "1419 [D loss: 0.128825, acc.: 96.88%] [G loss: 3.108206]\n",
      "1420 [D loss: 0.119056, acc.: 96.88%] [G loss: 3.068222]\n",
      "1421 [D loss: 0.129354, acc.: 96.88%] [G loss: 3.068009]\n",
      "1422 [D loss: 0.130199, acc.: 96.88%] [G loss: 3.130671]\n",
      "1423 [D loss: 0.112974, acc.: 96.88%] [G loss: 2.891934]\n",
      "1424 [D loss: 0.117550, acc.: 96.88%] [G loss: 2.960993]\n",
      "1425 [D loss: 0.118136, acc.: 96.88%] [G loss: 2.982960]\n",
      "1426 [D loss: 0.134243, acc.: 96.88%] [G loss: 2.979492]\n",
      "1427 [D loss: 0.115714, acc.: 96.88%] [G loss: 3.019314]\n",
      "1428 [D loss: 0.113751, acc.: 96.88%] [G loss: 2.996365]\n",
      "1429 [D loss: 0.127878, acc.: 96.88%] [G loss: 3.227514]\n",
      "1430 [D loss: 0.133274, acc.: 96.88%] [G loss: 2.893776]\n",
      "1431 [D loss: 0.120123, acc.: 96.88%] [G loss: 3.078051]\n",
      "1432 [D loss: 0.118538, acc.: 96.88%] [G loss: 3.039308]\n",
      "1433 [D loss: 0.131623, acc.: 96.88%] [G loss: 2.962715]\n",
      "1434 [D loss: 0.123679, acc.: 96.88%] [G loss: 2.998434]\n",
      "1435 [D loss: 0.133377, acc.: 96.88%] [G loss: 3.035165]\n",
      "1436 [D loss: 0.128296, acc.: 96.88%] [G loss: 2.959471]\n",
      "1437 [D loss: 0.125836, acc.: 96.88%] [G loss: 2.881649]\n",
      "1438 [D loss: 0.126433, acc.: 96.88%] [G loss: 2.956365]\n",
      "1439 [D loss: 0.118614, acc.: 96.88%] [G loss: 2.950126]\n",
      "1440 [D loss: 0.128221, acc.: 96.88%] [G loss: 2.931126]\n",
      "1441 [D loss: 0.115272, acc.: 96.88%] [G loss: 2.976062]\n",
      "1442 [D loss: 0.118502, acc.: 96.88%] [G loss: 3.140090]\n",
      "1443 [D loss: 0.121486, acc.: 96.88%] [G loss: 2.997223]\n",
      "1444 [D loss: 0.114247, acc.: 96.88%] [G loss: 3.126595]\n",
      "1445 [D loss: 0.114578, acc.: 96.88%] [G loss: 3.123751]\n",
      "1446 [D loss: 0.118742, acc.: 96.88%] [G loss: 3.101478]\n",
      "1447 [D loss: 0.120159, acc.: 96.88%] [G loss: 3.065804]\n",
      "1448 [D loss: 0.120958, acc.: 96.88%] [G loss: 3.090667]\n",
      "1449 [D loss: 0.128232, acc.: 96.88%] [G loss: 3.055288]\n",
      "1450 [D loss: 0.122666, acc.: 96.88%] [G loss: 3.139871]\n",
      "1451 [D loss: 0.119168, acc.: 96.88%] [G loss: 3.201022]\n",
      "1452 [D loss: 0.129235, acc.: 96.88%] [G loss: 2.980238]\n",
      "1453 [D loss: 0.116107, acc.: 96.88%] [G loss: 3.059265]\n",
      "1454 [D loss: 0.122426, acc.: 96.88%] [G loss: 3.247416]\n",
      "1455 [D loss: 0.115548, acc.: 96.88%] [G loss: 3.091554]\n",
      "1456 [D loss: 0.118807, acc.: 96.88%] [G loss: 3.033813]\n",
      "1457 [D loss: 0.118278, acc.: 96.88%] [G loss: 3.188030]\n",
      "1458 [D loss: 0.125185, acc.: 96.88%] [G loss: 3.031844]\n",
      "1459 [D loss: 0.127888, acc.: 96.88%] [G loss: 3.113685]\n",
      "1460 [D loss: 0.125315, acc.: 96.88%] [G loss: 3.193744]\n",
      "1461 [D loss: 0.124865, acc.: 96.88%] [G loss: 3.080990]\n",
      "1462 [D loss: 0.111415, acc.: 96.88%] [G loss: 2.976985]\n",
      "1463 [D loss: 0.123919, acc.: 96.88%] [G loss: 3.126085]\n",
      "1464 [D loss: 0.116039, acc.: 96.88%] [G loss: 3.217348]\n",
      "1465 [D loss: 0.131842, acc.: 96.88%] [G loss: 2.902857]\n",
      "1466 [D loss: 0.113863, acc.: 96.88%] [G loss: 2.942575]\n",
      "1467 [D loss: 0.109671, acc.: 96.88%] [G loss: 3.010673]\n",
      "1468 [D loss: 0.115764, acc.: 96.88%] [G loss: 3.066154]\n",
      "1469 [D loss: 0.120212, acc.: 96.88%] [G loss: 3.263076]\n",
      "1470 [D loss: 0.124867, acc.: 96.88%] [G loss: 3.222607]\n",
      "1471 [D loss: 0.138661, acc.: 96.88%] [G loss: 3.042252]\n",
      "1472 [D loss: 0.131926, acc.: 96.88%] [G loss: 2.982742]\n",
      "1473 [D loss: 0.120291, acc.: 96.88%] [G loss: 3.093559]\n",
      "1474 [D loss: 0.130398, acc.: 96.88%] [G loss: 3.054825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475 [D loss: 0.118229, acc.: 96.88%] [G loss: 3.162626]\n",
      "1476 [D loss: 0.114943, acc.: 96.88%] [G loss: 2.969464]\n",
      "1477 [D loss: 0.120055, acc.: 96.88%] [G loss: 2.953535]\n",
      "1478 [D loss: 0.123667, acc.: 96.88%] [G loss: 2.998813]\n",
      "1479 [D loss: 0.128685, acc.: 96.88%] [G loss: 2.951546]\n",
      "1480 [D loss: 0.127798, acc.: 96.88%] [G loss: 2.884235]\n",
      "1481 [D loss: 0.121656, acc.: 96.88%] [G loss: 2.973410]\n",
      "1482 [D loss: 0.116747, acc.: 96.88%] [G loss: 2.919504]\n",
      "1483 [D loss: 0.118574, acc.: 96.88%] [G loss: 2.926872]\n",
      "1484 [D loss: 0.121905, acc.: 96.88%] [G loss: 2.992541]\n",
      "1485 [D loss: 0.118449, acc.: 96.88%] [G loss: 3.117038]\n",
      "1486 [D loss: 0.111186, acc.: 96.88%] [G loss: 3.002707]\n",
      "1487 [D loss: 0.122562, acc.: 96.88%] [G loss: 3.226738]\n",
      "1488 [D loss: 0.115909, acc.: 96.88%] [G loss: 3.115022]\n",
      "1489 [D loss: 0.131054, acc.: 96.88%] [G loss: 2.972281]\n",
      "1490 [D loss: 0.125193, acc.: 96.88%] [G loss: 3.003314]\n",
      "1491 [D loss: 0.131739, acc.: 96.88%] [G loss: 3.076813]\n",
      "1492 [D loss: 0.121231, acc.: 96.88%] [G loss: 3.023584]\n",
      "1493 [D loss: 0.119481, acc.: 96.88%] [G loss: 3.047996]\n",
      "1494 [D loss: 0.121929, acc.: 96.88%] [G loss: 3.167174]\n",
      "1495 [D loss: 0.130182, acc.: 96.88%] [G loss: 3.094510]\n",
      "1496 [D loss: 0.115443, acc.: 96.88%] [G loss: 2.911657]\n",
      "1497 [D loss: 0.124088, acc.: 96.88%] [G loss: 3.035493]\n",
      "1498 [D loss: 0.120293, acc.: 96.88%] [G loss: 3.086779]\n",
      "1499 [D loss: 0.129225, acc.: 96.88%] [G loss: 2.974652]\n",
      "1500 [D loss: 0.122534, acc.: 96.88%] [G loss: 2.884298]\n",
      "generated_data\n",
      "1501 [D loss: 0.127824, acc.: 96.88%] [G loss: 2.933734]\n",
      "1502 [D loss: 0.122480, acc.: 96.88%] [G loss: 3.016607]\n",
      "1503 [D loss: 0.132724, acc.: 96.88%] [G loss: 3.068894]\n",
      "1504 [D loss: 0.128928, acc.: 96.88%] [G loss: 2.954383]\n",
      "1505 [D loss: 0.124280, acc.: 96.88%] [G loss: 2.998962]\n",
      "1506 [D loss: 0.117833, acc.: 96.88%] [G loss: 3.163670]\n",
      "1507 [D loss: 0.117281, acc.: 96.88%] [G loss: 3.014886]\n",
      "1508 [D loss: 0.115259, acc.: 96.88%] [G loss: 3.059965]\n",
      "1509 [D loss: 0.132725, acc.: 96.88%] [G loss: 2.868377]\n",
      "1510 [D loss: 0.122855, acc.: 96.88%] [G loss: 3.003777]\n",
      "1511 [D loss: 0.123856, acc.: 96.88%] [G loss: 2.971758]\n",
      "1512 [D loss: 0.127346, acc.: 96.88%] [G loss: 3.043101]\n",
      "1513 [D loss: 0.123127, acc.: 96.88%] [G loss: 3.037790]\n",
      "1514 [D loss: 0.123390, acc.: 96.88%] [G loss: 3.031593]\n",
      "1515 [D loss: 0.112403, acc.: 96.88%] [G loss: 3.070145]\n",
      "1516 [D loss: 0.125626, acc.: 96.88%] [G loss: 3.013802]\n",
      "1517 [D loss: 0.127095, acc.: 96.88%] [G loss: 3.123551]\n",
      "1518 [D loss: 0.132708, acc.: 96.88%] [G loss: 2.996590]\n",
      "1519 [D loss: 0.123223, acc.: 96.88%] [G loss: 3.052521]\n",
      "1520 [D loss: 0.115172, acc.: 96.88%] [G loss: 2.897445]\n",
      "1521 [D loss: 0.121484, acc.: 96.88%] [G loss: 2.888043]\n",
      "1522 [D loss: 0.121214, acc.: 96.88%] [G loss: 2.978353]\n",
      "1523 [D loss: 0.118862, acc.: 96.88%] [G loss: 2.994772]\n",
      "1524 [D loss: 0.127710, acc.: 96.88%] [G loss: 2.978687]\n",
      "1525 [D loss: 0.122221, acc.: 96.88%] [G loss: 3.070192]\n",
      "1526 [D loss: 0.119359, acc.: 96.88%] [G loss: 3.069074]\n",
      "1527 [D loss: 0.117147, acc.: 96.88%] [G loss: 3.088845]\n",
      "1528 [D loss: 0.112192, acc.: 96.88%] [G loss: 2.985811]\n",
      "1529 [D loss: 0.120692, acc.: 96.88%] [G loss: 3.004568]\n",
      "1530 [D loss: 0.116075, acc.: 96.88%] [G loss: 3.114969]\n",
      "1531 [D loss: 0.123239, acc.: 96.88%] [G loss: 2.890322]\n",
      "1532 [D loss: 0.115679, acc.: 96.88%] [G loss: 2.990519]\n",
      "1533 [D loss: 0.123106, acc.: 96.88%] [G loss: 3.035779]\n",
      "1534 [D loss: 0.112411, acc.: 96.88%] [G loss: 3.068080]\n",
      "1535 [D loss: 0.116006, acc.: 96.88%] [G loss: 3.023865]\n",
      "1536 [D loss: 0.121656, acc.: 96.88%] [G loss: 3.064295]\n",
      "1537 [D loss: 0.114168, acc.: 96.88%] [G loss: 2.914052]\n",
      "1538 [D loss: 0.110846, acc.: 96.88%] [G loss: 2.974972]\n",
      "1539 [D loss: 0.124362, acc.: 96.88%] [G loss: 3.125778]\n",
      "1540 [D loss: 0.124442, acc.: 96.88%] [G loss: 2.922217]\n",
      "1541 [D loss: 0.122469, acc.: 96.88%] [G loss: 2.995369]\n",
      "1542 [D loss: 0.140538, acc.: 96.88%] [G loss: 2.995115]\n",
      "1543 [D loss: 0.118748, acc.: 96.88%] [G loss: 2.975093]\n",
      "1544 [D loss: 0.127802, acc.: 96.88%] [G loss: 2.916154]\n",
      "1545 [D loss: 0.127558, acc.: 96.88%] [G loss: 2.935469]\n",
      "1546 [D loss: 0.123933, acc.: 96.88%] [G loss: 2.955086]\n",
      "1547 [D loss: 0.124489, acc.: 96.88%] [G loss: 2.969185]\n",
      "1548 [D loss: 0.123280, acc.: 96.88%] [G loss: 3.014370]\n",
      "1549 [D loss: 0.123439, acc.: 96.88%] [G loss: 3.016013]\n",
      "1550 [D loss: 0.123591, acc.: 96.88%] [G loss: 3.036212]\n",
      "1551 [D loss: 0.108911, acc.: 96.88%] [G loss: 3.115070]\n",
      "1552 [D loss: 0.131244, acc.: 96.88%] [G loss: 2.932081]\n",
      "1553 [D loss: 0.119280, acc.: 96.88%] [G loss: 3.015242]\n",
      "1554 [D loss: 0.120692, acc.: 96.88%] [G loss: 2.943753]\n",
      "1555 [D loss: 0.128251, acc.: 96.88%] [G loss: 2.904629]\n",
      "1556 [D loss: 0.122395, acc.: 96.88%] [G loss: 3.011704]\n",
      "1557 [D loss: 0.119487, acc.: 96.88%] [G loss: 3.069750]\n",
      "1558 [D loss: 0.135205, acc.: 96.88%] [G loss: 3.071897]\n",
      "1559 [D loss: 0.129977, acc.: 96.88%] [G loss: 3.044381]\n",
      "1560 [D loss: 0.117406, acc.: 96.88%] [G loss: 2.995822]\n",
      "1561 [D loss: 0.123513, acc.: 96.88%] [G loss: 2.971637]\n",
      "1562 [D loss: 0.130922, acc.: 96.88%] [G loss: 3.022365]\n",
      "1563 [D loss: 0.129977, acc.: 96.88%] [G loss: 3.040578]\n",
      "1564 [D loss: 0.109372, acc.: 96.88%] [G loss: 2.989954]\n",
      "1565 [D loss: 0.117233, acc.: 96.88%] [G loss: 3.080318]\n",
      "1566 [D loss: 0.118746, acc.: 96.88%] [G loss: 3.101053]\n",
      "1567 [D loss: 0.120820, acc.: 96.88%] [G loss: 3.100814]\n",
      "1568 [D loss: 0.126446, acc.: 96.88%] [G loss: 3.046027]\n",
      "1569 [D loss: 0.109793, acc.: 96.88%] [G loss: 3.018384]\n",
      "1570 [D loss: 0.124162, acc.: 96.88%] [G loss: 2.893409]\n",
      "1571 [D loss: 0.126963, acc.: 96.88%] [G loss: 3.047983]\n",
      "1572 [D loss: 0.122131, acc.: 96.88%] [G loss: 3.115595]\n",
      "1573 [D loss: 0.111748, acc.: 96.88%] [G loss: 3.007382]\n",
      "1574 [D loss: 0.124832, acc.: 96.88%] [G loss: 3.001219]\n",
      "1575 [D loss: 0.123892, acc.: 96.88%] [G loss: 2.964923]\n",
      "1576 [D loss: 0.125111, acc.: 96.88%] [G loss: 2.903438]\n",
      "1577 [D loss: 0.121340, acc.: 96.88%] [G loss: 2.880667]\n",
      "1578 [D loss: 0.125259, acc.: 96.88%] [G loss: 2.955011]\n",
      "1579 [D loss: 0.112640, acc.: 96.88%] [G loss: 3.179310]\n",
      "1580 [D loss: 0.128548, acc.: 96.88%] [G loss: 3.045598]\n",
      "1581 [D loss: 0.120488, acc.: 96.88%] [G loss: 2.935810]\n",
      "1582 [D loss: 0.127193, acc.: 96.88%] [G loss: 3.011497]\n",
      "1583 [D loss: 0.122433, acc.: 96.88%] [G loss: 2.853023]\n",
      "1584 [D loss: 0.121569, acc.: 96.88%] [G loss: 2.908882]\n",
      "1585 [D loss: 0.120786, acc.: 96.88%] [G loss: 3.048785]\n",
      "1586 [D loss: 0.123657, acc.: 96.88%] [G loss: 3.046545]\n",
      "1587 [D loss: 0.131693, acc.: 96.88%] [G loss: 3.047642]\n",
      "1588 [D loss: 0.111846, acc.: 96.88%] [G loss: 3.093862]\n",
      "1589 [D loss: 0.124740, acc.: 96.88%] [G loss: 3.200160]\n",
      "1590 [D loss: 0.114666, acc.: 96.88%] [G loss: 2.920478]\n",
      "1591 [D loss: 0.122501, acc.: 96.88%] [G loss: 3.040070]\n",
      "1592 [D loss: 0.114201, acc.: 96.88%] [G loss: 2.961854]\n",
      "1593 [D loss: 0.128144, acc.: 96.88%] [G loss: 3.080583]\n",
      "1594 [D loss: 0.126875, acc.: 96.88%] [G loss: 3.121674]\n",
      "1595 [D loss: 0.133353, acc.: 96.88%] [G loss: 2.991325]\n",
      "1596 [D loss: 0.125910, acc.: 96.88%] [G loss: 3.048135]\n",
      "1597 [D loss: 0.124753, acc.: 96.88%] [G loss: 3.171113]\n",
      "1598 [D loss: 0.122171, acc.: 96.88%] [G loss: 3.030953]\n",
      "1599 [D loss: 0.129016, acc.: 96.88%] [G loss: 3.165867]\n",
      "1600 [D loss: 0.127440, acc.: 96.88%] [G loss: 2.925990]\n",
      "generated_data\n",
      "1601 [D loss: 0.115719, acc.: 96.88%] [G loss: 3.009551]\n",
      "1602 [D loss: 0.116646, acc.: 96.88%] [G loss: 2.970069]\n",
      "1603 [D loss: 0.123012, acc.: 96.88%] [G loss: 3.121497]\n",
      "1604 [D loss: 0.120620, acc.: 96.88%] [G loss: 3.056389]\n",
      "1605 [D loss: 0.125490, acc.: 96.88%] [G loss: 3.012965]\n",
      "1606 [D loss: 0.117764, acc.: 96.88%] [G loss: 2.986524]\n",
      "1607 [D loss: 0.125127, acc.: 96.88%] [G loss: 2.901276]\n",
      "1608 [D loss: 0.126273, acc.: 96.88%] [G loss: 2.906015]\n",
      "1609 [D loss: 0.130665, acc.: 96.88%] [G loss: 2.936289]\n",
      "1610 [D loss: 0.121862, acc.: 96.88%] [G loss: 3.039779]\n",
      "1611 [D loss: 0.125983, acc.: 96.88%] [G loss: 2.940407]\n",
      "1612 [D loss: 0.118183, acc.: 96.88%] [G loss: 3.033397]\n",
      "1613 [D loss: 0.128578, acc.: 96.88%] [G loss: 3.028368]\n",
      "1614 [D loss: 0.118317, acc.: 96.88%] [G loss: 2.884610]\n",
      "1615 [D loss: 0.122688, acc.: 96.88%] [G loss: 3.110941]\n",
      "1616 [D loss: 0.118347, acc.: 96.88%] [G loss: 3.042953]\n",
      "1617 [D loss: 0.128138, acc.: 96.88%] [G loss: 2.981716]\n",
      "1618 [D loss: 0.121338, acc.: 96.88%] [G loss: 2.966950]\n",
      "1619 [D loss: 0.127845, acc.: 96.88%] [G loss: 3.059904]\n",
      "1620 [D loss: 0.111103, acc.: 96.88%] [G loss: 3.005304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1621 [D loss: 0.116648, acc.: 96.88%] [G loss: 3.032943]\n",
      "1622 [D loss: 0.117106, acc.: 96.88%] [G loss: 2.909998]\n",
      "1623 [D loss: 0.120642, acc.: 96.88%] [G loss: 3.017443]\n",
      "1624 [D loss: 0.128334, acc.: 96.88%] [G loss: 3.135300]\n",
      "1625 [D loss: 0.139835, acc.: 96.88%] [G loss: 2.894840]\n",
      "1626 [D loss: 0.125682, acc.: 96.88%] [G loss: 2.901748]\n",
      "1627 [D loss: 0.116552, acc.: 96.88%] [G loss: 2.914171]\n",
      "1628 [D loss: 0.119212, acc.: 96.88%] [G loss: 2.909162]\n",
      "1629 [D loss: 0.121599, acc.: 96.88%] [G loss: 2.828847]\n",
      "1630 [D loss: 0.113244, acc.: 96.88%] [G loss: 3.013106]\n",
      "1631 [D loss: 0.122875, acc.: 96.88%] [G loss: 2.858821]\n",
      "1632 [D loss: 0.126081, acc.: 96.88%] [G loss: 3.093437]\n",
      "1633 [D loss: 0.117633, acc.: 96.88%] [G loss: 3.125465]\n",
      "1634 [D loss: 0.128439, acc.: 96.88%] [G loss: 2.996421]\n",
      "1635 [D loss: 0.114828, acc.: 96.88%] [G loss: 3.072651]\n",
      "1636 [D loss: 0.128737, acc.: 96.88%] [G loss: 3.025923]\n",
      "1637 [D loss: 0.123579, acc.: 96.88%] [G loss: 2.931931]\n",
      "1638 [D loss: 0.116352, acc.: 96.88%] [G loss: 3.075733]\n",
      "1639 [D loss: 0.123295, acc.: 96.88%] [G loss: 3.055542]\n",
      "1640 [D loss: 0.124395, acc.: 96.88%] [G loss: 3.229779]\n",
      "1641 [D loss: 0.113424, acc.: 96.88%] [G loss: 3.081494]\n",
      "1642 [D loss: 0.129744, acc.: 96.88%] [G loss: 3.086863]\n",
      "1643 [D loss: 0.126309, acc.: 96.88%] [G loss: 2.984298]\n",
      "1644 [D loss: 0.122635, acc.: 96.88%] [G loss: 2.978759]\n",
      "1645 [D loss: 0.124198, acc.: 96.88%] [G loss: 2.993114]\n",
      "1646 [D loss: 0.121770, acc.: 96.88%] [G loss: 3.046179]\n",
      "1647 [D loss: 0.123599, acc.: 96.88%] [G loss: 3.109609]\n",
      "1648 [D loss: 0.118897, acc.: 96.88%] [G loss: 3.179451]\n",
      "1649 [D loss: 0.124523, acc.: 96.88%] [G loss: 2.999794]\n",
      "1650 [D loss: 0.125829, acc.: 96.88%] [G loss: 3.001383]\n",
      "1651 [D loss: 0.127042, acc.: 96.88%] [G loss: 2.817869]\n",
      "1652 [D loss: 0.134718, acc.: 96.88%] [G loss: 2.879455]\n",
      "1653 [D loss: 0.127885, acc.: 96.88%] [G loss: 2.985470]\n",
      "1654 [D loss: 0.111459, acc.: 96.88%] [G loss: 3.051804]\n",
      "1655 [D loss: 0.123784, acc.: 96.88%] [G loss: 3.064042]\n",
      "1656 [D loss: 0.116238, acc.: 96.88%] [G loss: 3.097518]\n",
      "1657 [D loss: 0.119038, acc.: 96.88%] [G loss: 2.970543]\n",
      "1658 [D loss: 0.127681, acc.: 96.88%] [G loss: 2.986048]\n",
      "1659 [D loss: 0.118989, acc.: 96.88%] [G loss: 3.053190]\n",
      "1660 [D loss: 0.118325, acc.: 96.88%] [G loss: 3.051263]\n",
      "1661 [D loss: 0.117345, acc.: 96.88%] [G loss: 3.186848]\n",
      "1662 [D loss: 0.126125, acc.: 96.88%] [G loss: 3.023216]\n",
      "1663 [D loss: 0.117504, acc.: 96.88%] [G loss: 2.919987]\n",
      "1664 [D loss: 0.125741, acc.: 96.88%] [G loss: 2.804298]\n",
      "1665 [D loss: 0.116833, acc.: 96.88%] [G loss: 3.012972]\n",
      "1666 [D loss: 0.119697, acc.: 96.88%] [G loss: 2.974835]\n",
      "1667 [D loss: 0.119796, acc.: 96.88%] [G loss: 2.999054]\n",
      "1668 [D loss: 0.125599, acc.: 96.88%] [G loss: 3.079063]\n",
      "1669 [D loss: 0.114352, acc.: 96.88%] [G loss: 3.132412]\n",
      "1670 [D loss: 0.113545, acc.: 96.88%] [G loss: 2.935650]\n",
      "1671 [D loss: 0.131029, acc.: 96.88%] [G loss: 2.937092]\n",
      "1672 [D loss: 0.126967, acc.: 96.88%] [G loss: 2.974086]\n",
      "1673 [D loss: 0.128587, acc.: 96.88%] [G loss: 2.855744]\n",
      "1674 [D loss: 0.116292, acc.: 96.88%] [G loss: 2.852564]\n",
      "1675 [D loss: 0.115988, acc.: 96.88%] [G loss: 2.929719]\n",
      "1676 [D loss: 0.117165, acc.: 96.88%] [G loss: 3.001555]\n",
      "1677 [D loss: 0.125997, acc.: 96.88%] [G loss: 3.062639]\n",
      "1678 [D loss: 0.131713, acc.: 96.88%] [G loss: 2.959280]\n",
      "1679 [D loss: 0.119646, acc.: 96.88%] [G loss: 3.093235]\n",
      "1680 [D loss: 0.118321, acc.: 96.88%] [G loss: 2.958948]\n",
      "1681 [D loss: 0.124993, acc.: 96.88%] [G loss: 2.963166]\n",
      "1682 [D loss: 0.124104, acc.: 96.88%] [G loss: 3.027622]\n",
      "1683 [D loss: 0.119501, acc.: 96.88%] [G loss: 2.925024]\n",
      "1684 [D loss: 0.133980, acc.: 96.88%] [G loss: 2.944414]\n",
      "1685 [D loss: 0.126905, acc.: 96.88%] [G loss: 3.056658]\n",
      "1686 [D loss: 0.113217, acc.: 96.88%] [G loss: 3.104279]\n",
      "1687 [D loss: 0.118204, acc.: 96.88%] [G loss: 3.032380]\n",
      "1688 [D loss: 0.120854, acc.: 96.88%] [G loss: 2.957236]\n",
      "1689 [D loss: 0.115416, acc.: 96.88%] [G loss: 2.979100]\n",
      "1690 [D loss: 0.130947, acc.: 96.88%] [G loss: 3.017112]\n",
      "1691 [D loss: 0.123915, acc.: 96.88%] [G loss: 3.112376]\n",
      "1692 [D loss: 0.113634, acc.: 96.88%] [G loss: 3.088089]\n",
      "1693 [D loss: 0.124754, acc.: 96.88%] [G loss: 2.977235]\n",
      "1694 [D loss: 0.119527, acc.: 96.88%] [G loss: 3.027165]\n",
      "1695 [D loss: 0.127173, acc.: 96.88%] [G loss: 3.167478]\n",
      "1696 [D loss: 0.123486, acc.: 96.88%] [G loss: 3.207389]\n",
      "1697 [D loss: 0.117416, acc.: 96.88%] [G loss: 2.924386]\n",
      "1698 [D loss: 0.123364, acc.: 96.88%] [G loss: 2.813915]\n",
      "1699 [D loss: 0.121553, acc.: 96.88%] [G loss: 2.905485]\n",
      "1700 [D loss: 0.126086, acc.: 96.88%] [G loss: 3.067048]\n",
      "generated_data\n",
      "1701 [D loss: 0.116942, acc.: 96.88%] [G loss: 2.983838]\n",
      "1702 [D loss: 0.122256, acc.: 96.88%] [G loss: 3.072423]\n",
      "1703 [D loss: 0.110292, acc.: 96.88%] [G loss: 2.950320]\n",
      "1704 [D loss: 0.120925, acc.: 96.88%] [G loss: 3.059708]\n",
      "1705 [D loss: 0.111579, acc.: 96.88%] [G loss: 3.053312]\n",
      "1706 [D loss: 0.129759, acc.: 96.88%] [G loss: 2.983757]\n",
      "1707 [D loss: 0.120794, acc.: 96.88%] [G loss: 2.874750]\n",
      "1708 [D loss: 0.131564, acc.: 96.88%] [G loss: 2.973878]\n",
      "1709 [D loss: 0.126537, acc.: 96.88%] [G loss: 2.949355]\n",
      "1710 [D loss: 0.112501, acc.: 96.88%] [G loss: 3.053959]\n",
      "1711 [D loss: 0.124718, acc.: 96.88%] [G loss: 2.873986]\n",
      "1712 [D loss: 0.115834, acc.: 96.88%] [G loss: 2.824901]\n",
      "1713 [D loss: 0.118508, acc.: 96.88%] [G loss: 3.068803]\n",
      "1714 [D loss: 0.120642, acc.: 96.88%] [G loss: 2.982832]\n",
      "1715 [D loss: 0.122320, acc.: 96.88%] [G loss: 2.864837]\n",
      "1716 [D loss: 0.124795, acc.: 96.88%] [G loss: 2.931576]\n",
      "1717 [D loss: 0.119365, acc.: 96.88%] [G loss: 2.934796]\n",
      "1718 [D loss: 0.129993, acc.: 96.88%] [G loss: 2.925386]\n",
      "1719 [D loss: 0.109817, acc.: 96.88%] [G loss: 2.980448]\n",
      "1720 [D loss: 0.121961, acc.: 96.88%] [G loss: 2.984058]\n",
      "1721 [D loss: 0.122212, acc.: 96.88%] [G loss: 3.037355]\n",
      "1722 [D loss: 0.125131, acc.: 96.88%] [G loss: 2.969267]\n",
      "1723 [D loss: 0.131574, acc.: 96.88%] [G loss: 2.890836]\n",
      "1724 [D loss: 0.127210, acc.: 96.88%] [G loss: 3.054102]\n",
      "1725 [D loss: 0.116129, acc.: 96.88%] [G loss: 3.062991]\n",
      "1726 [D loss: 0.122822, acc.: 96.88%] [G loss: 3.110949]\n",
      "1727 [D loss: 0.120525, acc.: 96.88%] [G loss: 3.014205]\n",
      "1728 [D loss: 0.124028, acc.: 96.88%] [G loss: 3.028234]\n",
      "1729 [D loss: 0.134629, acc.: 96.88%] [G loss: 2.943881]\n",
      "1730 [D loss: 0.114766, acc.: 96.88%] [G loss: 3.056956]\n",
      "1731 [D loss: 0.117013, acc.: 96.88%] [G loss: 2.974173]\n",
      "1732 [D loss: 0.128733, acc.: 96.88%] [G loss: 2.891374]\n",
      "1733 [D loss: 0.115078, acc.: 96.88%] [G loss: 3.168248]\n",
      "1734 [D loss: 0.126904, acc.: 96.88%] [G loss: 2.973181]\n",
      "1735 [D loss: 0.129401, acc.: 96.88%] [G loss: 2.993097]\n",
      "1736 [D loss: 0.122050, acc.: 96.88%] [G loss: 2.990667]\n",
      "1737 [D loss: 0.119516, acc.: 96.88%] [G loss: 3.102679]\n",
      "1738 [D loss: 0.110969, acc.: 96.88%] [G loss: 3.129041]\n",
      "1739 [D loss: 0.123718, acc.: 96.88%] [G loss: 3.172367]\n",
      "1740 [D loss: 0.125763, acc.: 96.88%] [G loss: 3.146571]\n",
      "1741 [D loss: 0.118253, acc.: 96.88%] [G loss: 3.139986]\n",
      "1742 [D loss: 0.125882, acc.: 96.88%] [G loss: 3.071051]\n",
      "1743 [D loss: 0.128566, acc.: 96.88%] [G loss: 3.169976]\n",
      "1744 [D loss: 0.132076, acc.: 96.88%] [G loss: 3.133134]\n",
      "1745 [D loss: 0.113489, acc.: 96.88%] [G loss: 3.118779]\n",
      "1746 [D loss: 0.120541, acc.: 96.88%] [G loss: 3.061764]\n",
      "1747 [D loss: 0.122373, acc.: 96.88%] [G loss: 2.914854]\n",
      "1748 [D loss: 0.119150, acc.: 96.88%] [G loss: 2.946687]\n",
      "1749 [D loss: 0.123725, acc.: 96.88%] [G loss: 2.996882]\n",
      "1750 [D loss: 0.125487, acc.: 96.88%] [G loss: 2.993327]\n",
      "1751 [D loss: 0.116825, acc.: 96.88%] [G loss: 2.938545]\n",
      "1752 [D loss: 0.127311, acc.: 96.88%] [G loss: 3.047129]\n",
      "1753 [D loss: 0.122679, acc.: 96.88%] [G loss: 3.072158]\n",
      "1754 [D loss: 0.116941, acc.: 96.88%] [G loss: 3.083565]\n",
      "1755 [D loss: 0.129152, acc.: 96.88%] [G loss: 2.993396]\n",
      "1756 [D loss: 0.126970, acc.: 96.88%] [G loss: 3.061335]\n",
      "1757 [D loss: 0.118181, acc.: 96.88%] [G loss: 3.078088]\n",
      "1758 [D loss: 0.126567, acc.: 96.88%] [G loss: 3.015133]\n",
      "1759 [D loss: 0.120654, acc.: 96.88%] [G loss: 3.004533]\n",
      "1760 [D loss: 0.124551, acc.: 96.88%] [G loss: 2.966043]\n",
      "1761 [D loss: 0.111327, acc.: 96.88%] [G loss: 3.199961]\n",
      "1762 [D loss: 0.124424, acc.: 96.88%] [G loss: 2.950398]\n",
      "1763 [D loss: 0.127789, acc.: 96.88%] [G loss: 2.985488]\n",
      "1764 [D loss: 0.112852, acc.: 96.88%] [G loss: 2.989922]\n",
      "1765 [D loss: 0.124078, acc.: 96.88%] [G loss: 3.001306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1766 [D loss: 0.124486, acc.: 96.88%] [G loss: 2.923058]\n",
      "1767 [D loss: 0.118369, acc.: 96.88%] [G loss: 2.858508]\n",
      "1768 [D loss: 0.127885, acc.: 96.88%] [G loss: 2.870196]\n",
      "1769 [D loss: 0.124426, acc.: 96.88%] [G loss: 2.870343]\n",
      "1770 [D loss: 0.116147, acc.: 96.88%] [G loss: 2.849251]\n",
      "1771 [D loss: 0.129177, acc.: 96.88%] [G loss: 2.944311]\n",
      "1772 [D loss: 0.126046, acc.: 96.88%] [G loss: 2.990551]\n",
      "1773 [D loss: 0.121076, acc.: 96.88%] [G loss: 3.016044]\n",
      "1774 [D loss: 0.119552, acc.: 96.88%] [G loss: 3.033242]\n",
      "1775 [D loss: 0.120351, acc.: 96.88%] [G loss: 3.056033]\n",
      "1776 [D loss: 0.125385, acc.: 96.88%] [G loss: 2.981470]\n",
      "1777 [D loss: 0.134720, acc.: 96.88%] [G loss: 3.011876]\n",
      "1778 [D loss: 0.117684, acc.: 96.88%] [G loss: 2.922133]\n",
      "1779 [D loss: 0.123142, acc.: 96.88%] [G loss: 2.968084]\n",
      "1780 [D loss: 0.128767, acc.: 96.88%] [G loss: 3.070607]\n",
      "1781 [D loss: 0.119900, acc.: 96.88%] [G loss: 3.120414]\n",
      "1782 [D loss: 0.117102, acc.: 96.88%] [G loss: 3.021121]\n",
      "1783 [D loss: 0.124801, acc.: 96.88%] [G loss: 2.960405]\n",
      "1784 [D loss: 0.122361, acc.: 96.88%] [G loss: 2.900371]\n",
      "1785 [D loss: 0.123411, acc.: 96.88%] [G loss: 2.940454]\n",
      "1786 [D loss: 0.114311, acc.: 96.88%] [G loss: 3.008418]\n",
      "1787 [D loss: 0.124509, acc.: 96.88%] [G loss: 3.141325]\n",
      "1788 [D loss: 0.125978, acc.: 96.88%] [G loss: 2.901530]\n",
      "1789 [D loss: 0.117266, acc.: 96.88%] [G loss: 2.887033]\n",
      "1790 [D loss: 0.118497, acc.: 96.88%] [G loss: 2.964552]\n",
      "1791 [D loss: 0.127645, acc.: 96.88%] [G loss: 2.908583]\n",
      "1792 [D loss: 0.125451, acc.: 96.88%] [G loss: 2.950180]\n",
      "1793 [D loss: 0.113810, acc.: 96.88%] [G loss: 3.050540]\n",
      "1794 [D loss: 0.130111, acc.: 96.88%] [G loss: 2.888512]\n",
      "1795 [D loss: 0.121010, acc.: 96.88%] [G loss: 2.878784]\n",
      "1796 [D loss: 0.125828, acc.: 96.88%] [G loss: 3.149682]\n",
      "1797 [D loss: 0.119243, acc.: 96.88%] [G loss: 2.906631]\n",
      "1798 [D loss: 0.133310, acc.: 96.88%] [G loss: 2.917680]\n",
      "1799 [D loss: 0.112855, acc.: 96.88%] [G loss: 3.037371]\n",
      "1800 [D loss: 0.119760, acc.: 96.88%] [G loss: 2.993989]\n",
      "generated_data\n",
      "1801 [D loss: 0.121752, acc.: 96.88%] [G loss: 2.920805]\n",
      "1802 [D loss: 0.117528, acc.: 96.88%] [G loss: 3.025365]\n",
      "1803 [D loss: 0.119829, acc.: 96.88%] [G loss: 3.060175]\n",
      "1804 [D loss: 0.121606, acc.: 96.88%] [G loss: 3.019113]\n",
      "1805 [D loss: 0.119199, acc.: 96.88%] [G loss: 2.986832]\n",
      "1806 [D loss: 0.122426, acc.: 96.88%] [G loss: 2.992197]\n",
      "1807 [D loss: 0.125208, acc.: 96.88%] [G loss: 3.087214]\n",
      "1808 [D loss: 0.124074, acc.: 96.88%] [G loss: 3.036623]\n",
      "1809 [D loss: 0.128761, acc.: 96.88%] [G loss: 3.059756]\n",
      "1810 [D loss: 0.117193, acc.: 96.88%] [G loss: 3.110041]\n",
      "1811 [D loss: 0.129936, acc.: 96.88%] [G loss: 3.119653]\n",
      "1812 [D loss: 0.121814, acc.: 96.88%] [G loss: 2.995925]\n",
      "1813 [D loss: 0.128970, acc.: 96.88%] [G loss: 2.971967]\n",
      "1814 [D loss: 0.125661, acc.: 96.88%] [G loss: 3.064301]\n",
      "1815 [D loss: 0.118356, acc.: 96.88%] [G loss: 3.011788]\n",
      "1816 [D loss: 0.132779, acc.: 96.88%] [G loss: 3.042108]\n",
      "1817 [D loss: 0.113072, acc.: 96.88%] [G loss: 3.129130]\n",
      "1818 [D loss: 0.127580, acc.: 96.88%] [G loss: 3.031776]\n",
      "1819 [D loss: 0.117768, acc.: 96.88%] [G loss: 3.061364]\n",
      "1820 [D loss: 0.123030, acc.: 96.88%] [G loss: 3.121559]\n",
      "1821 [D loss: 0.124223, acc.: 96.88%] [G loss: 2.947336]\n",
      "1822 [D loss: 0.112696, acc.: 96.88%] [G loss: 3.014122]\n",
      "1823 [D loss: 0.121212, acc.: 96.88%] [G loss: 2.983493]\n",
      "1824 [D loss: 0.117187, acc.: 96.88%] [G loss: 3.024262]\n",
      "1825 [D loss: 0.117852, acc.: 96.88%] [G loss: 2.991576]\n",
      "1826 [D loss: 0.122104, acc.: 96.88%] [G loss: 3.099448]\n",
      "1827 [D loss: 0.130234, acc.: 96.88%] [G loss: 3.018110]\n",
      "1828 [D loss: 0.123308, acc.: 96.88%] [G loss: 3.150923]\n",
      "1829 [D loss: 0.124231, acc.: 96.88%] [G loss: 2.989028]\n",
      "1830 [D loss: 0.119265, acc.: 96.88%] [G loss: 3.063397]\n",
      "1831 [D loss: 0.118474, acc.: 96.88%] [G loss: 3.032297]\n",
      "1832 [D loss: 0.124370, acc.: 96.88%] [G loss: 2.831209]\n",
      "1833 [D loss: 0.118297, acc.: 96.88%] [G loss: 3.073876]\n",
      "1834 [D loss: 0.127396, acc.: 96.88%] [G loss: 3.004457]\n",
      "1835 [D loss: 0.116758, acc.: 96.88%] [G loss: 2.992879]\n",
      "1836 [D loss: 0.122439, acc.: 96.88%] [G loss: 2.953771]\n",
      "1837 [D loss: 0.112739, acc.: 96.88%] [G loss: 3.038099]\n",
      "1838 [D loss: 0.121529, acc.: 96.88%] [G loss: 2.987038]\n",
      "1839 [D loss: 0.125951, acc.: 96.88%] [G loss: 3.105662]\n",
      "1840 [D loss: 0.120317, acc.: 96.88%] [G loss: 3.025552]\n",
      "1841 [D loss: 0.129820, acc.: 96.88%] [G loss: 3.039283]\n",
      "1842 [D loss: 0.113009, acc.: 96.88%] [G loss: 3.052653]\n",
      "1843 [D loss: 0.124441, acc.: 96.88%] [G loss: 3.029235]\n",
      "1844 [D loss: 0.134224, acc.: 96.88%] [G loss: 2.931673]\n",
      "1845 [D loss: 0.122230, acc.: 96.88%] [G loss: 2.854257]\n",
      "1846 [D loss: 0.125027, acc.: 96.88%] [G loss: 2.967768]\n",
      "1847 [D loss: 0.122105, acc.: 96.88%] [G loss: 2.909039]\n",
      "1848 [D loss: 0.128033, acc.: 96.88%] [G loss: 3.002546]\n",
      "1849 [D loss: 0.120841, acc.: 96.88%] [G loss: 2.969706]\n",
      "1850 [D loss: 0.122869, acc.: 96.88%] [G loss: 3.000536]\n",
      "1851 [D loss: 0.121373, acc.: 96.88%] [G loss: 3.005626]\n",
      "1852 [D loss: 0.128965, acc.: 96.88%] [G loss: 3.044877]\n",
      "1853 [D loss: 0.128692, acc.: 96.88%] [G loss: 3.071058]\n",
      "1854 [D loss: 0.114563, acc.: 96.88%] [G loss: 3.019833]\n",
      "1855 [D loss: 0.124659, acc.: 96.88%] [G loss: 2.902441]\n",
      "1856 [D loss: 0.121777, acc.: 96.88%] [G loss: 3.061067]\n",
      "1857 [D loss: 0.117158, acc.: 96.88%] [G loss: 2.987642]\n",
      "1858 [D loss: 0.117724, acc.: 96.88%] [G loss: 2.944510]\n",
      "1859 [D loss: 0.118742, acc.: 96.88%] [G loss: 2.978256]\n",
      "1860 [D loss: 0.127344, acc.: 96.88%] [G loss: 3.094930]\n",
      "1861 [D loss: 0.128030, acc.: 96.88%] [G loss: 2.944431]\n",
      "1862 [D loss: 0.126919, acc.: 96.88%] [G loss: 3.106632]\n",
      "1863 [D loss: 0.123235, acc.: 96.88%] [G loss: 2.974873]\n",
      "1864 [D loss: 0.123391, acc.: 96.88%] [G loss: 3.037985]\n",
      "1865 [D loss: 0.116420, acc.: 96.88%] [G loss: 3.034694]\n",
      "1866 [D loss: 0.124075, acc.: 96.88%] [G loss: 2.988327]\n",
      "1867 [D loss: 0.121174, acc.: 96.88%] [G loss: 2.878785]\n",
      "1868 [D loss: 0.125929, acc.: 96.88%] [G loss: 2.888485]\n",
      "1869 [D loss: 0.123954, acc.: 96.88%] [G loss: 2.871412]\n",
      "1870 [D loss: 0.112431, acc.: 96.88%] [G loss: 3.010140]\n",
      "1871 [D loss: 0.131134, acc.: 96.88%] [G loss: 3.030399]\n",
      "1872 [D loss: 0.122973, acc.: 96.88%] [G loss: 2.931827]\n",
      "1873 [D loss: 0.117883, acc.: 96.88%] [G loss: 2.962715]\n",
      "1874 [D loss: 0.137259, acc.: 96.88%] [G loss: 2.936074]\n",
      "1875 [D loss: 0.120003, acc.: 96.88%] [G loss: 3.025774]\n",
      "1876 [D loss: 0.120605, acc.: 96.88%] [G loss: 2.884524]\n",
      "1877 [D loss: 0.121744, acc.: 96.88%] [G loss: 3.000824]\n",
      "1878 [D loss: 0.116455, acc.: 96.88%] [G loss: 3.029096]\n",
      "1879 [D loss: 0.121645, acc.: 96.88%] [G loss: 2.965938]\n",
      "1880 [D loss: 0.126071, acc.: 96.88%] [G loss: 3.081791]\n",
      "1881 [D loss: 0.119824, acc.: 96.88%] [G loss: 2.939289]\n",
      "1882 [D loss: 0.125629, acc.: 96.88%] [G loss: 3.024541]\n",
      "1883 [D loss: 0.125992, acc.: 96.88%] [G loss: 2.941637]\n",
      "1884 [D loss: 0.124000, acc.: 96.88%] [G loss: 2.944275]\n",
      "1885 [D loss: 0.120907, acc.: 96.88%] [G loss: 2.891412]\n",
      "1886 [D loss: 0.115909, acc.: 96.88%] [G loss: 3.024151]\n",
      "1887 [D loss: 0.123094, acc.: 96.88%] [G loss: 2.981801]\n",
      "1888 [D loss: 0.126080, acc.: 96.88%] [G loss: 3.017670]\n",
      "1889 [D loss: 0.119141, acc.: 96.88%] [G loss: 2.937355]\n",
      "1890 [D loss: 0.121912, acc.: 96.88%] [G loss: 2.934409]\n",
      "1891 [D loss: 0.121437, acc.: 96.88%] [G loss: 2.847567]\n",
      "1892 [D loss: 0.122468, acc.: 96.88%] [G loss: 2.936941]\n",
      "1893 [D loss: 0.126257, acc.: 96.88%] [G loss: 2.906034]\n",
      "1894 [D loss: 0.126833, acc.: 96.88%] [G loss: 2.859686]\n",
      "1895 [D loss: 0.122203, acc.: 96.88%] [G loss: 3.054455]\n",
      "1896 [D loss: 0.112930, acc.: 96.88%] [G loss: 3.020769]\n",
      "1897 [D loss: 0.111611, acc.: 96.88%] [G loss: 2.974935]\n",
      "1898 [D loss: 0.119274, acc.: 96.88%] [G loss: 2.971590]\n",
      "1899 [D loss: 0.120902, acc.: 96.88%] [G loss: 2.937576]\n",
      "1900 [D loss: 0.118410, acc.: 96.88%] [G loss: 3.124603]\n",
      "generated_data\n",
      "1901 [D loss: 0.125870, acc.: 96.88%] [G loss: 3.052695]\n",
      "1902 [D loss: 0.128350, acc.: 96.88%] [G loss: 2.899884]\n",
      "1903 [D loss: 0.124431, acc.: 96.88%] [G loss: 2.946125]\n",
      "1904 [D loss: 0.118415, acc.: 96.88%] [G loss: 3.030344]\n",
      "1905 [D loss: 0.126338, acc.: 96.88%] [G loss: 2.945556]\n",
      "1906 [D loss: 0.125198, acc.: 96.88%] [G loss: 3.032311]\n",
      "1907 [D loss: 0.124316, acc.: 96.88%] [G loss: 2.941289]\n",
      "1908 [D loss: 0.114045, acc.: 96.88%] [G loss: 3.077884]\n",
      "1909 [D loss: 0.128097, acc.: 96.88%] [G loss: 3.009532]\n",
      "1910 [D loss: 0.116500, acc.: 96.88%] [G loss: 3.135173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1911 [D loss: 0.121721, acc.: 96.88%] [G loss: 2.978281]\n",
      "1912 [D loss: 0.125499, acc.: 96.88%] [G loss: 3.018865]\n",
      "1913 [D loss: 0.121433, acc.: 96.88%] [G loss: 3.044335]\n",
      "1914 [D loss: 0.123066, acc.: 96.88%] [G loss: 3.088726]\n",
      "1915 [D loss: 0.125718, acc.: 96.88%] [G loss: 3.100509]\n",
      "1916 [D loss: 0.124354, acc.: 96.88%] [G loss: 2.918208]\n",
      "1917 [D loss: 0.119667, acc.: 96.88%] [G loss: 3.034813]\n",
      "1918 [D loss: 0.122346, acc.: 96.88%] [G loss: 2.957473]\n",
      "1919 [D loss: 0.120737, acc.: 96.88%] [G loss: 3.048718]\n",
      "1920 [D loss: 0.117327, acc.: 96.88%] [G loss: 3.002222]\n",
      "1921 [D loss: 0.131696, acc.: 96.88%] [G loss: 2.998874]\n",
      "1922 [D loss: 0.119684, acc.: 96.88%] [G loss: 3.014954]\n",
      "1923 [D loss: 0.123286, acc.: 96.88%] [G loss: 3.061048]\n",
      "1924 [D loss: 0.127851, acc.: 96.88%] [G loss: 3.177473]\n",
      "1925 [D loss: 0.120455, acc.: 96.88%] [G loss: 2.964575]\n",
      "1926 [D loss: 0.125012, acc.: 96.88%] [G loss: 3.145582]\n",
      "1927 [D loss: 0.121154, acc.: 96.88%] [G loss: 2.950056]\n",
      "1928 [D loss: 0.125284, acc.: 96.88%] [G loss: 2.910284]\n",
      "1929 [D loss: 0.120847, acc.: 96.88%] [G loss: 3.001876]\n",
      "1930 [D loss: 0.127523, acc.: 96.88%] [G loss: 2.935540]\n",
      "1931 [D loss: 0.120155, acc.: 96.88%] [G loss: 2.849867]\n",
      "1932 [D loss: 0.118817, acc.: 96.88%] [G loss: 3.048031]\n",
      "1933 [D loss: 0.125626, acc.: 96.88%] [G loss: 2.919759]\n",
      "1934 [D loss: 0.124625, acc.: 96.88%] [G loss: 2.890320]\n",
      "1935 [D loss: 0.118150, acc.: 96.88%] [G loss: 2.883298]\n",
      "1936 [D loss: 0.124327, acc.: 96.88%] [G loss: 2.991719]\n",
      "1937 [D loss: 0.128425, acc.: 96.88%] [G loss: 2.903731]\n",
      "1938 [D loss: 0.122143, acc.: 96.88%] [G loss: 2.871085]\n",
      "1939 [D loss: 0.124604, acc.: 96.88%] [G loss: 2.793734]\n",
      "1940 [D loss: 0.128760, acc.: 96.88%] [G loss: 2.928078]\n",
      "1941 [D loss: 0.121999, acc.: 96.88%] [G loss: 2.872015]\n",
      "1942 [D loss: 0.123383, acc.: 96.88%] [G loss: 2.905526]\n",
      "1943 [D loss: 0.128388, acc.: 96.88%] [G loss: 2.883615]\n",
      "1944 [D loss: 0.117732, acc.: 96.88%] [G loss: 2.907346]\n",
      "1945 [D loss: 0.126514, acc.: 96.88%] [G loss: 2.915568]\n",
      "1946 [D loss: 0.123532, acc.: 96.88%] [G loss: 2.874860]\n",
      "1947 [D loss: 0.117023, acc.: 96.88%] [G loss: 2.968874]\n",
      "1948 [D loss: 0.117548, acc.: 96.88%] [G loss: 3.027790]\n",
      "1949 [D loss: 0.128228, acc.: 96.88%] [G loss: 2.994572]\n",
      "1950 [D loss: 0.118944, acc.: 96.88%] [G loss: 3.036636]\n",
      "1951 [D loss: 0.123750, acc.: 96.88%] [G loss: 2.950618]\n",
      "1952 [D loss: 0.112511, acc.: 96.88%] [G loss: 2.929184]\n",
      "1953 [D loss: 0.122276, acc.: 96.88%] [G loss: 2.914687]\n",
      "1954 [D loss: 0.123543, acc.: 96.88%] [G loss: 2.948037]\n",
      "1955 [D loss: 0.121618, acc.: 96.88%] [G loss: 3.124868]\n",
      "1956 [D loss: 0.121139, acc.: 96.88%] [G loss: 2.917426]\n",
      "1957 [D loss: 0.123393, acc.: 96.88%] [G loss: 2.991409]\n",
      "1958 [D loss: 0.116763, acc.: 96.88%] [G loss: 2.989095]\n",
      "1959 [D loss: 0.124208, acc.: 96.88%] [G loss: 2.957744]\n",
      "1960 [D loss: 0.126849, acc.: 96.88%] [G loss: 2.975908]\n",
      "1961 [D loss: 0.122426, acc.: 96.88%] [G loss: 2.879491]\n",
      "1962 [D loss: 0.122581, acc.: 96.88%] [G loss: 2.996837]\n",
      "1963 [D loss: 0.127575, acc.: 96.88%] [G loss: 3.006985]\n",
      "1964 [D loss: 0.125241, acc.: 96.88%] [G loss: 3.022715]\n",
      "1965 [D loss: 0.125400, acc.: 96.88%] [G loss: 2.905924]\n",
      "1966 [D loss: 0.115259, acc.: 96.88%] [G loss: 2.840065]\n",
      "1967 [D loss: 0.124746, acc.: 96.88%] [G loss: 3.122286]\n",
      "1968 [D loss: 0.122724, acc.: 96.88%] [G loss: 2.875074]\n",
      "1969 [D loss: 0.114809, acc.: 96.88%] [G loss: 3.059356]\n",
      "1970 [D loss: 0.118871, acc.: 96.88%] [G loss: 3.005908]\n",
      "1971 [D loss: 0.127025, acc.: 96.88%] [G loss: 3.045999]\n",
      "1972 [D loss: 0.120508, acc.: 96.88%] [G loss: 3.185948]\n",
      "1973 [D loss: 0.118407, acc.: 96.88%] [G loss: 3.044000]\n",
      "1974 [D loss: 0.128069, acc.: 96.88%] [G loss: 3.042187]\n",
      "1975 [D loss: 0.122077, acc.: 96.88%] [G loss: 3.000992]\n",
      "1976 [D loss: 0.126687, acc.: 96.88%] [G loss: 2.877491]\n",
      "1977 [D loss: 0.116723, acc.: 96.88%] [G loss: 2.976889]\n",
      "1978 [D loss: 0.118125, acc.: 96.88%] [G loss: 2.928335]\n",
      "1979 [D loss: 0.122460, acc.: 96.88%] [G loss: 2.981322]\n",
      "1980 [D loss: 0.119777, acc.: 96.88%] [G loss: 2.978405]\n",
      "1981 [D loss: 0.114872, acc.: 96.88%] [G loss: 2.975051]\n",
      "1982 [D loss: 0.131957, acc.: 96.88%] [G loss: 3.031869]\n",
      "1983 [D loss: 0.120948, acc.: 96.88%] [G loss: 2.886597]\n",
      "1984 [D loss: 0.121725, acc.: 96.88%] [G loss: 2.899113]\n",
      "1985 [D loss: 0.114204, acc.: 96.88%] [G loss: 2.964535]\n",
      "1986 [D loss: 0.124069, acc.: 96.88%] [G loss: 2.996757]\n",
      "1987 [D loss: 0.128983, acc.: 96.88%] [G loss: 3.082454]\n",
      "1988 [D loss: 0.122793, acc.: 96.88%] [G loss: 2.979339]\n",
      "1989 [D loss: 0.122306, acc.: 96.88%] [G loss: 2.937235]\n",
      "1990 [D loss: 0.136229, acc.: 96.88%] [G loss: 2.978398]\n",
      "1991 [D loss: 0.110914, acc.: 96.88%] [G loss: 2.911699]\n",
      "1992 [D loss: 0.117873, acc.: 96.88%] [G loss: 3.039578]\n",
      "1993 [D loss: 0.121034, acc.: 96.88%] [G loss: 3.098439]\n",
      "1994 [D loss: 0.127690, acc.: 96.88%] [G loss: 3.065446]\n",
      "1995 [D loss: 0.124363, acc.: 96.88%] [G loss: 2.978277]\n",
      "1996 [D loss: 0.123539, acc.: 96.88%] [G loss: 3.033026]\n",
      "1997 [D loss: 0.121237, acc.: 96.88%] [G loss: 2.997070]\n",
      "1998 [D loss: 0.120080, acc.: 96.88%] [G loss: 2.954447]\n",
      "1999 [D loss: 0.121851, acc.: 96.88%] [G loss: 3.005026]\n",
      "2000 [D loss: 0.130389, acc.: 96.88%] [G loss: 2.958552]\n",
      "generated_data\n",
      "2001 [D loss: 0.123842, acc.: 96.88%] [G loss: 3.035421]\n",
      "2002 [D loss: 0.124966, acc.: 96.88%] [G loss: 3.023103]\n",
      "2003 [D loss: 0.119530, acc.: 96.88%] [G loss: 3.049131]\n",
      "2004 [D loss: 0.119890, acc.: 96.88%] [G loss: 2.854908]\n",
      "2005 [D loss: 0.118078, acc.: 96.88%] [G loss: 2.938615]\n",
      "2006 [D loss: 0.121660, acc.: 96.88%] [G loss: 3.074813]\n",
      "2007 [D loss: 0.125471, acc.: 96.88%] [G loss: 2.938733]\n",
      "2008 [D loss: 0.118467, acc.: 96.88%] [G loss: 3.001353]\n",
      "2009 [D loss: 0.117940, acc.: 96.88%] [G loss: 3.051165]\n",
      "2010 [D loss: 0.120507, acc.: 96.88%] [G loss: 2.999152]\n",
      "2011 [D loss: 0.125321, acc.: 96.88%] [G loss: 2.987383]\n",
      "2012 [D loss: 0.120905, acc.: 96.88%] [G loss: 2.972576]\n",
      "2013 [D loss: 0.115473, acc.: 96.88%] [G loss: 3.146888]\n",
      "2014 [D loss: 0.120052, acc.: 96.88%] [G loss: 3.045962]\n",
      "2015 [D loss: 0.123611, acc.: 96.88%] [G loss: 2.948386]\n",
      "2016 [D loss: 0.121059, acc.: 96.88%] [G loss: 2.985352]\n",
      "2017 [D loss: 0.117870, acc.: 96.88%] [G loss: 2.976389]\n",
      "2018 [D loss: 0.127215, acc.: 96.88%] [G loss: 2.981705]\n",
      "2019 [D loss: 0.120422, acc.: 96.88%] [G loss: 2.963064]\n",
      "2020 [D loss: 0.124449, acc.: 96.88%] [G loss: 2.973381]\n",
      "2021 [D loss: 0.119315, acc.: 96.88%] [G loss: 3.028409]\n",
      "2022 [D loss: 0.112788, acc.: 96.88%] [G loss: 3.137739]\n",
      "2023 [D loss: 0.121625, acc.: 96.88%] [G loss: 2.988102]\n",
      "2024 [D loss: 0.119268, acc.: 96.88%] [G loss: 2.989278]\n",
      "2025 [D loss: 0.112434, acc.: 96.88%] [G loss: 2.977154]\n",
      "2026 [D loss: 0.122941, acc.: 96.88%] [G loss: 3.098134]\n",
      "2027 [D loss: 0.134617, acc.: 96.88%] [G loss: 2.931330]\n",
      "2028 [D loss: 0.120611, acc.: 96.88%] [G loss: 2.916422]\n",
      "2029 [D loss: 0.129668, acc.: 96.88%] [G loss: 2.910206]\n",
      "2030 [D loss: 0.120632, acc.: 96.88%] [G loss: 2.953739]\n",
      "2031 [D loss: 0.119532, acc.: 96.88%] [G loss: 3.027292]\n",
      "2032 [D loss: 0.118250, acc.: 96.88%] [G loss: 3.020230]\n",
      "2033 [D loss: 0.124740, acc.: 96.88%] [G loss: 3.061774]\n",
      "2034 [D loss: 0.124138, acc.: 96.88%] [G loss: 2.942980]\n",
      "2035 [D loss: 0.127647, acc.: 96.88%] [G loss: 3.060589]\n",
      "2036 [D loss: 0.126346, acc.: 96.88%] [G loss: 3.060895]\n",
      "2037 [D loss: 0.119531, acc.: 96.88%] [G loss: 3.054317]\n",
      "2038 [D loss: 0.119618, acc.: 96.88%] [G loss: 3.140544]\n",
      "2039 [D loss: 0.123008, acc.: 96.88%] [G loss: 3.012216]\n",
      "2040 [D loss: 0.126179, acc.: 96.88%] [G loss: 3.023962]\n",
      "2041 [D loss: 0.124264, acc.: 96.88%] [G loss: 3.020672]\n",
      "2042 [D loss: 0.109247, acc.: 96.88%] [G loss: 2.907924]\n",
      "2043 [D loss: 0.128117, acc.: 96.88%] [G loss: 2.941395]\n",
      "2044 [D loss: 0.124072, acc.: 96.88%] [G loss: 3.020258]\n",
      "2045 [D loss: 0.123955, acc.: 96.88%] [G loss: 3.055546]\n",
      "2046 [D loss: 0.125661, acc.: 96.88%] [G loss: 3.053665]\n",
      "2047 [D loss: 0.126377, acc.: 96.88%] [G loss: 2.992439]\n",
      "2048 [D loss: 0.121576, acc.: 96.88%] [G loss: 3.101869]\n",
      "2049 [D loss: 0.123049, acc.: 96.88%] [G loss: 2.909554]\n",
      "2050 [D loss: 0.116779, acc.: 96.88%] [G loss: 3.103981]\n",
      "2051 [D loss: 0.118058, acc.: 96.88%] [G loss: 3.021234]\n",
      "2052 [D loss: 0.110389, acc.: 96.88%] [G loss: 3.064502]\n",
      "2053 [D loss: 0.129363, acc.: 96.88%] [G loss: 3.015454]\n",
      "2054 [D loss: 0.124013, acc.: 96.88%] [G loss: 3.020265]\n",
      "2055 [D loss: 0.127266, acc.: 96.88%] [G loss: 2.916020]\n",
      "2056 [D loss: 0.118161, acc.: 96.88%] [G loss: 2.947520]\n",
      "2057 [D loss: 0.125312, acc.: 96.88%] [G loss: 3.010271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058 [D loss: 0.118609, acc.: 96.88%] [G loss: 3.163736]\n",
      "2059 [D loss: 0.117980, acc.: 96.88%] [G loss: 3.000055]\n",
      "2060 [D loss: 0.114188, acc.: 96.88%] [G loss: 3.003996]\n",
      "2061 [D loss: 0.120082, acc.: 96.88%] [G loss: 2.991521]\n",
      "2062 [D loss: 0.119822, acc.: 96.88%] [G loss: 2.973253]\n",
      "2063 [D loss: 0.128067, acc.: 96.88%] [G loss: 3.007315]\n",
      "2064 [D loss: 0.122458, acc.: 96.88%] [G loss: 2.967908]\n",
      "2065 [D loss: 0.124008, acc.: 96.88%] [G loss: 3.161997]\n",
      "2066 [D loss: 0.119297, acc.: 96.88%] [G loss: 2.900413]\n",
      "2067 [D loss: 0.123657, acc.: 96.88%] [G loss: 2.982728]\n",
      "2068 [D loss: 0.126013, acc.: 96.88%] [G loss: 2.930481]\n",
      "2069 [D loss: 0.113795, acc.: 96.88%] [G loss: 2.881303]\n",
      "2070 [D loss: 0.121036, acc.: 96.88%] [G loss: 2.926181]\n",
      "2071 [D loss: 0.124810, acc.: 96.88%] [G loss: 2.890202]\n",
      "2072 [D loss: 0.122123, acc.: 96.88%] [G loss: 3.037174]\n",
      "2073 [D loss: 0.111413, acc.: 96.88%] [G loss: 2.968087]\n",
      "2074 [D loss: 0.126342, acc.: 96.88%] [G loss: 2.980321]\n",
      "2075 [D loss: 0.122762, acc.: 96.88%] [G loss: 2.963546]\n",
      "2076 [D loss: 0.121679, acc.: 96.88%] [G loss: 2.918655]\n",
      "2077 [D loss: 0.115924, acc.: 96.88%] [G loss: 2.962118]\n",
      "2078 [D loss: 0.122110, acc.: 96.88%] [G loss: 2.959766]\n",
      "2079 [D loss: 0.124298, acc.: 96.88%] [G loss: 2.966466]\n",
      "2080 [D loss: 0.120604, acc.: 96.88%] [G loss: 2.920125]\n",
      "2081 [D loss: 0.129503, acc.: 96.88%] [G loss: 2.900733]\n",
      "2082 [D loss: 0.121921, acc.: 96.88%] [G loss: 3.077991]\n",
      "2083 [D loss: 0.126695, acc.: 96.88%] [G loss: 2.955616]\n",
      "2084 [D loss: 0.125881, acc.: 96.88%] [G loss: 2.936060]\n",
      "2085 [D loss: 0.124088, acc.: 96.88%] [G loss: 2.956632]\n",
      "2086 [D loss: 0.112473, acc.: 96.88%] [G loss: 3.060786]\n",
      "2087 [D loss: 0.115661, acc.: 96.88%] [G loss: 3.094899]\n",
      "2088 [D loss: 0.124900, acc.: 96.88%] [G loss: 3.035356]\n",
      "2089 [D loss: 0.126045, acc.: 96.88%] [G loss: 2.946947]\n",
      "2090 [D loss: 0.117399, acc.: 96.88%] [G loss: 3.040068]\n",
      "2091 [D loss: 0.124999, acc.: 96.88%] [G loss: 2.950945]\n",
      "2092 [D loss: 0.118468, acc.: 96.88%] [G loss: 2.891146]\n",
      "2093 [D loss: 0.127465, acc.: 96.88%] [G loss: 2.855591]\n",
      "2094 [D loss: 0.124956, acc.: 96.88%] [G loss: 2.967644]\n",
      "2095 [D loss: 0.125660, acc.: 96.88%] [G loss: 2.983986]\n",
      "2096 [D loss: 0.119593, acc.: 96.88%] [G loss: 2.987181]\n",
      "2097 [D loss: 0.125894, acc.: 96.88%] [G loss: 2.920145]\n",
      "2098 [D loss: 0.120282, acc.: 96.88%] [G loss: 2.895135]\n",
      "2099 [D loss: 0.119867, acc.: 96.88%] [G loss: 2.873428]\n",
      "2100 [D loss: 0.135591, acc.: 96.88%] [G loss: 2.817140]\n",
      "generated_data\n",
      "2101 [D loss: 0.123722, acc.: 96.88%] [G loss: 2.899889]\n",
      "2102 [D loss: 0.129773, acc.: 96.88%] [G loss: 2.875921]\n",
      "2103 [D loss: 0.125106, acc.: 96.88%] [G loss: 2.853187]\n",
      "2104 [D loss: 0.129792, acc.: 96.88%] [G loss: 2.845359]\n",
      "2105 [D loss: 0.126840, acc.: 96.88%] [G loss: 2.932698]\n",
      "2106 [D loss: 0.122301, acc.: 96.88%] [G loss: 2.924205]\n",
      "2107 [D loss: 0.117825, acc.: 96.88%] [G loss: 2.898926]\n",
      "2108 [D loss: 0.121513, acc.: 96.88%] [G loss: 3.025292]\n",
      "2109 [D loss: 0.115080, acc.: 96.88%] [G loss: 2.892963]\n",
      "2110 [D loss: 0.125458, acc.: 96.88%] [G loss: 2.914325]\n",
      "2111 [D loss: 0.121827, acc.: 96.88%] [G loss: 2.943409]\n",
      "2112 [D loss: 0.126832, acc.: 96.88%] [G loss: 2.940092]\n",
      "2113 [D loss: 0.118121, acc.: 96.88%] [G loss: 2.879575]\n",
      "2114 [D loss: 0.132979, acc.: 96.88%] [G loss: 2.974352]\n",
      "2115 [D loss: 0.114954, acc.: 96.88%] [G loss: 2.928155]\n",
      "2116 [D loss: 0.122381, acc.: 96.88%] [G loss: 2.972765]\n",
      "2117 [D loss: 0.116406, acc.: 96.88%] [G loss: 2.936289]\n",
      "2118 [D loss: 0.126805, acc.: 96.88%] [G loss: 2.978538]\n",
      "2119 [D loss: 0.120395, acc.: 96.88%] [G loss: 3.106691]\n",
      "2120 [D loss: 0.120975, acc.: 96.88%] [G loss: 2.941714]\n",
      "2121 [D loss: 0.118183, acc.: 96.88%] [G loss: 3.094653]\n",
      "2122 [D loss: 0.126356, acc.: 96.88%] [G loss: 3.031518]\n",
      "2123 [D loss: 0.123306, acc.: 96.88%] [G loss: 2.921608]\n",
      "2124 [D loss: 0.126005, acc.: 96.88%] [G loss: 2.944736]\n",
      "2125 [D loss: 0.114721, acc.: 96.88%] [G loss: 3.038625]\n",
      "2126 [D loss: 0.127259, acc.: 96.88%] [G loss: 2.970820]\n",
      "2127 [D loss: 0.122633, acc.: 96.88%] [G loss: 2.958338]\n",
      "2128 [D loss: 0.127267, acc.: 96.88%] [G loss: 2.853223]\n",
      "2129 [D loss: 0.123920, acc.: 96.88%] [G loss: 2.929206]\n",
      "2130 [D loss: 0.126339, acc.: 96.88%] [G loss: 2.883793]\n",
      "2131 [D loss: 0.118204, acc.: 96.88%] [G loss: 3.037457]\n",
      "2132 [D loss: 0.126945, acc.: 96.88%] [G loss: 2.923050]\n",
      "2133 [D loss: 0.130520, acc.: 96.88%] [G loss: 2.929511]\n",
      "2134 [D loss: 0.119970, acc.: 96.88%] [G loss: 3.002236]\n",
      "2135 [D loss: 0.120687, acc.: 96.88%] [G loss: 2.902878]\n",
      "2136 [D loss: 0.117449, acc.: 96.88%] [G loss: 3.004664]\n",
      "2137 [D loss: 0.120790, acc.: 96.88%] [G loss: 2.955932]\n",
      "2138 [D loss: 0.121237, acc.: 96.88%] [G loss: 2.960771]\n",
      "2139 [D loss: 0.129676, acc.: 96.88%] [G loss: 2.988407]\n",
      "2140 [D loss: 0.116500, acc.: 96.88%] [G loss: 2.958920]\n",
      "2141 [D loss: 0.128145, acc.: 96.88%] [G loss: 2.923378]\n",
      "2142 [D loss: 0.114700, acc.: 96.88%] [G loss: 3.063014]\n",
      "2143 [D loss: 0.126692, acc.: 96.88%] [G loss: 2.902552]\n",
      "2144 [D loss: 0.118190, acc.: 96.88%] [G loss: 2.954453]\n",
      "2145 [D loss: 0.118011, acc.: 96.88%] [G loss: 3.174763]\n",
      "2146 [D loss: 0.122828, acc.: 96.88%] [G loss: 3.096506]\n",
      "2147 [D loss: 0.122177, acc.: 96.88%] [G loss: 3.028533]\n",
      "2148 [D loss: 0.125943, acc.: 96.88%] [G loss: 2.972126]\n",
      "2149 [D loss: 0.121325, acc.: 96.88%] [G loss: 2.973154]\n",
      "2150 [D loss: 0.121593, acc.: 96.88%] [G loss: 2.935474]\n",
      "2151 [D loss: 0.120248, acc.: 96.88%] [G loss: 3.037370]\n",
      "2152 [D loss: 0.121735, acc.: 96.88%] [G loss: 2.942776]\n",
      "2153 [D loss: 0.118873, acc.: 96.88%] [G loss: 2.887737]\n",
      "2154 [D loss: 0.122782, acc.: 96.88%] [G loss: 3.003729]\n",
      "2155 [D loss: 0.118843, acc.: 96.88%] [G loss: 2.910954]\n",
      "2156 [D loss: 0.134029, acc.: 96.88%] [G loss: 2.925751]\n",
      "2157 [D loss: 0.122917, acc.: 96.88%] [G loss: 2.834799]\n",
      "2158 [D loss: 0.114903, acc.: 96.88%] [G loss: 2.950389]\n",
      "2159 [D loss: 0.129278, acc.: 96.88%] [G loss: 2.895903]\n",
      "2160 [D loss: 0.129405, acc.: 96.88%] [G loss: 2.890793]\n",
      "2161 [D loss: 0.119243, acc.: 96.88%] [G loss: 2.866670]\n",
      "2162 [D loss: 0.118225, acc.: 96.88%] [G loss: 3.188544]\n",
      "2163 [D loss: 0.129101, acc.: 96.88%] [G loss: 2.917089]\n",
      "2164 [D loss: 0.121075, acc.: 96.88%] [G loss: 3.021870]\n",
      "2165 [D loss: 0.126882, acc.: 96.88%] [G loss: 3.049943]\n",
      "2166 [D loss: 0.122952, acc.: 96.88%] [G loss: 2.982808]\n",
      "2167 [D loss: 0.130025, acc.: 96.88%] [G loss: 3.017644]\n",
      "2168 [D loss: 0.125954, acc.: 96.88%] [G loss: 2.894838]\n",
      "2169 [D loss: 0.123566, acc.: 96.88%] [G loss: 2.923259]\n",
      "2170 [D loss: 0.126708, acc.: 96.88%] [G loss: 2.838029]\n",
      "2171 [D loss: 0.122992, acc.: 96.88%] [G loss: 2.948967]\n",
      "2172 [D loss: 0.125529, acc.: 96.88%] [G loss: 2.916277]\n",
      "2173 [D loss: 0.118627, acc.: 96.88%] [G loss: 2.848818]\n",
      "2174 [D loss: 0.116578, acc.: 96.88%] [G loss: 2.836394]\n",
      "2175 [D loss: 0.118428, acc.: 96.88%] [G loss: 2.882161]\n",
      "2176 [D loss: 0.127709, acc.: 96.88%] [G loss: 2.940781]\n",
      "2177 [D loss: 0.126571, acc.: 96.88%] [G loss: 2.910708]\n",
      "2178 [D loss: 0.120617, acc.: 96.88%] [G loss: 2.852087]\n",
      "2179 [D loss: 0.112360, acc.: 96.88%] [G loss: 2.927600]\n",
      "2180 [D loss: 0.125069, acc.: 96.88%] [G loss: 2.912044]\n",
      "2181 [D loss: 0.117373, acc.: 96.88%] [G loss: 3.036767]\n",
      "2182 [D loss: 0.118297, acc.: 96.88%] [G loss: 2.981736]\n",
      "2183 [D loss: 0.123005, acc.: 96.88%] [G loss: 2.882717]\n",
      "2184 [D loss: 0.121004, acc.: 96.88%] [G loss: 2.960721]\n",
      "2185 [D loss: 0.124549, acc.: 96.88%] [G loss: 3.029253]\n",
      "2186 [D loss: 0.124105, acc.: 96.88%] [G loss: 2.988807]\n",
      "2187 [D loss: 0.120053, acc.: 96.88%] [G loss: 2.943680]\n",
      "2188 [D loss: 0.117990, acc.: 96.88%] [G loss: 2.990894]\n",
      "2189 [D loss: 0.117068, acc.: 96.88%] [G loss: 3.034410]\n",
      "2190 [D loss: 0.117077, acc.: 96.88%] [G loss: 2.933179]\n",
      "2191 [D loss: 0.121986, acc.: 96.88%] [G loss: 2.992342]\n",
      "2192 [D loss: 0.121065, acc.: 96.88%] [G loss: 3.002510]\n",
      "2193 [D loss: 0.124506, acc.: 96.88%] [G loss: 2.947394]\n",
      "2194 [D loss: 0.125486, acc.: 96.88%] [G loss: 2.896494]\n",
      "2195 [D loss: 0.123584, acc.: 96.88%] [G loss: 2.953958]\n",
      "2196 [D loss: 0.120706, acc.: 96.88%] [G loss: 2.927101]\n",
      "2197 [D loss: 0.120240, acc.: 96.88%] [G loss: 2.943716]\n",
      "2198 [D loss: 0.129611, acc.: 96.88%] [G loss: 2.945797]\n",
      "2199 [D loss: 0.128077, acc.: 96.88%] [G loss: 2.924538]\n",
      "2200 [D loss: 0.124658, acc.: 96.88%] [G loss: 2.974654]\n",
      "generated_data\n",
      "2201 [D loss: 0.120218, acc.: 96.88%] [G loss: 2.932283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202 [D loss: 0.118906, acc.: 96.88%] [G loss: 3.078382]\n",
      "2203 [D loss: 0.122554, acc.: 96.88%] [G loss: 2.937932]\n",
      "2204 [D loss: 0.125394, acc.: 96.88%] [G loss: 2.872859]\n",
      "2205 [D loss: 0.120828, acc.: 96.88%] [G loss: 2.918455]\n",
      "2206 [D loss: 0.123855, acc.: 96.88%] [G loss: 2.951292]\n",
      "2207 [D loss: 0.121861, acc.: 96.88%] [G loss: 2.880862]\n",
      "2208 [D loss: 0.120040, acc.: 96.88%] [G loss: 2.981888]\n",
      "2209 [D loss: 0.119287, acc.: 96.88%] [G loss: 3.043128]\n",
      "2210 [D loss: 0.115946, acc.: 96.88%] [G loss: 2.926520]\n",
      "2211 [D loss: 0.126913, acc.: 96.88%] [G loss: 2.878434]\n",
      "2212 [D loss: 0.123005, acc.: 96.88%] [G loss: 3.103745]\n",
      "2213 [D loss: 0.117389, acc.: 96.88%] [G loss: 2.988110]\n",
      "2214 [D loss: 0.121636, acc.: 96.88%] [G loss: 3.018223]\n",
      "2215 [D loss: 0.131549, acc.: 96.88%] [G loss: 2.885226]\n",
      "2216 [D loss: 0.116271, acc.: 96.88%] [G loss: 2.917418]\n",
      "2217 [D loss: 0.124186, acc.: 96.88%] [G loss: 2.973119]\n",
      "2218 [D loss: 0.124082, acc.: 96.88%] [G loss: 2.946716]\n",
      "2219 [D loss: 0.120781, acc.: 96.88%] [G loss: 2.888649]\n",
      "2220 [D loss: 0.122317, acc.: 96.88%] [G loss: 2.946043]\n",
      "2221 [D loss: 0.122515, acc.: 96.88%] [G loss: 2.927641]\n",
      "2222 [D loss: 0.122666, acc.: 96.88%] [G loss: 2.886256]\n",
      "2223 [D loss: 0.123400, acc.: 96.88%] [G loss: 3.002643]\n",
      "2224 [D loss: 0.121209, acc.: 96.88%] [G loss: 2.952067]\n",
      "2225 [D loss: 0.120507, acc.: 96.88%] [G loss: 2.930213]\n",
      "2226 [D loss: 0.120882, acc.: 96.88%] [G loss: 2.903270]\n",
      "2227 [D loss: 0.122979, acc.: 96.88%] [G loss: 2.880262]\n",
      "2228 [D loss: 0.123916, acc.: 96.88%] [G loss: 2.956114]\n",
      "2229 [D loss: 0.121998, acc.: 96.88%] [G loss: 2.922138]\n",
      "2230 [D loss: 0.124532, acc.: 96.88%] [G loss: 2.890880]\n",
      "2231 [D loss: 0.117425, acc.: 96.88%] [G loss: 3.086025]\n",
      "2232 [D loss: 0.127157, acc.: 96.88%] [G loss: 2.988515]\n",
      "2233 [D loss: 0.119332, acc.: 96.88%] [G loss: 2.915514]\n",
      "2234 [D loss: 0.123007, acc.: 96.88%] [G loss: 2.913807]\n",
      "2235 [D loss: 0.126712, acc.: 96.88%] [G loss: 2.920365]\n",
      "2236 [D loss: 0.127407, acc.: 96.88%] [G loss: 2.915563]\n",
      "2237 [D loss: 0.120335, acc.: 96.88%] [G loss: 2.873115]\n",
      "2238 [D loss: 0.131310, acc.: 96.88%] [G loss: 3.092307]\n",
      "2239 [D loss: 0.119514, acc.: 96.88%] [G loss: 2.930420]\n",
      "2240 [D loss: 0.121529, acc.: 96.88%] [G loss: 2.933043]\n",
      "2241 [D loss: 0.124243, acc.: 96.88%] [G loss: 2.887740]\n",
      "2242 [D loss: 0.118592, acc.: 96.88%] [G loss: 2.981033]\n",
      "2243 [D loss: 0.120169, acc.: 96.88%] [G loss: 2.894056]\n",
      "2244 [D loss: 0.123019, acc.: 96.88%] [G loss: 3.008368]\n",
      "2245 [D loss: 0.121761, acc.: 96.88%] [G loss: 2.977645]\n",
      "2246 [D loss: 0.121533, acc.: 96.88%] [G loss: 2.963062]\n",
      "2247 [D loss: 0.116977, acc.: 96.88%] [G loss: 2.999765]\n",
      "2248 [D loss: 0.123779, acc.: 96.88%] [G loss: 3.029390]\n",
      "2249 [D loss: 0.116621, acc.: 96.88%] [G loss: 3.048108]\n",
      "2250 [D loss: 0.117611, acc.: 96.88%] [G loss: 2.981633]\n",
      "2251 [D loss: 0.127524, acc.: 96.88%] [G loss: 3.046287]\n",
      "2252 [D loss: 0.127420, acc.: 96.88%] [G loss: 2.887658]\n",
      "2253 [D loss: 0.115754, acc.: 96.88%] [G loss: 2.944254]\n",
      "2254 [D loss: 0.128677, acc.: 96.88%] [G loss: 2.931896]\n",
      "2255 [D loss: 0.122119, acc.: 96.88%] [G loss: 2.934032]\n",
      "2256 [D loss: 0.122471, acc.: 96.88%] [G loss: 2.970001]\n",
      "2257 [D loss: 0.123148, acc.: 96.88%] [G loss: 2.871506]\n",
      "2258 [D loss: 0.121078, acc.: 96.88%] [G loss: 2.879909]\n",
      "2259 [D loss: 0.124582, acc.: 96.88%] [G loss: 2.939831]\n",
      "2260 [D loss: 0.118708, acc.: 96.88%] [G loss: 3.052705]\n",
      "2261 [D loss: 0.129509, acc.: 96.88%] [G loss: 2.909323]\n",
      "2262 [D loss: 0.121938, acc.: 96.88%] [G loss: 2.940930]\n",
      "2263 [D loss: 0.116387, acc.: 96.88%] [G loss: 3.048862]\n",
      "2264 [D loss: 0.125236, acc.: 96.88%] [G loss: 2.884406]\n",
      "2265 [D loss: 0.129524, acc.: 96.88%] [G loss: 2.849673]\n",
      "2266 [D loss: 0.126766, acc.: 96.88%] [G loss: 2.928604]\n",
      "2267 [D loss: 0.118981, acc.: 96.88%] [G loss: 2.953402]\n",
      "2268 [D loss: 0.118404, acc.: 96.88%] [G loss: 2.945984]\n",
      "2269 [D loss: 0.126179, acc.: 96.88%] [G loss: 2.926826]\n",
      "2270 [D loss: 0.121968, acc.: 96.88%] [G loss: 2.971425]\n",
      "2271 [D loss: 0.122046, acc.: 96.88%] [G loss: 2.951668]\n",
      "2272 [D loss: 0.129439, acc.: 96.88%] [G loss: 2.893893]\n",
      "2273 [D loss: 0.121642, acc.: 96.88%] [G loss: 2.841381]\n",
      "2274 [D loss: 0.118929, acc.: 96.88%] [G loss: 2.954440]\n",
      "2275 [D loss: 0.119677, acc.: 96.88%] [G loss: 3.005604]\n",
      "2276 [D loss: 0.123205, acc.: 96.88%] [G loss: 3.007610]\n",
      "2277 [D loss: 0.121839, acc.: 96.88%] [G loss: 2.890884]\n",
      "2278 [D loss: 0.122112, acc.: 96.88%] [G loss: 2.977213]\n",
      "2279 [D loss: 0.124636, acc.: 96.88%] [G loss: 3.022457]\n",
      "2280 [D loss: 0.123170, acc.: 96.88%] [G loss: 3.054069]\n",
      "2281 [D loss: 0.116008, acc.: 96.88%] [G loss: 2.995963]\n",
      "2282 [D loss: 0.118729, acc.: 96.88%] [G loss: 2.881529]\n",
      "2283 [D loss: 0.121102, acc.: 96.88%] [G loss: 2.970545]\n",
      "2284 [D loss: 0.119174, acc.: 96.88%] [G loss: 2.943083]\n",
      "2285 [D loss: 0.120436, acc.: 96.88%] [G loss: 3.041827]\n",
      "2286 [D loss: 0.122571, acc.: 96.88%] [G loss: 3.040853]\n",
      "2287 [D loss: 0.124048, acc.: 96.88%] [G loss: 2.896787]\n",
      "2288 [D loss: 0.124269, acc.: 96.88%] [G loss: 2.913271]\n",
      "2289 [D loss: 0.118375, acc.: 96.88%] [G loss: 2.966498]\n",
      "2290 [D loss: 0.114127, acc.: 96.88%] [G loss: 2.978304]\n",
      "2291 [D loss: 0.115174, acc.: 96.88%] [G loss: 2.928107]\n",
      "2292 [D loss: 0.124226, acc.: 96.88%] [G loss: 2.967550]\n",
      "2293 [D loss: 0.122871, acc.: 96.88%] [G loss: 3.046700]\n",
      "2294 [D loss: 0.123389, acc.: 96.88%] [G loss: 3.007573]\n",
      "2295 [D loss: 0.129787, acc.: 96.88%] [G loss: 3.004618]\n",
      "2296 [D loss: 0.121073, acc.: 96.88%] [G loss: 2.952706]\n",
      "2297 [D loss: 0.121769, acc.: 96.88%] [G loss: 2.992335]\n",
      "2298 [D loss: 0.125609, acc.: 96.88%] [G loss: 3.016177]\n",
      "2299 [D loss: 0.117817, acc.: 96.88%] [G loss: 2.982541]\n",
      "2300 [D loss: 0.127544, acc.: 96.88%] [G loss: 2.876375]\n",
      "generated_data\n",
      "2301 [D loss: 0.121016, acc.: 96.88%] [G loss: 2.938401]\n",
      "2302 [D loss: 0.118302, acc.: 96.88%] [G loss: 3.166429]\n",
      "2303 [D loss: 0.117450, acc.: 96.88%] [G loss: 3.064890]\n",
      "2304 [D loss: 0.123396, acc.: 96.88%] [G loss: 3.003028]\n",
      "2305 [D loss: 0.126121, acc.: 96.88%] [G loss: 3.045546]\n",
      "2306 [D loss: 0.128578, acc.: 96.88%] [G loss: 2.969710]\n",
      "2307 [D loss: 0.129260, acc.: 96.88%] [G loss: 3.008655]\n",
      "2308 [D loss: 0.123967, acc.: 96.88%] [G loss: 2.977980]\n",
      "2309 [D loss: 0.124483, acc.: 96.88%] [G loss: 2.946434]\n",
      "2310 [D loss: 0.125039, acc.: 96.88%] [G loss: 2.889551]\n",
      "2311 [D loss: 0.128996, acc.: 96.88%] [G loss: 2.953211]\n",
      "2312 [D loss: 0.120671, acc.: 96.88%] [G loss: 2.970432]\n",
      "2313 [D loss: 0.127991, acc.: 96.88%] [G loss: 2.893551]\n",
      "2314 [D loss: 0.129491, acc.: 96.88%] [G loss: 2.899430]\n",
      "2315 [D loss: 0.119460, acc.: 96.88%] [G loss: 3.020156]\n",
      "2316 [D loss: 0.124190, acc.: 96.88%] [G loss: 2.973602]\n",
      "2317 [D loss: 0.126811, acc.: 96.88%] [G loss: 2.924283]\n",
      "2318 [D loss: 0.114194, acc.: 96.88%] [G loss: 2.913354]\n",
      "2319 [D loss: 0.120739, acc.: 96.88%] [G loss: 2.901086]\n",
      "2320 [D loss: 0.124266, acc.: 96.88%] [G loss: 2.957385]\n",
      "2321 [D loss: 0.115640, acc.: 96.88%] [G loss: 3.036057]\n",
      "2322 [D loss: 0.119524, acc.: 96.88%] [G loss: 3.035161]\n",
      "2323 [D loss: 0.123889, acc.: 96.88%] [G loss: 3.082694]\n",
      "2324 [D loss: 0.130747, acc.: 96.88%] [G loss: 2.960011]\n",
      "2325 [D loss: 0.121066, acc.: 96.88%] [G loss: 2.971595]\n",
      "2326 [D loss: 0.115965, acc.: 96.88%] [G loss: 3.015759]\n",
      "2327 [D loss: 0.119264, acc.: 96.88%] [G loss: 2.965409]\n",
      "2328 [D loss: 0.120705, acc.: 96.88%] [G loss: 2.996039]\n",
      "2329 [D loss: 0.121457, acc.: 96.88%] [G loss: 2.889288]\n",
      "2330 [D loss: 0.130175, acc.: 96.88%] [G loss: 2.949552]\n",
      "2331 [D loss: 0.123263, acc.: 96.88%] [G loss: 2.982895]\n",
      "2332 [D loss: 0.122551, acc.: 96.88%] [G loss: 2.878498]\n",
      "2333 [D loss: 0.126227, acc.: 96.88%] [G loss: 2.951817]\n",
      "2334 [D loss: 0.116202, acc.: 96.88%] [G loss: 2.871522]\n",
      "2335 [D loss: 0.121007, acc.: 96.88%] [G loss: 2.891342]\n",
      "2336 [D loss: 0.118592, acc.: 96.88%] [G loss: 3.022212]\n",
      "2337 [D loss: 0.116175, acc.: 96.88%] [G loss: 2.904545]\n",
      "2338 [D loss: 0.123374, acc.: 96.88%] [G loss: 2.942526]\n",
      "2339 [D loss: 0.119725, acc.: 96.88%] [G loss: 2.982515]\n",
      "2340 [D loss: 0.115413, acc.: 96.88%] [G loss: 3.006164]\n",
      "2341 [D loss: 0.121031, acc.: 96.88%] [G loss: 2.890792]\n",
      "2342 [D loss: 0.121342, acc.: 96.88%] [G loss: 2.914520]\n",
      "2343 [D loss: 0.129109, acc.: 96.88%] [G loss: 3.000562]\n",
      "2344 [D loss: 0.132423, acc.: 96.88%] [G loss: 2.900302]\n",
      "2345 [D loss: 0.122187, acc.: 96.88%] [G loss: 2.958123]\n",
      "2346 [D loss: 0.118843, acc.: 96.88%] [G loss: 2.883708]\n",
      "2347 [D loss: 0.119268, acc.: 96.88%] [G loss: 3.044983]\n",
      "2348 [D loss: 0.119870, acc.: 96.88%] [G loss: 2.929322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349 [D loss: 0.128397, acc.: 96.88%] [G loss: 2.939987]\n",
      "2350 [D loss: 0.127731, acc.: 96.88%] [G loss: 2.964449]\n",
      "2351 [D loss: 0.128345, acc.: 96.88%] [G loss: 2.984937]\n",
      "2352 [D loss: 0.123854, acc.: 96.88%] [G loss: 2.872901]\n",
      "2353 [D loss: 0.119832, acc.: 96.88%] [G loss: 2.907592]\n",
      "2354 [D loss: 0.125107, acc.: 96.88%] [G loss: 2.956901]\n",
      "2355 [D loss: 0.121285, acc.: 96.88%] [G loss: 2.947587]\n",
      "2356 [D loss: 0.118204, acc.: 96.88%] [G loss: 3.120530]\n",
      "2357 [D loss: 0.117617, acc.: 96.88%] [G loss: 3.046053]\n",
      "2358 [D loss: 0.121083, acc.: 96.88%] [G loss: 2.972747]\n",
      "2359 [D loss: 0.127218, acc.: 96.88%] [G loss: 2.954122]\n",
      "2360 [D loss: 0.111813, acc.: 96.88%] [G loss: 3.019491]\n",
      "2361 [D loss: 0.125283, acc.: 96.88%] [G loss: 2.969122]\n",
      "2362 [D loss: 0.120316, acc.: 96.88%] [G loss: 2.940980]\n",
      "2363 [D loss: 0.122113, acc.: 96.88%] [G loss: 3.019263]\n",
      "2364 [D loss: 0.115581, acc.: 96.88%] [G loss: 3.144688]\n",
      "2365 [D loss: 0.124051, acc.: 96.88%] [G loss: 2.935134]\n",
      "2366 [D loss: 0.116999, acc.: 96.88%] [G loss: 2.851105]\n",
      "2367 [D loss: 0.124612, acc.: 96.88%] [G loss: 3.018881]\n",
      "2368 [D loss: 0.126317, acc.: 96.88%] [G loss: 2.955506]\n",
      "2369 [D loss: 0.125446, acc.: 96.88%] [G loss: 2.911782]\n",
      "2370 [D loss: 0.125164, acc.: 96.88%] [G loss: 3.087289]\n",
      "2371 [D loss: 0.123400, acc.: 96.88%] [G loss: 2.884256]\n",
      "2372 [D loss: 0.116764, acc.: 96.88%] [G loss: 2.980437]\n",
      "2373 [D loss: 0.122116, acc.: 96.88%] [G loss: 2.943401]\n",
      "2374 [D loss: 0.121706, acc.: 96.88%] [G loss: 2.986355]\n",
      "2375 [D loss: 0.131087, acc.: 96.88%] [G loss: 3.069937]\n",
      "2376 [D loss: 0.115873, acc.: 96.88%] [G loss: 2.934707]\n",
      "2377 [D loss: 0.121831, acc.: 96.88%] [G loss: 2.999081]\n",
      "2378 [D loss: 0.126738, acc.: 96.88%] [G loss: 2.920727]\n",
      "2379 [D loss: 0.119488, acc.: 96.88%] [G loss: 2.933194]\n",
      "2380 [D loss: 0.121463, acc.: 96.88%] [G loss: 2.913549]\n",
      "2381 [D loss: 0.120615, acc.: 96.88%] [G loss: 3.122868]\n",
      "2382 [D loss: 0.120670, acc.: 96.88%] [G loss: 2.968827]\n",
      "2383 [D loss: 0.122682, acc.: 96.88%] [G loss: 2.988444]\n",
      "2384 [D loss: 0.115679, acc.: 96.88%] [G loss: 2.964092]\n",
      "2385 [D loss: 0.128847, acc.: 96.88%] [G loss: 2.944450]\n",
      "2386 [D loss: 0.115027, acc.: 96.88%] [G loss: 2.894009]\n",
      "2387 [D loss: 0.121059, acc.: 96.88%] [G loss: 2.968292]\n",
      "2388 [D loss: 0.122811, acc.: 96.88%] [G loss: 2.996185]\n",
      "2389 [D loss: 0.127521, acc.: 96.88%] [G loss: 2.925138]\n",
      "2390 [D loss: 0.119864, acc.: 96.88%] [G loss: 2.858575]\n",
      "2391 [D loss: 0.120882, acc.: 96.88%] [G loss: 2.946185]\n",
      "2392 [D loss: 0.119344, acc.: 96.88%] [G loss: 2.945796]\n",
      "2393 [D loss: 0.120864, acc.: 96.88%] [G loss: 3.039902]\n",
      "2394 [D loss: 0.123649, acc.: 96.88%] [G loss: 2.906981]\n",
      "2395 [D loss: 0.117488, acc.: 96.88%] [G loss: 2.961946]\n",
      "2396 [D loss: 0.124305, acc.: 96.88%] [G loss: 2.996781]\n",
      "2397 [D loss: 0.114872, acc.: 96.88%] [G loss: 2.976175]\n",
      "2398 [D loss: 0.118566, acc.: 96.88%] [G loss: 3.002896]\n",
      "2399 [D loss: 0.121194, acc.: 96.88%] [G loss: 2.982168]\n",
      "2400 [D loss: 0.118373, acc.: 96.88%] [G loss: 2.968002]\n",
      "generated_data\n",
      "2401 [D loss: 0.126503, acc.: 96.88%] [G loss: 2.959134]\n",
      "2402 [D loss: 0.122101, acc.: 96.88%] [G loss: 3.014960]\n",
      "2403 [D loss: 0.123425, acc.: 96.88%] [G loss: 2.973864]\n",
      "2404 [D loss: 0.123534, acc.: 96.88%] [G loss: 3.000514]\n",
      "2405 [D loss: 0.126892, acc.: 96.88%] [G loss: 2.889631]\n",
      "2406 [D loss: 0.122115, acc.: 96.88%] [G loss: 2.933074]\n",
      "2407 [D loss: 0.122044, acc.: 96.88%] [G loss: 2.991738]\n",
      "2408 [D loss: 0.120523, acc.: 96.88%] [G loss: 2.937888]\n",
      "2409 [D loss: 0.123967, acc.: 96.88%] [G loss: 2.972074]\n",
      "2410 [D loss: 0.123816, acc.: 96.88%] [G loss: 2.912415]\n",
      "2411 [D loss: 0.120243, acc.: 96.88%] [G loss: 3.081173]\n",
      "2412 [D loss: 0.121470, acc.: 96.88%] [G loss: 2.997680]\n",
      "2413 [D loss: 0.126419, acc.: 96.88%] [G loss: 2.961486]\n",
      "2414 [D loss: 0.127016, acc.: 96.88%] [G loss: 2.941057]\n",
      "2415 [D loss: 0.122206, acc.: 96.88%] [G loss: 2.936494]\n",
      "2416 [D loss: 0.121610, acc.: 96.88%] [G loss: 3.043861]\n",
      "2417 [D loss: 0.126186, acc.: 96.88%] [G loss: 2.887346]\n",
      "2418 [D loss: 0.121624, acc.: 96.88%] [G loss: 2.929992]\n",
      "2419 [D loss: 0.120951, acc.: 96.88%] [G loss: 2.965346]\n",
      "2420 [D loss: 0.120650, acc.: 96.88%] [G loss: 2.927527]\n",
      "2421 [D loss: 0.126039, acc.: 96.88%] [G loss: 2.908987]\n",
      "2422 [D loss: 0.115232, acc.: 96.88%] [G loss: 2.957014]\n",
      "2423 [D loss: 0.123988, acc.: 96.88%] [G loss: 2.879099]\n",
      "2424 [D loss: 0.119137, acc.: 96.88%] [G loss: 2.849617]\n",
      "2425 [D loss: 0.120425, acc.: 96.88%] [G loss: 3.031433]\n",
      "2426 [D loss: 0.113682, acc.: 96.88%] [G loss: 2.910109]\n",
      "2427 [D loss: 0.118991, acc.: 96.88%] [G loss: 2.918468]\n",
      "2428 [D loss: 0.118380, acc.: 96.88%] [G loss: 2.969525]\n",
      "2429 [D loss: 0.122516, acc.: 96.88%] [G loss: 2.978157]\n",
      "2430 [D loss: 0.121017, acc.: 96.88%] [G loss: 3.023939]\n",
      "2431 [D loss: 0.120549, acc.: 96.88%] [G loss: 3.032593]\n",
      "2432 [D loss: 0.123237, acc.: 96.88%] [G loss: 2.956544]\n",
      "2433 [D loss: 0.131474, acc.: 96.88%] [G loss: 2.995687]\n",
      "2434 [D loss: 0.125326, acc.: 96.88%] [G loss: 2.953851]\n",
      "2435 [D loss: 0.121775, acc.: 96.88%] [G loss: 2.987211]\n",
      "2436 [D loss: 0.126397, acc.: 96.88%] [G loss: 2.958527]\n",
      "2437 [D loss: 0.120617, acc.: 96.88%] [G loss: 2.871062]\n",
      "2438 [D loss: 0.121891, acc.: 96.88%] [G loss: 2.965728]\n",
      "2439 [D loss: 0.118127, acc.: 96.88%] [G loss: 2.980183]\n",
      "2440 [D loss: 0.122874, acc.: 96.88%] [G loss: 2.969984]\n",
      "2441 [D loss: 0.123485, acc.: 96.88%] [G loss: 2.993315]\n",
      "2442 [D loss: 0.124402, acc.: 96.88%] [G loss: 2.903826]\n",
      "2443 [D loss: 0.126561, acc.: 96.88%] [G loss: 3.014499]\n",
      "2444 [D loss: 0.127903, acc.: 96.88%] [G loss: 2.906958]\n",
      "2445 [D loss: 0.122407, acc.: 96.88%] [G loss: 2.932078]\n",
      "2446 [D loss: 0.121030, acc.: 96.88%] [G loss: 2.927821]\n",
      "2447 [D loss: 0.125311, acc.: 96.88%] [G loss: 2.856763]\n",
      "2448 [D loss: 0.127098, acc.: 96.88%] [G loss: 2.913124]\n",
      "2449 [D loss: 0.120113, acc.: 96.88%] [G loss: 2.957232]\n",
      "2450 [D loss: 0.119456, acc.: 96.88%] [G loss: 2.852870]\n",
      "2451 [D loss: 0.114821, acc.: 96.88%] [G loss: 2.934236]\n",
      "2452 [D loss: 0.123284, acc.: 96.88%] [G loss: 2.924026]\n",
      "2453 [D loss: 0.119410, acc.: 96.88%] [G loss: 3.033770]\n",
      "2454 [D loss: 0.131182, acc.: 96.88%] [G loss: 3.000557]\n",
      "2455 [D loss: 0.121591, acc.: 96.88%] [G loss: 3.013304]\n",
      "2456 [D loss: 0.120365, acc.: 96.88%] [G loss: 2.924538]\n",
      "2457 [D loss: 0.120899, acc.: 96.88%] [G loss: 2.977157]\n",
      "2458 [D loss: 0.115698, acc.: 96.88%] [G loss: 2.961130]\n",
      "2459 [D loss: 0.118771, acc.: 96.88%] [G loss: 2.966608]\n",
      "2460 [D loss: 0.126786, acc.: 96.88%] [G loss: 2.948248]\n",
      "2461 [D loss: 0.123549, acc.: 96.88%] [G loss: 2.906912]\n",
      "2462 [D loss: 0.124122, acc.: 96.88%] [G loss: 2.882970]\n",
      "2463 [D loss: 0.117878, acc.: 96.88%] [G loss: 2.877601]\n",
      "2464 [D loss: 0.122284, acc.: 96.88%] [G loss: 2.935786]\n",
      "2465 [D loss: 0.127202, acc.: 96.88%] [G loss: 2.886883]\n",
      "2466 [D loss: 0.122892, acc.: 96.88%] [G loss: 2.909694]\n",
      "2467 [D loss: 0.124898, acc.: 96.88%] [G loss: 2.846439]\n",
      "2468 [D loss: 0.120929, acc.: 96.88%] [G loss: 2.970675]\n",
      "2469 [D loss: 0.129208, acc.: 96.88%] [G loss: 2.977414]\n",
      "2470 [D loss: 0.128357, acc.: 96.88%] [G loss: 2.905802]\n",
      "2471 [D loss: 0.127578, acc.: 96.88%] [G loss: 2.967718]\n",
      "2472 [D loss: 0.117226, acc.: 96.88%] [G loss: 2.965914]\n",
      "2473 [D loss: 0.128140, acc.: 96.88%] [G loss: 2.899738]\n",
      "2474 [D loss: 0.121873, acc.: 96.88%] [G loss: 2.874331]\n",
      "2475 [D loss: 0.116055, acc.: 96.88%] [G loss: 2.947875]\n",
      "2476 [D loss: 0.117367, acc.: 96.88%] [G loss: 2.957336]\n",
      "2477 [D loss: 0.120256, acc.: 96.88%] [G loss: 2.959315]\n",
      "2478 [D loss: 0.122209, acc.: 96.88%] [G loss: 3.023617]\n",
      "2479 [D loss: 0.123992, acc.: 96.88%] [G loss: 3.016135]\n",
      "2480 [D loss: 0.127255, acc.: 96.88%] [G loss: 2.989428]\n",
      "2481 [D loss: 0.117300, acc.: 96.88%] [G loss: 2.945383]\n",
      "2482 [D loss: 0.116289, acc.: 96.88%] [G loss: 2.995138]\n",
      "2483 [D loss: 0.115293, acc.: 96.88%] [G loss: 2.930201]\n",
      "2484 [D loss: 0.122973, acc.: 96.88%] [G loss: 3.012657]\n",
      "2485 [D loss: 0.132232, acc.: 96.88%] [G loss: 2.986413]\n",
      "2486 [D loss: 0.125415, acc.: 96.88%] [G loss: 2.904521]\n",
      "2487 [D loss: 0.131205, acc.: 96.88%] [G loss: 2.938106]\n",
      "2488 [D loss: 0.124848, acc.: 96.88%] [G loss: 2.902419]\n",
      "2489 [D loss: 0.125347, acc.: 96.88%] [G loss: 2.918458]\n",
      "2490 [D loss: 0.126024, acc.: 96.88%] [G loss: 2.974280]\n",
      "2491 [D loss: 0.125131, acc.: 96.88%] [G loss: 2.867834]\n",
      "2492 [D loss: 0.131055, acc.: 96.88%] [G loss: 2.912363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2493 [D loss: 0.116696, acc.: 96.88%] [G loss: 2.827705]\n",
      "2494 [D loss: 0.118822, acc.: 96.88%] [G loss: 2.953501]\n",
      "2495 [D loss: 0.119874, acc.: 96.88%] [G loss: 2.894521]\n",
      "2496 [D loss: 0.124995, acc.: 96.88%] [G loss: 2.968058]\n",
      "2497 [D loss: 0.120109, acc.: 96.88%] [G loss: 2.979945]\n",
      "2498 [D loss: 0.122863, acc.: 96.88%] [G loss: 2.980469]\n",
      "2499 [D loss: 0.115813, acc.: 96.88%] [G loss: 2.959529]\n",
      "2500 [D loss: 0.128691, acc.: 96.88%] [G loss: 2.845093]\n",
      "generated_data\n",
      "2501 [D loss: 0.123411, acc.: 96.88%] [G loss: 2.930257]\n",
      "2502 [D loss: 0.128233, acc.: 96.88%] [G loss: 2.878207]\n",
      "2503 [D loss: 0.112142, acc.: 96.88%] [G loss: 2.834580]\n",
      "2504 [D loss: 0.122016, acc.: 96.88%] [G loss: 2.883618]\n",
      "2505 [D loss: 0.120500, acc.: 96.88%] [G loss: 2.917685]\n",
      "2506 [D loss: 0.128541, acc.: 96.88%] [G loss: 2.883122]\n",
      "2507 [D loss: 0.121484, acc.: 96.88%] [G loss: 2.964108]\n",
      "2508 [D loss: 0.120538, acc.: 96.88%] [G loss: 2.894424]\n",
      "2509 [D loss: 0.120491, acc.: 96.88%] [G loss: 2.877487]\n",
      "2510 [D loss: 0.121525, acc.: 96.88%] [G loss: 2.892354]\n",
      "2511 [D loss: 0.120175, acc.: 96.88%] [G loss: 2.964836]\n",
      "2512 [D loss: 0.121004, acc.: 96.88%] [G loss: 2.867223]\n",
      "2513 [D loss: 0.118365, acc.: 96.88%] [G loss: 2.962917]\n",
      "2514 [D loss: 0.114453, acc.: 96.88%] [G loss: 2.953270]\n",
      "2515 [D loss: 0.125558, acc.: 96.88%] [G loss: 2.934267]\n",
      "2516 [D loss: 0.123532, acc.: 96.88%] [G loss: 2.954720]\n",
      "2517 [D loss: 0.118222, acc.: 96.88%] [G loss: 2.964617]\n",
      "2518 [D loss: 0.117939, acc.: 96.88%] [G loss: 2.873187]\n",
      "2519 [D loss: 0.128420, acc.: 96.88%] [G loss: 2.866402]\n",
      "2520 [D loss: 0.118297, acc.: 96.88%] [G loss: 2.917746]\n",
      "2521 [D loss: 0.119565, acc.: 96.88%] [G loss: 2.962915]\n",
      "2522 [D loss: 0.115845, acc.: 96.88%] [G loss: 2.923331]\n",
      "2523 [D loss: 0.116142, acc.: 96.88%] [G loss: 2.984776]\n",
      "2524 [D loss: 0.118111, acc.: 96.88%] [G loss: 3.012382]\n",
      "2525 [D loss: 0.124361, acc.: 96.88%] [G loss: 2.982036]\n",
      "2526 [D loss: 0.120524, acc.: 96.88%] [G loss: 3.019128]\n",
      "2527 [D loss: 0.123408, acc.: 96.88%] [G loss: 2.985924]\n",
      "2528 [D loss: 0.118477, acc.: 96.88%] [G loss: 3.004194]\n",
      "2529 [D loss: 0.116760, acc.: 96.88%] [G loss: 3.025779]\n",
      "2530 [D loss: 0.124466, acc.: 96.88%] [G loss: 2.888840]\n",
      "2531 [D loss: 0.111489, acc.: 96.88%] [G loss: 2.929982]\n",
      "2532 [D loss: 0.117336, acc.: 96.88%] [G loss: 2.890754]\n",
      "2533 [D loss: 0.124318, acc.: 96.88%] [G loss: 2.963532]\n",
      "2534 [D loss: 0.126283, acc.: 96.88%] [G loss: 2.995662]\n",
      "2535 [D loss: 0.129463, acc.: 96.88%] [G loss: 2.950703]\n",
      "2536 [D loss: 0.121999, acc.: 96.88%] [G loss: 2.924382]\n",
      "2537 [D loss: 0.122432, acc.: 96.88%] [G loss: 2.919329]\n",
      "2538 [D loss: 0.122457, acc.: 96.88%] [G loss: 3.033197]\n",
      "2539 [D loss: 0.118299, acc.: 96.88%] [G loss: 3.027667]\n",
      "2540 [D loss: 0.132202, acc.: 96.88%] [G loss: 3.006336]\n",
      "2541 [D loss: 0.116284, acc.: 96.88%] [G loss: 2.973641]\n",
      "2542 [D loss: 0.118285, acc.: 96.88%] [G loss: 2.976709]\n",
      "2543 [D loss: 0.120176, acc.: 96.88%] [G loss: 3.115545]\n",
      "2544 [D loss: 0.119403, acc.: 96.88%] [G loss: 2.969994]\n",
      "2545 [D loss: 0.113082, acc.: 96.88%] [G loss: 2.993028]\n",
      "2546 [D loss: 0.119196, acc.: 96.88%] [G loss: 2.916209]\n",
      "2547 [D loss: 0.121370, acc.: 96.88%] [G loss: 2.966928]\n",
      "2548 [D loss: 0.122704, acc.: 96.88%] [G loss: 2.918573]\n",
      "2549 [D loss: 0.118751, acc.: 96.88%] [G loss: 3.013525]\n",
      "2550 [D loss: 0.123640, acc.: 96.88%] [G loss: 2.964525]\n",
      "2551 [D loss: 0.126908, acc.: 96.88%] [G loss: 3.113326]\n",
      "2552 [D loss: 0.121815, acc.: 96.88%] [G loss: 2.979681]\n",
      "2553 [D loss: 0.120131, acc.: 96.88%] [G loss: 2.928745]\n",
      "2554 [D loss: 0.122148, acc.: 96.88%] [G loss: 2.940212]\n",
      "2555 [D loss: 0.112787, acc.: 96.88%] [G loss: 2.906925]\n",
      "2556 [D loss: 0.125012, acc.: 96.88%] [G loss: 2.995395]\n",
      "2557 [D loss: 0.121379, acc.: 96.88%] [G loss: 2.955103]\n",
      "2558 [D loss: 0.123529, acc.: 96.88%] [G loss: 3.049771]\n",
      "2559 [D loss: 0.113948, acc.: 96.88%] [G loss: 2.884875]\n",
      "2560 [D loss: 0.116921, acc.: 96.88%] [G loss: 3.074998]\n",
      "2561 [D loss: 0.123143, acc.: 96.88%] [G loss: 2.974643]\n",
      "2562 [D loss: 0.125292, acc.: 96.88%] [G loss: 2.980340]\n",
      "2563 [D loss: 0.121875, acc.: 96.88%] [G loss: 2.929044]\n",
      "2564 [D loss: 0.115550, acc.: 96.88%] [G loss: 3.028563]\n",
      "2565 [D loss: 0.121327, acc.: 96.88%] [G loss: 2.971354]\n",
      "2566 [D loss: 0.124312, acc.: 96.88%] [G loss: 3.023249]\n",
      "2567 [D loss: 0.123783, acc.: 96.88%] [G loss: 2.943039]\n",
      "2568 [D loss: 0.124773, acc.: 96.88%] [G loss: 2.910621]\n",
      "2569 [D loss: 0.117094, acc.: 96.88%] [G loss: 2.930356]\n",
      "2570 [D loss: 0.129654, acc.: 96.88%] [G loss: 2.929236]\n",
      "2571 [D loss: 0.123412, acc.: 96.88%] [G loss: 2.937451]\n",
      "2572 [D loss: 0.118678, acc.: 96.88%] [G loss: 2.962977]\n",
      "2573 [D loss: 0.124695, acc.: 96.88%] [G loss: 2.926579]\n",
      "2574 [D loss: 0.122471, acc.: 96.88%] [G loss: 3.043856]\n",
      "2575 [D loss: 0.129078, acc.: 96.88%] [G loss: 3.074892]\n",
      "2576 [D loss: 0.115694, acc.: 96.88%] [G loss: 2.925245]\n",
      "2577 [D loss: 0.116048, acc.: 96.88%] [G loss: 3.077207]\n",
      "2578 [D loss: 0.121101, acc.: 96.88%] [G loss: 2.992574]\n",
      "2579 [D loss: 0.123005, acc.: 96.88%] [G loss: 2.972461]\n",
      "2580 [D loss: 0.119148, acc.: 96.88%] [G loss: 2.857625]\n",
      "2581 [D loss: 0.116974, acc.: 96.88%] [G loss: 2.955556]\n",
      "2582 [D loss: 0.122912, acc.: 96.88%] [G loss: 3.077078]\n",
      "2583 [D loss: 0.120144, acc.: 96.88%] [G loss: 2.937546]\n",
      "2584 [D loss: 0.119607, acc.: 96.88%] [G loss: 2.995544]\n",
      "2585 [D loss: 0.124312, acc.: 96.88%] [G loss: 2.973478]\n",
      "2586 [D loss: 0.118836, acc.: 96.88%] [G loss: 2.982120]\n",
      "2587 [D loss: 0.126987, acc.: 96.88%] [G loss: 2.919966]\n",
      "2588 [D loss: 0.120619, acc.: 96.88%] [G loss: 3.102014]\n",
      "2589 [D loss: 0.114793, acc.: 96.88%] [G loss: 2.932677]\n",
      "2590 [D loss: 0.122298, acc.: 96.88%] [G loss: 2.926743]\n",
      "2591 [D loss: 0.127323, acc.: 96.88%] [G loss: 2.979834]\n",
      "2592 [D loss: 0.121694, acc.: 96.88%] [G loss: 2.938518]\n",
      "2593 [D loss: 0.120138, acc.: 96.88%] [G loss: 2.864229]\n",
      "2594 [D loss: 0.121239, acc.: 96.88%] [G loss: 2.924396]\n",
      "2595 [D loss: 0.116874, acc.: 96.88%] [G loss: 2.870195]\n",
      "2596 [D loss: 0.123916, acc.: 96.88%] [G loss: 3.034471]\n",
      "2597 [D loss: 0.117299, acc.: 96.88%] [G loss: 2.953417]\n",
      "2598 [D loss: 0.122908, acc.: 96.88%] [G loss: 2.892320]\n",
      "2599 [D loss: 0.121477, acc.: 96.88%] [G loss: 2.946555]\n",
      "2600 [D loss: 0.121104, acc.: 96.88%] [G loss: 2.988255]\n",
      "generated_data\n",
      "2601 [D loss: 0.128022, acc.: 96.88%] [G loss: 2.913539]\n",
      "2602 [D loss: 0.113268, acc.: 96.88%] [G loss: 2.938108]\n",
      "2603 [D loss: 0.118647, acc.: 96.88%] [G loss: 3.048383]\n",
      "2604 [D loss: 0.124357, acc.: 96.88%] [G loss: 2.954596]\n",
      "2605 [D loss: 0.119596, acc.: 96.88%] [G loss: 2.956321]\n",
      "2606 [D loss: 0.111489, acc.: 96.88%] [G loss: 2.960573]\n",
      "2607 [D loss: 0.124066, acc.: 96.88%] [G loss: 3.085895]\n",
      "2608 [D loss: 0.124203, acc.: 96.88%] [G loss: 3.069147]\n",
      "2609 [D loss: 0.120169, acc.: 96.88%] [G loss: 2.927968]\n",
      "2610 [D loss: 0.124845, acc.: 96.88%] [G loss: 2.969043]\n",
      "2611 [D loss: 0.127741, acc.: 96.88%] [G loss: 2.937208]\n",
      "2612 [D loss: 0.119134, acc.: 96.88%] [G loss: 3.010869]\n",
      "2613 [D loss: 0.122314, acc.: 96.88%] [G loss: 2.831160]\n",
      "2614 [D loss: 0.124958, acc.: 96.88%] [G loss: 3.019255]\n",
      "2615 [D loss: 0.117941, acc.: 96.88%] [G loss: 2.951565]\n",
      "2616 [D loss: 0.122229, acc.: 96.88%] [G loss: 2.966732]\n",
      "2617 [D loss: 0.121173, acc.: 96.88%] [G loss: 3.003784]\n",
      "2618 [D loss: 0.124233, acc.: 96.88%] [G loss: 2.932289]\n",
      "2619 [D loss: 0.122015, acc.: 96.88%] [G loss: 2.999304]\n",
      "2620 [D loss: 0.124757, acc.: 96.88%] [G loss: 2.927512]\n",
      "2621 [D loss: 0.120508, acc.: 96.88%] [G loss: 2.938482]\n",
      "2622 [D loss: 0.115113, acc.: 96.88%] [G loss: 3.025691]\n",
      "2623 [D loss: 0.124783, acc.: 96.88%] [G loss: 2.919240]\n",
      "2624 [D loss: 0.121336, acc.: 96.88%] [G loss: 2.972908]\n",
      "2625 [D loss: 0.119334, acc.: 96.88%] [G loss: 2.962592]\n",
      "2626 [D loss: 0.127087, acc.: 96.88%] [G loss: 2.892637]\n",
      "2627 [D loss: 0.117231, acc.: 96.88%] [G loss: 2.932677]\n",
      "2628 [D loss: 0.124393, acc.: 96.88%] [G loss: 3.000117]\n",
      "2629 [D loss: 0.121212, acc.: 96.88%] [G loss: 2.897358]\n",
      "2630 [D loss: 0.123892, acc.: 96.88%] [G loss: 2.913002]\n",
      "2631 [D loss: 0.127664, acc.: 96.88%] [G loss: 2.972555]\n",
      "2632 [D loss: 0.123513, acc.: 96.88%] [G loss: 2.925592]\n",
      "2633 [D loss: 0.126971, acc.: 96.88%] [G loss: 2.887527]\n",
      "2634 [D loss: 0.123605, acc.: 96.88%] [G loss: 3.038092]\n",
      "2635 [D loss: 0.121517, acc.: 96.88%] [G loss: 2.877770]\n",
      "2636 [D loss: 0.118939, acc.: 96.88%] [G loss: 2.938713]\n",
      "2637 [D loss: 0.126057, acc.: 96.88%] [G loss: 2.868352]\n",
      "2638 [D loss: 0.121127, acc.: 96.88%] [G loss: 2.915256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2639 [D loss: 0.115813, acc.: 96.88%] [G loss: 3.036485]\n",
      "2640 [D loss: 0.130621, acc.: 96.88%] [G loss: 2.897943]\n",
      "2641 [D loss: 0.117956, acc.: 96.88%] [G loss: 2.931438]\n",
      "2642 [D loss: 0.120275, acc.: 96.88%] [G loss: 2.949538]\n",
      "2643 [D loss: 0.120953, acc.: 96.88%] [G loss: 2.868708]\n",
      "2644 [D loss: 0.123380, acc.: 96.88%] [G loss: 2.876697]\n",
      "2645 [D loss: 0.116887, acc.: 96.88%] [G loss: 2.956033]\n",
      "2646 [D loss: 0.121198, acc.: 96.88%] [G loss: 2.937701]\n",
      "2647 [D loss: 0.122875, acc.: 96.88%] [G loss: 2.918915]\n",
      "2648 [D loss: 0.121959, acc.: 96.88%] [G loss: 2.899317]\n",
      "2649 [D loss: 0.124263, acc.: 96.88%] [G loss: 2.881702]\n",
      "2650 [D loss: 0.120326, acc.: 96.88%] [G loss: 3.012228]\n",
      "2651 [D loss: 0.123548, acc.: 96.88%] [G loss: 2.919303]\n",
      "2652 [D loss: 0.127557, acc.: 96.88%] [G loss: 2.984226]\n",
      "2653 [D loss: 0.129041, acc.: 96.88%] [G loss: 2.925906]\n",
      "2654 [D loss: 0.127507, acc.: 96.88%] [G loss: 2.901696]\n",
      "2655 [D loss: 0.118579, acc.: 96.88%] [G loss: 2.954912]\n",
      "2656 [D loss: 0.122687, acc.: 96.88%] [G loss: 2.873686]\n",
      "2657 [D loss: 0.114899, acc.: 96.88%] [G loss: 2.909108]\n",
      "2658 [D loss: 0.124177, acc.: 96.88%] [G loss: 2.867625]\n",
      "2659 [D loss: 0.123307, acc.: 96.88%] [G loss: 2.875384]\n",
      "2660 [D loss: 0.119679, acc.: 96.88%] [G loss: 2.968446]\n",
      "2661 [D loss: 0.121442, acc.: 96.88%] [G loss: 2.866100]\n",
      "2662 [D loss: 0.120031, acc.: 96.88%] [G loss: 3.027601]\n",
      "2663 [D loss: 0.128587, acc.: 96.88%] [G loss: 2.940610]\n",
      "2664 [D loss: 0.122860, acc.: 96.88%] [G loss: 2.955253]\n",
      "2665 [D loss: 0.125274, acc.: 96.88%] [G loss: 2.857804]\n",
      "2666 [D loss: 0.123746, acc.: 96.88%] [G loss: 2.895155]\n",
      "2667 [D loss: 0.123311, acc.: 96.88%] [G loss: 2.993278]\n",
      "2668 [D loss: 0.117096, acc.: 96.88%] [G loss: 2.913220]\n",
      "2669 [D loss: 0.122677, acc.: 96.88%] [G loss: 2.966335]\n",
      "2670 [D loss: 0.116246, acc.: 96.88%] [G loss: 2.929236]\n",
      "2671 [D loss: 0.121598, acc.: 96.88%] [G loss: 3.054233]\n",
      "2672 [D loss: 0.113576, acc.: 96.88%] [G loss: 2.950588]\n",
      "2673 [D loss: 0.120437, acc.: 96.88%] [G loss: 2.865098]\n",
      "2674 [D loss: 0.123414, acc.: 96.88%] [G loss: 2.958747]\n",
      "2675 [D loss: 0.122745, acc.: 96.88%] [G loss: 2.911132]\n",
      "2676 [D loss: 0.126423, acc.: 96.88%] [G loss: 2.965045]\n",
      "2677 [D loss: 0.123613, acc.: 96.88%] [G loss: 2.829294]\n",
      "2678 [D loss: 0.123868, acc.: 96.88%] [G loss: 2.812789]\n",
      "2679 [D loss: 0.116238, acc.: 96.88%] [G loss: 2.998552]\n",
      "2680 [D loss: 0.127503, acc.: 96.88%] [G loss: 2.860737]\n",
      "2681 [D loss: 0.122362, acc.: 96.88%] [G loss: 2.882402]\n",
      "2682 [D loss: 0.121675, acc.: 96.88%] [G loss: 2.904382]\n",
      "2683 [D loss: 0.118632, acc.: 96.88%] [G loss: 2.965524]\n",
      "2684 [D loss: 0.114818, acc.: 96.88%] [G loss: 2.978808]\n",
      "2685 [D loss: 0.115051, acc.: 96.88%] [G loss: 3.039533]\n",
      "2686 [D loss: 0.116762, acc.: 96.88%] [G loss: 3.006898]\n",
      "2687 [D loss: 0.122337, acc.: 96.88%] [G loss: 3.005561]\n",
      "2688 [D loss: 0.127387, acc.: 96.88%] [G loss: 2.894994]\n",
      "2689 [D loss: 0.129577, acc.: 96.88%] [G loss: 2.902854]\n",
      "2690 [D loss: 0.125535, acc.: 96.88%] [G loss: 2.904639]\n",
      "2691 [D loss: 0.122857, acc.: 96.88%] [G loss: 2.882688]\n",
      "2692 [D loss: 0.123391, acc.: 96.88%] [G loss: 2.915247]\n",
      "2693 [D loss: 0.120746, acc.: 96.88%] [G loss: 2.956321]\n",
      "2694 [D loss: 0.125253, acc.: 96.88%] [G loss: 2.795151]\n",
      "2695 [D loss: 0.122094, acc.: 96.88%] [G loss: 2.916629]\n",
      "2696 [D loss: 0.124215, acc.: 96.88%] [G loss: 2.919409]\n",
      "2697 [D loss: 0.122554, acc.: 96.88%] [G loss: 2.904116]\n",
      "2698 [D loss: 0.122411, acc.: 96.88%] [G loss: 2.893332]\n",
      "2699 [D loss: 0.121641, acc.: 96.88%] [G loss: 2.926729]\n",
      "2700 [D loss: 0.120557, acc.: 96.88%] [G loss: 2.884426]\n",
      "generated_data\n",
      "2701 [D loss: 0.122400, acc.: 96.88%] [G loss: 2.872554]\n",
      "2702 [D loss: 0.120665, acc.: 96.88%] [G loss: 2.944830]\n",
      "2703 [D loss: 0.123241, acc.: 96.88%] [G loss: 2.956679]\n",
      "2704 [D loss: 0.120466, acc.: 96.88%] [G loss: 2.914925]\n",
      "2705 [D loss: 0.124170, acc.: 96.88%] [G loss: 2.995142]\n",
      "2706 [D loss: 0.122271, acc.: 96.88%] [G loss: 2.859545]\n",
      "2707 [D loss: 0.121767, acc.: 96.88%] [G loss: 3.018011]\n",
      "2708 [D loss: 0.126915, acc.: 96.88%] [G loss: 2.944878]\n",
      "2709 [D loss: 0.114844, acc.: 96.88%] [G loss: 3.068292]\n",
      "2710 [D loss: 0.119964, acc.: 96.88%] [G loss: 2.895734]\n",
      "2711 [D loss: 0.114648, acc.: 96.88%] [G loss: 2.912834]\n",
      "2712 [D loss: 0.119441, acc.: 96.88%] [G loss: 3.041652]\n",
      "2713 [D loss: 0.118413, acc.: 96.88%] [G loss: 2.955667]\n",
      "2714 [D loss: 0.124352, acc.: 96.88%] [G loss: 2.964007]\n",
      "2715 [D loss: 0.121901, acc.: 96.88%] [G loss: 3.011978]\n",
      "2716 [D loss: 0.120163, acc.: 96.88%] [G loss: 2.963556]\n",
      "2717 [D loss: 0.120893, acc.: 96.88%] [G loss: 2.908835]\n",
      "2718 [D loss: 0.113307, acc.: 96.88%] [G loss: 2.994446]\n",
      "2719 [D loss: 0.126265, acc.: 96.88%] [G loss: 2.913897]\n",
      "2720 [D loss: 0.124002, acc.: 96.88%] [G loss: 2.977570]\n",
      "2721 [D loss: 0.124609, acc.: 96.88%] [G loss: 2.983268]\n",
      "2722 [D loss: 0.124455, acc.: 96.88%] [G loss: 2.968776]\n",
      "2723 [D loss: 0.124769, acc.: 96.88%] [G loss: 2.900269]\n",
      "2724 [D loss: 0.122206, acc.: 96.88%] [G loss: 2.920200]\n",
      "2725 [D loss: 0.118541, acc.: 96.88%] [G loss: 3.004522]\n",
      "2726 [D loss: 0.117763, acc.: 96.88%] [G loss: 2.991200]\n",
      "2727 [D loss: 0.123572, acc.: 96.88%] [G loss: 2.951780]\n",
      "2728 [D loss: 0.120832, acc.: 96.88%] [G loss: 2.922355]\n",
      "2729 [D loss: 0.119421, acc.: 96.88%] [G loss: 2.914558]\n",
      "2730 [D loss: 0.122017, acc.: 96.88%] [G loss: 2.961507]\n",
      "2731 [D loss: 0.124676, acc.: 96.88%] [G loss: 2.950675]\n",
      "2732 [D loss: 0.125190, acc.: 96.88%] [G loss: 2.938587]\n",
      "2733 [D loss: 0.123366, acc.: 96.88%] [G loss: 2.960954]\n",
      "2734 [D loss: 0.125992, acc.: 96.88%] [G loss: 2.915128]\n",
      "2735 [D loss: 0.127926, acc.: 96.88%] [G loss: 2.920197]\n",
      "2736 [D loss: 0.125494, acc.: 96.88%] [G loss: 2.926344]\n",
      "2737 [D loss: 0.122883, acc.: 96.88%] [G loss: 2.972553]\n",
      "2738 [D loss: 0.123412, acc.: 96.88%] [G loss: 2.792491]\n",
      "2739 [D loss: 0.119068, acc.: 96.88%] [G loss: 3.012868]\n",
      "2740 [D loss: 0.126836, acc.: 96.88%] [G loss: 2.878855]\n",
      "2741 [D loss: 0.117893, acc.: 96.88%] [G loss: 2.890692]\n",
      "2742 [D loss: 0.122410, acc.: 96.88%] [G loss: 2.914793]\n",
      "2743 [D loss: 0.121932, acc.: 96.88%] [G loss: 2.977933]\n",
      "2744 [D loss: 0.117561, acc.: 96.88%] [G loss: 2.916471]\n",
      "2745 [D loss: 0.120878, acc.: 96.88%] [G loss: 3.001863]\n",
      "2746 [D loss: 0.122225, acc.: 96.88%] [G loss: 2.970292]\n",
      "2747 [D loss: 0.124556, acc.: 96.88%] [G loss: 2.964731]\n",
      "2748 [D loss: 0.124181, acc.: 96.88%] [G loss: 2.955958]\n",
      "2749 [D loss: 0.118993, acc.: 96.88%] [G loss: 2.884038]\n",
      "2750 [D loss: 0.119107, acc.: 96.88%] [G loss: 3.032189]\n",
      "2751 [D loss: 0.122526, acc.: 96.88%] [G loss: 2.957907]\n",
      "2752 [D loss: 0.118899, acc.: 96.88%] [G loss: 2.946190]\n",
      "2753 [D loss: 0.118396, acc.: 96.88%] [G loss: 3.015322]\n",
      "2754 [D loss: 0.133057, acc.: 96.88%] [G loss: 2.886077]\n",
      "2755 [D loss: 0.120728, acc.: 96.88%] [G loss: 2.993656]\n",
      "2756 [D loss: 0.120396, acc.: 96.88%] [G loss: 3.005719]\n",
      "2757 [D loss: 0.112864, acc.: 96.88%] [G loss: 2.998505]\n",
      "2758 [D loss: 0.123101, acc.: 96.88%] [G loss: 3.019885]\n",
      "2759 [D loss: 0.121182, acc.: 96.88%] [G loss: 2.988942]\n",
      "2760 [D loss: 0.120872, acc.: 96.88%] [G loss: 2.955364]\n",
      "2761 [D loss: 0.121950, acc.: 96.88%] [G loss: 2.985783]\n",
      "2762 [D loss: 0.129862, acc.: 96.88%] [G loss: 2.844709]\n",
      "2763 [D loss: 0.124538, acc.: 96.88%] [G loss: 3.041089]\n",
      "2764 [D loss: 0.129780, acc.: 96.88%] [G loss: 3.036462]\n",
      "2765 [D loss: 0.113918, acc.: 96.88%] [G loss: 2.928678]\n",
      "2766 [D loss: 0.116359, acc.: 96.88%] [G loss: 3.044992]\n",
      "2767 [D loss: 0.126447, acc.: 96.88%] [G loss: 2.885522]\n",
      "2768 [D loss: 0.118849, acc.: 96.88%] [G loss: 2.908713]\n",
      "2769 [D loss: 0.129270, acc.: 96.88%] [G loss: 3.009431]\n",
      "2770 [D loss: 0.130021, acc.: 96.88%] [G loss: 2.857576]\n",
      "2771 [D loss: 0.118897, acc.: 96.88%] [G loss: 2.935144]\n",
      "2772 [D loss: 0.127707, acc.: 96.88%] [G loss: 2.973387]\n",
      "2773 [D loss: 0.122124, acc.: 96.88%] [G loss: 3.086116]\n",
      "2774 [D loss: 0.122845, acc.: 96.88%] [G loss: 2.978169]\n",
      "2775 [D loss: 0.126518, acc.: 96.88%] [G loss: 2.924371]\n",
      "2776 [D loss: 0.118346, acc.: 96.88%] [G loss: 2.902355]\n",
      "2777 [D loss: 0.122128, acc.: 96.88%] [G loss: 2.924242]\n",
      "2778 [D loss: 0.119316, acc.: 96.88%] [G loss: 2.930665]\n",
      "2779 [D loss: 0.125660, acc.: 96.88%] [G loss: 2.963775]\n",
      "2780 [D loss: 0.123082, acc.: 96.88%] [G loss: 3.051377]\n",
      "2781 [D loss: 0.120664, acc.: 96.88%] [G loss: 2.984738]\n",
      "2782 [D loss: 0.123632, acc.: 96.88%] [G loss: 2.956851]\n",
      "2783 [D loss: 0.125263, acc.: 96.88%] [G loss: 2.963059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2784 [D loss: 0.119004, acc.: 96.88%] [G loss: 2.906173]\n",
      "2785 [D loss: 0.124214, acc.: 96.88%] [G loss: 2.868945]\n",
      "2786 [D loss: 0.128392, acc.: 96.88%] [G loss: 2.979170]\n",
      "2787 [D loss: 0.117775, acc.: 96.88%] [G loss: 2.963343]\n",
      "2788 [D loss: 0.120344, acc.: 96.88%] [G loss: 2.944960]\n",
      "2789 [D loss: 0.116988, acc.: 96.88%] [G loss: 2.981192]\n",
      "2790 [D loss: 0.112646, acc.: 96.88%] [G loss: 2.992702]\n",
      "2791 [D loss: 0.115481, acc.: 96.88%] [G loss: 2.975422]\n",
      "2792 [D loss: 0.124090, acc.: 96.88%] [G loss: 2.988297]\n",
      "2793 [D loss: 0.125792, acc.: 96.88%] [G loss: 2.910431]\n",
      "2794 [D loss: 0.125293, acc.: 96.88%] [G loss: 2.841265]\n",
      "2795 [D loss: 0.126143, acc.: 96.88%] [G loss: 2.816899]\n",
      "2796 [D loss: 0.113976, acc.: 96.88%] [G loss: 2.959333]\n",
      "2797 [D loss: 0.121615, acc.: 96.88%] [G loss: 2.834522]\n",
      "2798 [D loss: 0.118310, acc.: 96.88%] [G loss: 2.945543]\n",
      "2799 [D loss: 0.123817, acc.: 96.88%] [G loss: 3.073542]\n",
      "2800 [D loss: 0.125407, acc.: 96.88%] [G loss: 2.945959]\n",
      "generated_data\n",
      "2801 [D loss: 0.109633, acc.: 96.88%] [G loss: 2.982595]\n",
      "2802 [D loss: 0.121374, acc.: 96.88%] [G loss: 2.976260]\n",
      "2803 [D loss: 0.122194, acc.: 96.88%] [G loss: 2.893589]\n",
      "2804 [D loss: 0.126982, acc.: 96.88%] [G loss: 2.879897]\n",
      "2805 [D loss: 0.128989, acc.: 96.88%] [G loss: 2.822862]\n",
      "2806 [D loss: 0.113941, acc.: 96.88%] [G loss: 3.039568]\n",
      "2807 [D loss: 0.123441, acc.: 96.88%] [G loss: 2.919492]\n",
      "2808 [D loss: 0.116471, acc.: 96.88%] [G loss: 2.903786]\n",
      "2809 [D loss: 0.111393, acc.: 96.88%] [G loss: 2.947162]\n",
      "2810 [D loss: 0.121516, acc.: 96.88%] [G loss: 3.072567]\n",
      "2811 [D loss: 0.124049, acc.: 96.88%] [G loss: 2.983034]\n",
      "2812 [D loss: 0.124700, acc.: 96.88%] [G loss: 2.957300]\n",
      "2813 [D loss: 0.122942, acc.: 96.88%] [G loss: 2.920231]\n",
      "2814 [D loss: 0.124537, acc.: 96.88%] [G loss: 2.973675]\n",
      "2815 [D loss: 0.119987, acc.: 96.88%] [G loss: 2.925255]\n",
      "2816 [D loss: 0.118993, acc.: 96.88%] [G loss: 2.865834]\n",
      "2817 [D loss: 0.118294, acc.: 96.88%] [G loss: 2.966284]\n",
      "2818 [D loss: 0.121203, acc.: 96.88%] [G loss: 2.913516]\n",
      "2819 [D loss: 0.123721, acc.: 96.88%] [G loss: 2.877339]\n",
      "2820 [D loss: 0.127461, acc.: 96.88%] [G loss: 2.861627]\n",
      "2821 [D loss: 0.115268, acc.: 96.88%] [G loss: 2.899349]\n",
      "2822 [D loss: 0.120638, acc.: 96.88%] [G loss: 2.921989]\n",
      "2823 [D loss: 0.118259, acc.: 96.88%] [G loss: 2.942901]\n",
      "2824 [D loss: 0.124409, acc.: 96.88%] [G loss: 3.009743]\n",
      "2825 [D loss: 0.126165, acc.: 96.88%] [G loss: 3.055560]\n",
      "2826 [D loss: 0.112254, acc.: 96.88%] [G loss: 2.862083]\n",
      "2827 [D loss: 0.119811, acc.: 96.88%] [G loss: 2.999839]\n",
      "2828 [D loss: 0.117958, acc.: 96.88%] [G loss: 3.005897]\n",
      "2829 [D loss: 0.123828, acc.: 96.88%] [G loss: 2.843315]\n",
      "2830 [D loss: 0.114940, acc.: 96.88%] [G loss: 2.977930]\n",
      "2831 [D loss: 0.122485, acc.: 96.88%] [G loss: 2.923753]\n",
      "2832 [D loss: 0.128546, acc.: 96.88%] [G loss: 3.111613]\n",
      "2833 [D loss: 0.117634, acc.: 96.88%] [G loss: 2.948492]\n",
      "2834 [D loss: 0.118013, acc.: 96.88%] [G loss: 2.998508]\n",
      "2835 [D loss: 0.122973, acc.: 96.88%] [G loss: 2.972291]\n",
      "2836 [D loss: 0.122367, acc.: 96.88%] [G loss: 2.966921]\n",
      "2837 [D loss: 0.118804, acc.: 96.88%] [G loss: 3.051733]\n",
      "2838 [D loss: 0.119283, acc.: 96.88%] [G loss: 3.013206]\n",
      "2839 [D loss: 0.124646, acc.: 96.88%] [G loss: 2.897793]\n",
      "2840 [D loss: 0.121082, acc.: 96.88%] [G loss: 3.000229]\n",
      "2841 [D loss: 0.118862, acc.: 96.88%] [G loss: 2.979022]\n",
      "2842 [D loss: 0.127699, acc.: 96.88%] [G loss: 2.925424]\n",
      "2843 [D loss: 0.116327, acc.: 96.88%] [G loss: 2.910817]\n",
      "2844 [D loss: 0.122775, acc.: 96.88%] [G loss: 2.878282]\n",
      "2845 [D loss: 0.120650, acc.: 96.88%] [G loss: 2.851422]\n",
      "2846 [D loss: 0.115907, acc.: 96.88%] [G loss: 2.986114]\n",
      "2847 [D loss: 0.123444, acc.: 96.88%] [G loss: 2.919343]\n",
      "2848 [D loss: 0.127036, acc.: 96.88%] [G loss: 2.891447]\n",
      "2849 [D loss: 0.124529, acc.: 96.88%] [G loss: 2.874457]\n",
      "2850 [D loss: 0.121310, acc.: 96.88%] [G loss: 2.866671]\n",
      "2851 [D loss: 0.117676, acc.: 96.88%] [G loss: 2.949269]\n",
      "2852 [D loss: 0.123877, acc.: 96.88%] [G loss: 2.904624]\n",
      "2853 [D loss: 0.128688, acc.: 96.88%] [G loss: 2.887914]\n",
      "2854 [D loss: 0.124045, acc.: 96.88%] [G loss: 2.919926]\n",
      "2855 [D loss: 0.123181, acc.: 96.88%] [G loss: 2.979623]\n",
      "2856 [D loss: 0.121014, acc.: 96.88%] [G loss: 2.954377]\n",
      "2857 [D loss: 0.123902, acc.: 96.88%] [G loss: 2.841952]\n",
      "2858 [D loss: 0.116472, acc.: 96.88%] [G loss: 2.927844]\n",
      "2859 [D loss: 0.127559, acc.: 96.88%] [G loss: 2.948217]\n",
      "2860 [D loss: 0.114327, acc.: 96.88%] [G loss: 2.971493]\n",
      "2861 [D loss: 0.120199, acc.: 96.88%] [G loss: 2.930860]\n",
      "2862 [D loss: 0.128209, acc.: 96.88%] [G loss: 2.898710]\n",
      "2863 [D loss: 0.120629, acc.: 96.88%] [G loss: 2.847997]\n",
      "2864 [D loss: 0.124713, acc.: 96.88%] [G loss: 2.912462]\n",
      "2865 [D loss: 0.118142, acc.: 96.88%] [G loss: 2.912489]\n",
      "2866 [D loss: 0.132691, acc.: 96.88%] [G loss: 2.898869]\n",
      "2867 [D loss: 0.115006, acc.: 96.88%] [G loss: 2.899271]\n",
      "2868 [D loss: 0.130143, acc.: 96.88%] [G loss: 3.002326]\n",
      "2869 [D loss: 0.122241, acc.: 96.88%] [G loss: 2.981140]\n",
      "2870 [D loss: 0.121133, acc.: 96.88%] [G loss: 2.943397]\n",
      "2871 [D loss: 0.120853, acc.: 96.88%] [G loss: 2.943722]\n",
      "2872 [D loss: 0.126832, acc.: 96.88%] [G loss: 2.917570]\n",
      "2873 [D loss: 0.121496, acc.: 96.88%] [G loss: 2.908557]\n",
      "2874 [D loss: 0.118833, acc.: 96.88%] [G loss: 2.923800]\n",
      "2875 [D loss: 0.117308, acc.: 96.88%] [G loss: 2.868933]\n",
      "2876 [D loss: 0.129648, acc.: 96.88%] [G loss: 2.885743]\n",
      "2877 [D loss: 0.123725, acc.: 96.88%] [G loss: 2.901514]\n",
      "2878 [D loss: 0.122258, acc.: 96.88%] [G loss: 2.858946]\n",
      "2879 [D loss: 0.121434, acc.: 96.88%] [G loss: 2.867952]\n",
      "2880 [D loss: 0.116765, acc.: 96.88%] [G loss: 2.906902]\n",
      "2881 [D loss: 0.125053, acc.: 96.88%] [G loss: 2.959086]\n",
      "2882 [D loss: 0.117550, acc.: 96.88%] [G loss: 2.936827]\n",
      "2883 [D loss: 0.115735, acc.: 96.88%] [G loss: 2.920323]\n",
      "2884 [D loss: 0.120850, acc.: 96.88%] [G loss: 2.996193]\n",
      "2885 [D loss: 0.111329, acc.: 96.88%] [G loss: 2.975984]\n",
      "2886 [D loss: 0.114331, acc.: 96.88%] [G loss: 3.097882]\n",
      "2887 [D loss: 0.127353, acc.: 96.88%] [G loss: 2.996965]\n",
      "2888 [D loss: 0.117560, acc.: 96.88%] [G loss: 2.911656]\n",
      "2889 [D loss: 0.121880, acc.: 96.88%] [G loss: 3.042302]\n",
      "2890 [D loss: 0.125463, acc.: 96.88%] [G loss: 2.856056]\n",
      "2891 [D loss: 0.114528, acc.: 96.88%] [G loss: 2.965070]\n",
      "2892 [D loss: 0.122362, acc.: 96.88%] [G loss: 2.970910]\n",
      "2893 [D loss: 0.127159, acc.: 96.88%] [G loss: 2.913679]\n",
      "2894 [D loss: 0.125825, acc.: 96.88%] [G loss: 2.861746]\n",
      "2895 [D loss: 0.124629, acc.: 96.88%] [G loss: 2.811788]\n",
      "2896 [D loss: 0.121969, acc.: 96.88%] [G loss: 3.027873]\n",
      "2897 [D loss: 0.117572, acc.: 96.88%] [G loss: 2.831268]\n",
      "2898 [D loss: 0.123851, acc.: 96.88%] [G loss: 3.015241]\n",
      "2899 [D loss: 0.119354, acc.: 96.88%] [G loss: 2.940125]\n",
      "2900 [D loss: 0.118146, acc.: 96.88%] [G loss: 2.841872]\n",
      "generated_data\n",
      "2901 [D loss: 0.118785, acc.: 96.88%] [G loss: 3.186583]\n",
      "2902 [D loss: 0.127548, acc.: 96.88%] [G loss: 3.004817]\n",
      "2903 [D loss: 0.126463, acc.: 96.88%] [G loss: 2.957729]\n",
      "2904 [D loss: 0.119196, acc.: 96.88%] [G loss: 2.939104]\n",
      "2905 [D loss: 0.121372, acc.: 96.88%] [G loss: 2.911226]\n",
      "2906 [D loss: 0.122645, acc.: 96.88%] [G loss: 3.042270]\n",
      "2907 [D loss: 0.122451, acc.: 96.88%] [G loss: 2.883884]\n",
      "2908 [D loss: 0.124199, acc.: 96.88%] [G loss: 2.996791]\n",
      "2909 [D loss: 0.115353, acc.: 96.88%] [G loss: 2.907799]\n",
      "2910 [D loss: 0.112990, acc.: 96.88%] [G loss: 2.896291]\n",
      "2911 [D loss: 0.118524, acc.: 96.88%] [G loss: 3.016889]\n",
      "2912 [D loss: 0.122755, acc.: 96.88%] [G loss: 2.967389]\n",
      "2913 [D loss: 0.115246, acc.: 96.88%] [G loss: 2.887956]\n",
      "2914 [D loss: 0.122867, acc.: 96.88%] [G loss: 2.864238]\n",
      "2915 [D loss: 0.120544, acc.: 96.88%] [G loss: 2.872405]\n",
      "2916 [D loss: 0.118144, acc.: 96.88%] [G loss: 2.942406]\n",
      "2917 [D loss: 0.127121, acc.: 96.88%] [G loss: 2.846793]\n",
      "2918 [D loss: 0.117704, acc.: 96.88%] [G loss: 2.971873]\n",
      "2919 [D loss: 0.121747, acc.: 96.88%] [G loss: 2.921771]\n",
      "2920 [D loss: 0.127998, acc.: 96.88%] [G loss: 2.965086]\n",
      "2921 [D loss: 0.119021, acc.: 96.88%] [G loss: 2.964809]\n",
      "2922 [D loss: 0.115039, acc.: 96.88%] [G loss: 2.961169]\n",
      "2923 [D loss: 0.127440, acc.: 96.88%] [G loss: 2.980527]\n",
      "2924 [D loss: 0.123480, acc.: 96.88%] [G loss: 2.879468]\n",
      "2925 [D loss: 0.120590, acc.: 96.88%] [G loss: 2.873751]\n",
      "2926 [D loss: 0.116282, acc.: 96.88%] [G loss: 2.927205]\n",
      "2927 [D loss: 0.131094, acc.: 96.88%] [G loss: 2.974758]\n",
      "2928 [D loss: 0.120253, acc.: 96.88%] [G loss: 2.900255]\n",
      "2929 [D loss: 0.116939, acc.: 96.88%] [G loss: 2.860363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2930 [D loss: 0.115166, acc.: 96.88%] [G loss: 2.984827]\n",
      "2931 [D loss: 0.124865, acc.: 96.88%] [G loss: 2.963652]\n",
      "2932 [D loss: 0.121846, acc.: 96.88%] [G loss: 2.956879]\n",
      "2933 [D loss: 0.126030, acc.: 96.88%] [G loss: 2.918222]\n",
      "2934 [D loss: 0.120805, acc.: 96.88%] [G loss: 2.897759]\n",
      "2935 [D loss: 0.122141, acc.: 96.88%] [G loss: 2.931293]\n",
      "2936 [D loss: 0.119776, acc.: 96.88%] [G loss: 3.034021]\n",
      "2937 [D loss: 0.125883, acc.: 96.88%] [G loss: 3.012951]\n",
      "2938 [D loss: 0.120776, acc.: 96.88%] [G loss: 2.923925]\n",
      "2939 [D loss: 0.125510, acc.: 96.88%] [G loss: 2.895202]\n",
      "2940 [D loss: 0.122929, acc.: 96.88%] [G loss: 2.939490]\n",
      "2941 [D loss: 0.116579, acc.: 96.88%] [G loss: 2.866972]\n",
      "2942 [D loss: 0.125477, acc.: 96.88%] [G loss: 2.967147]\n",
      "2943 [D loss: 0.123112, acc.: 96.88%] [G loss: 2.873890]\n",
      "2944 [D loss: 0.121342, acc.: 96.88%] [G loss: 2.942595]\n",
      "2945 [D loss: 0.124649, acc.: 96.88%] [G loss: 2.878727]\n",
      "2946 [D loss: 0.124461, acc.: 96.88%] [G loss: 2.933029]\n",
      "2947 [D loss: 0.121444, acc.: 96.88%] [G loss: 2.894651]\n",
      "2948 [D loss: 0.121896, acc.: 96.88%] [G loss: 2.832607]\n",
      "2949 [D loss: 0.124465, acc.: 96.88%] [G loss: 2.882362]\n",
      "2950 [D loss: 0.127063, acc.: 96.88%] [G loss: 2.868192]\n",
      "2951 [D loss: 0.118606, acc.: 96.88%] [G loss: 2.870769]\n",
      "2952 [D loss: 0.124910, acc.: 96.88%] [G loss: 2.930205]\n",
      "2953 [D loss: 0.123376, acc.: 96.88%] [G loss: 2.982114]\n",
      "2954 [D loss: 0.117898, acc.: 96.88%] [G loss: 2.868690]\n",
      "2955 [D loss: 0.119615, acc.: 96.88%] [G loss: 3.016740]\n",
      "2956 [D loss: 0.117224, acc.: 96.88%] [G loss: 2.908110]\n",
      "2957 [D loss: 0.120055, acc.: 96.88%] [G loss: 2.910635]\n",
      "2958 [D loss: 0.116390, acc.: 96.88%] [G loss: 3.054062]\n",
      "2959 [D loss: 0.122321, acc.: 96.88%] [G loss: 2.885958]\n",
      "2960 [D loss: 0.117847, acc.: 96.88%] [G loss: 2.864382]\n",
      "2961 [D loss: 0.118540, acc.: 96.88%] [G loss: 2.938022]\n",
      "2962 [D loss: 0.125986, acc.: 96.88%] [G loss: 2.956875]\n",
      "2963 [D loss: 0.119835, acc.: 96.88%] [G loss: 2.929664]\n",
      "2964 [D loss: 0.115276, acc.: 96.88%] [G loss: 2.948210]\n",
      "2965 [D loss: 0.127964, acc.: 96.88%] [G loss: 2.884588]\n",
      "2966 [D loss: 0.122273, acc.: 96.88%] [G loss: 2.955922]\n",
      "2967 [D loss: 0.120725, acc.: 96.88%] [G loss: 2.872428]\n",
      "2968 [D loss: 0.118764, acc.: 96.88%] [G loss: 2.946846]\n",
      "2969 [D loss: 0.126225, acc.: 96.88%] [G loss: 2.937341]\n",
      "2970 [D loss: 0.127908, acc.: 96.88%] [G loss: 2.970630]\n",
      "2971 [D loss: 0.128946, acc.: 96.88%] [G loss: 2.879572]\n",
      "2972 [D loss: 0.120584, acc.: 96.88%] [G loss: 3.008338]\n",
      "2973 [D loss: 0.117640, acc.: 96.88%] [G loss: 2.887985]\n",
      "2974 [D loss: 0.115565, acc.: 96.88%] [G loss: 2.957231]\n",
      "2975 [D loss: 0.126120, acc.: 96.88%] [G loss: 2.914634]\n",
      "2976 [D loss: 0.123567, acc.: 96.88%] [G loss: 2.978532]\n",
      "2977 [D loss: 0.115629, acc.: 96.88%] [G loss: 2.871890]\n",
      "2978 [D loss: 0.125804, acc.: 96.88%] [G loss: 2.919092]\n",
      "2979 [D loss: 0.118188, acc.: 96.88%] [G loss: 2.948465]\n",
      "2980 [D loss: 0.120356, acc.: 96.88%] [G loss: 2.903823]\n",
      "2981 [D loss: 0.117183, acc.: 96.88%] [G loss: 2.926033]\n",
      "2982 [D loss: 0.112307, acc.: 96.88%] [G loss: 2.944818]\n",
      "2983 [D loss: 0.128302, acc.: 96.88%] [G loss: 2.972590]\n",
      "2984 [D loss: 0.121220, acc.: 96.88%] [G loss: 3.005832]\n",
      "2985 [D loss: 0.122279, acc.: 96.88%] [G loss: 2.946603]\n",
      "2986 [D loss: 0.122597, acc.: 96.88%] [G loss: 2.946042]\n",
      "2987 [D loss: 0.122085, acc.: 96.88%] [G loss: 2.849388]\n",
      "2988 [D loss: 0.119986, acc.: 96.88%] [G loss: 2.790834]\n",
      "2989 [D loss: 0.114770, acc.: 96.88%] [G loss: 2.921384]\n",
      "2990 [D loss: 0.127891, acc.: 96.88%] [G loss: 2.926875]\n",
      "2991 [D loss: 0.123600, acc.: 96.88%] [G loss: 2.988064]\n",
      "2992 [D loss: 0.128545, acc.: 96.88%] [G loss: 3.014501]\n",
      "2993 [D loss: 0.126217, acc.: 96.88%] [G loss: 3.084690]\n",
      "2994 [D loss: 0.121167, acc.: 96.88%] [G loss: 2.848149]\n",
      "2995 [D loss: 0.115183, acc.: 96.88%] [G loss: 2.935941]\n",
      "2996 [D loss: 0.123807, acc.: 96.88%] [G loss: 2.885863]\n",
      "2997 [D loss: 0.124705, acc.: 96.88%] [G loss: 2.953652]\n",
      "2998 [D loss: 0.121293, acc.: 96.88%] [G loss: 2.891433]\n",
      "2999 [D loss: 0.119456, acc.: 96.88%] [G loss: 2.964370]\n",
      "3000 [D loss: 0.117833, acc.: 96.88%] [G loss: 2.944315]\n",
      "generated_data\n",
      "3001 [D loss: 0.124832, acc.: 96.88%] [G loss: 2.960247]\n",
      "3002 [D loss: 0.120018, acc.: 96.88%] [G loss: 3.023359]\n",
      "3003 [D loss: 0.121657, acc.: 96.88%] [G loss: 2.948710]\n",
      "3004 [D loss: 0.125325, acc.: 96.88%] [G loss: 2.890215]\n",
      "3005 [D loss: 0.119259, acc.: 96.88%] [G loss: 2.879642]\n",
      "3006 [D loss: 0.122369, acc.: 96.88%] [G loss: 2.840184]\n",
      "3007 [D loss: 0.110191, acc.: 96.88%] [G loss: 2.931195]\n",
      "3008 [D loss: 0.120107, acc.: 96.88%] [G loss: 2.908124]\n",
      "3009 [D loss: 0.124968, acc.: 96.88%] [G loss: 2.935873]\n",
      "3010 [D loss: 0.117581, acc.: 96.88%] [G loss: 3.069789]\n",
      "3011 [D loss: 0.123751, acc.: 96.88%] [G loss: 2.936312]\n",
      "3012 [D loss: 0.122734, acc.: 96.88%] [G loss: 2.968315]\n",
      "3013 [D loss: 0.120695, acc.: 96.88%] [G loss: 2.968152]\n",
      "3014 [D loss: 0.122139, acc.: 96.88%] [G loss: 2.969646]\n",
      "3015 [D loss: 0.118318, acc.: 96.88%] [G loss: 2.975182]\n",
      "3016 [D loss: 0.118761, acc.: 96.88%] [G loss: 2.989944]\n",
      "3017 [D loss: 0.119316, acc.: 96.88%] [G loss: 2.900547]\n",
      "3018 [D loss: 0.130318, acc.: 96.88%] [G loss: 2.928399]\n",
      "3019 [D loss: 0.126241, acc.: 96.88%] [G loss: 3.084940]\n",
      "3020 [D loss: 0.126116, acc.: 96.88%] [G loss: 2.909944]\n",
      "3021 [D loss: 0.118786, acc.: 96.88%] [G loss: 2.945300]\n",
      "3022 [D loss: 0.123757, acc.: 96.88%] [G loss: 3.028308]\n",
      "3023 [D loss: 0.123589, acc.: 96.88%] [G loss: 2.942689]\n",
      "3024 [D loss: 0.119046, acc.: 96.88%] [G loss: 2.936753]\n",
      "3025 [D loss: 0.117286, acc.: 96.88%] [G loss: 2.940969]\n",
      "3026 [D loss: 0.118558, acc.: 96.88%] [G loss: 2.959926]\n",
      "3027 [D loss: 0.126407, acc.: 96.88%] [G loss: 2.951966]\n",
      "3028 [D loss: 0.129355, acc.: 96.88%] [G loss: 2.969426]\n",
      "3029 [D loss: 0.121585, acc.: 96.88%] [G loss: 2.920822]\n",
      "3030 [D loss: 0.117931, acc.: 96.88%] [G loss: 2.915472]\n",
      "3031 [D loss: 0.118249, acc.: 96.88%] [G loss: 2.885080]\n",
      "3032 [D loss: 0.122513, acc.: 96.88%] [G loss: 2.993588]\n",
      "3033 [D loss: 0.124270, acc.: 96.88%] [G loss: 2.857565]\n",
      "3034 [D loss: 0.110874, acc.: 96.88%] [G loss: 2.975765]\n",
      "3035 [D loss: 0.126081, acc.: 96.88%] [G loss: 2.951598]\n",
      "3036 [D loss: 0.125285, acc.: 96.88%] [G loss: 2.905551]\n",
      "3037 [D loss: 0.122062, acc.: 96.88%] [G loss: 2.923501]\n",
      "3038 [D loss: 0.124654, acc.: 96.88%] [G loss: 2.910572]\n",
      "3039 [D loss: 0.124549, acc.: 96.88%] [G loss: 2.900484]\n",
      "3040 [D loss: 0.123027, acc.: 96.88%] [G loss: 2.912138]\n",
      "3041 [D loss: 0.120222, acc.: 96.88%] [G loss: 2.978842]\n",
      "3042 [D loss: 0.123963, acc.: 96.88%] [G loss: 3.068447]\n",
      "3043 [D loss: 0.122226, acc.: 96.88%] [G loss: 2.963964]\n",
      "3044 [D loss: 0.129059, acc.: 96.88%] [G loss: 2.951073]\n",
      "3045 [D loss: 0.126176, acc.: 96.88%] [G loss: 2.860035]\n",
      "3046 [D loss: 0.115259, acc.: 96.88%] [G loss: 2.844769]\n",
      "3047 [D loss: 0.121140, acc.: 96.88%] [G loss: 2.915121]\n",
      "3048 [D loss: 0.120082, acc.: 96.88%] [G loss: 3.007061]\n",
      "3049 [D loss: 0.125321, acc.: 96.88%] [G loss: 2.870869]\n",
      "3050 [D loss: 0.116533, acc.: 96.88%] [G loss: 2.905524]\n",
      "3051 [D loss: 0.126665, acc.: 96.88%] [G loss: 3.007029]\n",
      "3052 [D loss: 0.127863, acc.: 96.88%] [G loss: 2.893784]\n",
      "3053 [D loss: 0.122321, acc.: 96.88%] [G loss: 2.891227]\n",
      "3054 [D loss: 0.117287, acc.: 96.88%] [G loss: 2.852021]\n",
      "3055 [D loss: 0.122951, acc.: 96.88%] [G loss: 2.974190]\n",
      "3056 [D loss: 0.125431, acc.: 96.88%] [G loss: 3.000458]\n",
      "3057 [D loss: 0.118124, acc.: 96.88%] [G loss: 2.970628]\n",
      "3058 [D loss: 0.119467, acc.: 96.88%] [G loss: 2.972297]\n",
      "3059 [D loss: 0.122749, acc.: 96.88%] [G loss: 2.966675]\n",
      "3060 [D loss: 0.125574, acc.: 96.88%] [G loss: 2.944717]\n",
      "3061 [D loss: 0.117875, acc.: 96.88%] [G loss: 2.914340]\n",
      "3062 [D loss: 0.127667, acc.: 96.88%] [G loss: 2.970437]\n",
      "3063 [D loss: 0.115031, acc.: 96.88%] [G loss: 2.862617]\n",
      "3064 [D loss: 0.119794, acc.: 96.88%] [G loss: 2.928512]\n",
      "3065 [D loss: 0.120654, acc.: 96.88%] [G loss: 2.967083]\n",
      "3066 [D loss: 0.124270, acc.: 96.88%] [G loss: 2.986099]\n",
      "3067 [D loss: 0.120036, acc.: 96.88%] [G loss: 2.926472]\n",
      "3068 [D loss: 0.121878, acc.: 96.88%] [G loss: 2.946731]\n",
      "3069 [D loss: 0.119130, acc.: 96.88%] [G loss: 3.059010]\n",
      "3070 [D loss: 0.121696, acc.: 96.88%] [G loss: 2.915596]\n",
      "3071 [D loss: 0.128012, acc.: 96.88%] [G loss: 2.914953]\n",
      "3072 [D loss: 0.122987, acc.: 96.88%] [G loss: 2.943478]\n",
      "3073 [D loss: 0.121164, acc.: 96.88%] [G loss: 2.995940]\n",
      "3074 [D loss: 0.127257, acc.: 96.88%] [G loss: 2.932733]\n",
      "3075 [D loss: 0.117694, acc.: 96.88%] [G loss: 2.890337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3076 [D loss: 0.120546, acc.: 96.88%] [G loss: 2.914532]\n",
      "3077 [D loss: 0.125168, acc.: 96.88%] [G loss: 3.049744]\n",
      "3078 [D loss: 0.119187, acc.: 96.88%] [G loss: 3.057367]\n",
      "3079 [D loss: 0.121802, acc.: 96.88%] [G loss: 3.128494]\n",
      "3080 [D loss: 0.118913, acc.: 96.88%] [G loss: 2.953406]\n",
      "3081 [D loss: 0.123093, acc.: 96.88%] [G loss: 2.974801]\n",
      "3082 [D loss: 0.116902, acc.: 96.88%] [G loss: 2.981213]\n",
      "3083 [D loss: 0.115889, acc.: 96.88%] [G loss: 2.931542]\n",
      "3084 [D loss: 0.125031, acc.: 96.88%] [G loss: 2.884766]\n",
      "3085 [D loss: 0.123496, acc.: 96.88%] [G loss: 2.876145]\n",
      "3086 [D loss: 0.116910, acc.: 96.88%] [G loss: 2.935549]\n",
      "3087 [D loss: 0.119992, acc.: 96.88%] [G loss: 2.936927]\n",
      "3088 [D loss: 0.113468, acc.: 96.88%] [G loss: 2.917609]\n",
      "3089 [D loss: 0.128028, acc.: 96.88%] [G loss: 2.988295]\n",
      "3090 [D loss: 0.119766, acc.: 96.88%] [G loss: 2.892179]\n",
      "3091 [D loss: 0.123068, acc.: 96.88%] [G loss: 2.949875]\n",
      "3092 [D loss: 0.122603, acc.: 96.88%] [G loss: 2.922051]\n",
      "3093 [D loss: 0.123922, acc.: 96.88%] [G loss: 2.930568]\n",
      "3094 [D loss: 0.126507, acc.: 96.88%] [G loss: 2.931832]\n",
      "3095 [D loss: 0.124233, acc.: 96.88%] [G loss: 2.836073]\n",
      "3096 [D loss: 0.128798, acc.: 96.88%] [G loss: 2.937081]\n",
      "3097 [D loss: 0.122853, acc.: 96.88%] [G loss: 2.808397]\n",
      "3098 [D loss: 0.127981, acc.: 96.88%] [G loss: 2.900057]\n",
      "3099 [D loss: 0.122868, acc.: 96.88%] [G loss: 2.862920]\n",
      "3100 [D loss: 0.126777, acc.: 96.88%] [G loss: 2.955875]\n",
      "generated_data\n",
      "3101 [D loss: 0.117480, acc.: 96.88%] [G loss: 2.955796]\n",
      "3102 [D loss: 0.125134, acc.: 96.88%] [G loss: 2.889683]\n",
      "3103 [D loss: 0.121587, acc.: 96.88%] [G loss: 2.883539]\n",
      "3104 [D loss: 0.126528, acc.: 96.88%] [G loss: 2.794659]\n",
      "3105 [D loss: 0.127863, acc.: 96.88%] [G loss: 2.902537]\n",
      "3106 [D loss: 0.116088, acc.: 96.88%] [G loss: 2.944516]\n",
      "3107 [D loss: 0.124216, acc.: 96.88%] [G loss: 2.878082]\n",
      "3108 [D loss: 0.123831, acc.: 96.88%] [G loss: 2.848235]\n",
      "3109 [D loss: 0.122089, acc.: 96.88%] [G loss: 2.862325]\n",
      "3110 [D loss: 0.121309, acc.: 96.88%] [G loss: 2.911600]\n",
      "3111 [D loss: 0.117869, acc.: 96.88%] [G loss: 2.901171]\n",
      "3112 [D loss: 0.120947, acc.: 96.88%] [G loss: 2.885566]\n",
      "3113 [D loss: 0.120892, acc.: 96.88%] [G loss: 2.888588]\n",
      "3114 [D loss: 0.119559, acc.: 96.88%] [G loss: 2.935422]\n",
      "3115 [D loss: 0.122994, acc.: 96.88%] [G loss: 2.860475]\n",
      "3116 [D loss: 0.120130, acc.: 96.88%] [G loss: 3.022435]\n",
      "3117 [D loss: 0.119536, acc.: 96.88%] [G loss: 2.915714]\n",
      "3118 [D loss: 0.118017, acc.: 96.88%] [G loss: 2.938851]\n",
      "3119 [D loss: 0.128985, acc.: 96.88%] [G loss: 3.017692]\n",
      "3120 [D loss: 0.125263, acc.: 96.88%] [G loss: 2.993575]\n",
      "3121 [D loss: 0.126736, acc.: 96.88%] [G loss: 2.933186]\n",
      "3122 [D loss: 0.126871, acc.: 96.88%] [G loss: 2.920682]\n",
      "3123 [D loss: 0.126818, acc.: 96.88%] [G loss: 2.886307]\n",
      "3124 [D loss: 0.119600, acc.: 96.88%] [G loss: 2.828136]\n",
      "3125 [D loss: 0.126057, acc.: 96.88%] [G loss: 2.890258]\n",
      "3126 [D loss: 0.122849, acc.: 96.88%] [G loss: 2.889784]\n",
      "3127 [D loss: 0.119260, acc.: 96.88%] [G loss: 2.870323]\n",
      "3128 [D loss: 0.124595, acc.: 96.88%] [G loss: 2.865813]\n",
      "3129 [D loss: 0.130028, acc.: 96.88%] [G loss: 2.903423]\n",
      "3130 [D loss: 0.121765, acc.: 96.88%] [G loss: 2.942151]\n",
      "3131 [D loss: 0.127621, acc.: 96.88%] [G loss: 2.804563]\n",
      "3132 [D loss: 0.124132, acc.: 96.88%] [G loss: 2.785049]\n",
      "3133 [D loss: 0.118548, acc.: 96.88%] [G loss: 2.935837]\n",
      "3134 [D loss: 0.122425, acc.: 96.88%] [G loss: 3.046989]\n",
      "3135 [D loss: 0.119295, acc.: 96.88%] [G loss: 2.893078]\n",
      "3136 [D loss: 0.117730, acc.: 96.88%] [G loss: 2.912979]\n",
      "3137 [D loss: 0.120461, acc.: 96.88%] [G loss: 2.913938]\n",
      "3138 [D loss: 0.112341, acc.: 96.88%] [G loss: 2.960310]\n",
      "3139 [D loss: 0.122100, acc.: 96.88%] [G loss: 3.089290]\n",
      "3140 [D loss: 0.117088, acc.: 96.88%] [G loss: 2.990387]\n",
      "3141 [D loss: 0.127405, acc.: 96.88%] [G loss: 2.925249]\n",
      "3142 [D loss: 0.123684, acc.: 96.88%] [G loss: 2.903943]\n",
      "3143 [D loss: 0.120878, acc.: 96.88%] [G loss: 3.024318]\n",
      "3144 [D loss: 0.124868, acc.: 96.88%] [G loss: 2.943320]\n",
      "3145 [D loss: 0.126129, acc.: 96.88%] [G loss: 2.956032]\n",
      "3146 [D loss: 0.121709, acc.: 96.88%] [G loss: 2.930675]\n",
      "3147 [D loss: 0.123990, acc.: 96.88%] [G loss: 2.913053]\n",
      "3148 [D loss: 0.124743, acc.: 96.88%] [G loss: 2.892057]\n",
      "3149 [D loss: 0.122209, acc.: 96.88%] [G loss: 2.905155]\n",
      "3150 [D loss: 0.121128, acc.: 96.88%] [G loss: 2.879260]\n",
      "3151 [D loss: 0.117210, acc.: 96.88%] [G loss: 2.922323]\n",
      "3152 [D loss: 0.120971, acc.: 96.88%] [G loss: 2.905417]\n",
      "3153 [D loss: 0.121075, acc.: 96.88%] [G loss: 2.938848]\n",
      "3154 [D loss: 0.117801, acc.: 96.88%] [G loss: 2.998126]\n",
      "3155 [D loss: 0.116002, acc.: 96.88%] [G loss: 3.008015]\n",
      "3156 [D loss: 0.118305, acc.: 96.88%] [G loss: 3.067082]\n",
      "3157 [D loss: 0.122798, acc.: 96.88%] [G loss: 2.975482]\n",
      "3158 [D loss: 0.115346, acc.: 96.88%] [G loss: 2.899261]\n",
      "3159 [D loss: 0.127508, acc.: 96.88%] [G loss: 2.940825]\n",
      "3160 [D loss: 0.116378, acc.: 96.88%] [G loss: 2.975767]\n",
      "3161 [D loss: 0.126377, acc.: 96.88%] [G loss: 2.921663]\n",
      "3162 [D loss: 0.123659, acc.: 96.88%] [G loss: 2.936993]\n",
      "3163 [D loss: 0.124269, acc.: 96.88%] [G loss: 2.864946]\n",
      "3164 [D loss: 0.122278, acc.: 96.88%] [G loss: 2.881659]\n",
      "3165 [D loss: 0.117145, acc.: 96.88%] [G loss: 2.936675]\n",
      "3166 [D loss: 0.127174, acc.: 96.88%] [G loss: 2.875006]\n",
      "3167 [D loss: 0.121680, acc.: 96.88%] [G loss: 2.885771]\n",
      "3168 [D loss: 0.122477, acc.: 96.88%] [G loss: 2.850896]\n",
      "3169 [D loss: 0.118159, acc.: 96.88%] [G loss: 2.918959]\n",
      "3170 [D loss: 0.121872, acc.: 96.88%] [G loss: 2.899590]\n",
      "3171 [D loss: 0.118131, acc.: 96.88%] [G loss: 2.917438]\n",
      "3172 [D loss: 0.122656, acc.: 96.88%] [G loss: 2.887813]\n",
      "3173 [D loss: 0.119960, acc.: 96.88%] [G loss: 2.938506]\n",
      "3174 [D loss: 0.123602, acc.: 96.88%] [G loss: 2.950326]\n",
      "3175 [D loss: 0.126779, acc.: 96.88%] [G loss: 2.952317]\n",
      "3176 [D loss: 0.121444, acc.: 96.88%] [G loss: 2.828558]\n",
      "3177 [D loss: 0.121711, acc.: 96.88%] [G loss: 2.927455]\n",
      "3178 [D loss: 0.120673, acc.: 96.88%] [G loss: 2.886741]\n",
      "3179 [D loss: 0.120448, acc.: 96.88%] [G loss: 2.970459]\n",
      "3180 [D loss: 0.124813, acc.: 96.88%] [G loss: 2.907648]\n",
      "3181 [D loss: 0.119013, acc.: 96.88%] [G loss: 2.910795]\n",
      "3182 [D loss: 0.123444, acc.: 96.88%] [G loss: 2.906475]\n",
      "3183 [D loss: 0.114555, acc.: 96.88%] [G loss: 2.918199]\n",
      "3184 [D loss: 0.124454, acc.: 96.88%] [G loss: 2.977955]\n",
      "3185 [D loss: 0.119306, acc.: 96.88%] [G loss: 3.073834]\n",
      "3186 [D loss: 0.124503, acc.: 96.88%] [G loss: 3.019019]\n",
      "3187 [D loss: 0.122343, acc.: 96.88%] [G loss: 2.884900]\n",
      "3188 [D loss: 0.118146, acc.: 96.88%] [G loss: 2.901423]\n",
      "3189 [D loss: 0.119452, acc.: 96.88%] [G loss: 2.977417]\n",
      "3190 [D loss: 0.120893, acc.: 96.88%] [G loss: 2.923590]\n",
      "3191 [D loss: 0.121600, acc.: 96.88%] [G loss: 2.945347]\n",
      "3192 [D loss: 0.113882, acc.: 96.88%] [G loss: 2.972992]\n",
      "3193 [D loss: 0.122949, acc.: 96.88%] [G loss: 2.895627]\n",
      "3194 [D loss: 0.125687, acc.: 96.88%] [G loss: 2.953421]\n",
      "3195 [D loss: 0.123741, acc.: 96.88%] [G loss: 2.908176]\n",
      "3196 [D loss: 0.119233, acc.: 96.88%] [G loss: 2.943907]\n",
      "3197 [D loss: 0.121107, acc.: 96.88%] [G loss: 2.969491]\n",
      "3198 [D loss: 0.117136, acc.: 96.88%] [G loss: 2.863681]\n",
      "3199 [D loss: 0.125120, acc.: 96.88%] [G loss: 2.816871]\n",
      "3200 [D loss: 0.122219, acc.: 96.88%] [G loss: 2.941211]\n",
      "generated_data\n",
      "3201 [D loss: 0.123618, acc.: 96.88%] [G loss: 2.914897]\n",
      "3202 [D loss: 0.126952, acc.: 96.88%] [G loss: 2.931503]\n",
      "3203 [D loss: 0.123105, acc.: 96.88%] [G loss: 2.880891]\n",
      "3204 [D loss: 0.124860, acc.: 96.88%] [G loss: 2.929099]\n",
      "3205 [D loss: 0.117689, acc.: 96.88%] [G loss: 2.926798]\n",
      "3206 [D loss: 0.121813, acc.: 96.88%] [G loss: 2.905746]\n",
      "3207 [D loss: 0.123930, acc.: 96.88%] [G loss: 2.918485]\n",
      "3208 [D loss: 0.123612, acc.: 96.88%] [G loss: 2.936962]\n",
      "3209 [D loss: 0.119525, acc.: 96.88%] [G loss: 3.018254]\n",
      "3210 [D loss: 0.118251, acc.: 96.88%] [G loss: 2.993857]\n",
      "3211 [D loss: 0.122732, acc.: 96.88%] [G loss: 2.937947]\n",
      "3212 [D loss: 0.122511, acc.: 96.88%] [G loss: 2.948166]\n",
      "3213 [D loss: 0.118488, acc.: 96.88%] [G loss: 2.966524]\n",
      "3214 [D loss: 0.119695, acc.: 96.88%] [G loss: 2.962228]\n",
      "3215 [D loss: 0.128940, acc.: 96.88%] [G loss: 2.963392]\n",
      "3216 [D loss: 0.116739, acc.: 96.88%] [G loss: 2.899236]\n",
      "3217 [D loss: 0.115013, acc.: 96.88%] [G loss: 2.991803]\n",
      "3218 [D loss: 0.120907, acc.: 96.88%] [G loss: 2.989031]\n",
      "3219 [D loss: 0.115159, acc.: 96.88%] [G loss: 3.030596]\n",
      "3220 [D loss: 0.119653, acc.: 96.88%] [G loss: 2.921498]\n",
      "3221 [D loss: 0.120626, acc.: 96.88%] [G loss: 2.871016]\n",
      "3222 [D loss: 0.122389, acc.: 96.88%] [G loss: 3.033310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3223 [D loss: 0.118103, acc.: 96.88%] [G loss: 3.016048]\n",
      "3224 [D loss: 0.118969, acc.: 96.88%] [G loss: 2.958355]\n",
      "3225 [D loss: 0.118953, acc.: 96.88%] [G loss: 3.035927]\n",
      "3226 [D loss: 0.122317, acc.: 96.88%] [G loss: 3.125391]\n",
      "3227 [D loss: 0.128016, acc.: 96.88%] [G loss: 2.868076]\n",
      "3228 [D loss: 0.127097, acc.: 96.88%] [G loss: 2.854524]\n",
      "3229 [D loss: 0.119820, acc.: 96.88%] [G loss: 3.007154]\n",
      "3230 [D loss: 0.125488, acc.: 96.88%] [G loss: 2.944086]\n",
      "3231 [D loss: 0.120669, acc.: 96.88%] [G loss: 2.902068]\n",
      "3232 [D loss: 0.123696, acc.: 96.88%] [G loss: 2.924619]\n",
      "3233 [D loss: 0.118669, acc.: 96.88%] [G loss: 2.911801]\n",
      "3234 [D loss: 0.121630, acc.: 96.88%] [G loss: 2.923279]\n",
      "3235 [D loss: 0.118802, acc.: 96.88%] [G loss: 2.974108]\n",
      "3236 [D loss: 0.119375, acc.: 96.88%] [G loss: 2.970623]\n",
      "3237 [D loss: 0.126163, acc.: 96.88%] [G loss: 2.879676]\n",
      "3238 [D loss: 0.125331, acc.: 96.88%] [G loss: 2.872037]\n",
      "3239 [D loss: 0.119764, acc.: 96.88%] [G loss: 2.856094]\n",
      "3240 [D loss: 0.123576, acc.: 96.88%] [G loss: 2.884804]\n",
      "3241 [D loss: 0.116488, acc.: 96.88%] [G loss: 2.913230]\n",
      "3242 [D loss: 0.122010, acc.: 96.88%] [G loss: 2.947240]\n",
      "3243 [D loss: 0.122870, acc.: 96.88%] [G loss: 3.029859]\n",
      "3244 [D loss: 0.122886, acc.: 96.88%] [G loss: 2.957752]\n",
      "3245 [D loss: 0.117887, acc.: 96.88%] [G loss: 3.027353]\n",
      "3246 [D loss: 0.127574, acc.: 96.88%] [G loss: 2.961293]\n",
      "3247 [D loss: 0.119932, acc.: 96.88%] [G loss: 2.969120]\n",
      "3248 [D loss: 0.124914, acc.: 96.88%] [G loss: 2.953238]\n",
      "3249 [D loss: 0.115527, acc.: 96.88%] [G loss: 2.929187]\n",
      "3250 [D loss: 0.118849, acc.: 96.88%] [G loss: 2.930387]\n",
      "3251 [D loss: 0.116524, acc.: 96.88%] [G loss: 3.136512]\n",
      "3252 [D loss: 0.124596, acc.: 96.88%] [G loss: 2.882546]\n",
      "3253 [D loss: 0.118723, acc.: 96.88%] [G loss: 2.987159]\n",
      "3254 [D loss: 0.124875, acc.: 96.88%] [G loss: 2.901148]\n",
      "3255 [D loss: 0.124793, acc.: 96.88%] [G loss: 2.852088]\n",
      "3256 [D loss: 0.126225, acc.: 96.88%] [G loss: 2.895012]\n",
      "3257 [D loss: 0.118451, acc.: 96.88%] [G loss: 2.916939]\n",
      "3258 [D loss: 0.122223, acc.: 96.88%] [G loss: 2.891795]\n",
      "3259 [D loss: 0.125166, acc.: 96.88%] [G loss: 2.869874]\n",
      "3260 [D loss: 0.123098, acc.: 96.88%] [G loss: 2.915544]\n",
      "3261 [D loss: 0.122974, acc.: 96.88%] [G loss: 2.939242]\n",
      "3262 [D loss: 0.120691, acc.: 96.88%] [G loss: 2.837325]\n",
      "3263 [D loss: 0.129346, acc.: 96.88%] [G loss: 2.870387]\n",
      "3264 [D loss: 0.114224, acc.: 96.88%] [G loss: 2.953557]\n",
      "3265 [D loss: 0.117970, acc.: 96.88%] [G loss: 2.880888]\n",
      "3266 [D loss: 0.122505, acc.: 96.88%] [G loss: 3.042285]\n",
      "3267 [D loss: 0.126046, acc.: 96.88%] [G loss: 2.862682]\n",
      "3268 [D loss: 0.124364, acc.: 96.88%] [G loss: 2.952470]\n",
      "3269 [D loss: 0.123431, acc.: 96.88%] [G loss: 2.969552]\n",
      "3270 [D loss: 0.126118, acc.: 96.88%] [G loss: 2.965514]\n",
      "3271 [D loss: 0.121010, acc.: 96.88%] [G loss: 2.911415]\n",
      "3272 [D loss: 0.120718, acc.: 96.88%] [G loss: 2.851180]\n",
      "3273 [D loss: 0.121213, acc.: 96.88%] [G loss: 2.926510]\n",
      "3274 [D loss: 0.120268, acc.: 96.88%] [G loss: 2.893247]\n",
      "3275 [D loss: 0.118967, acc.: 96.88%] [G loss: 2.896477]\n",
      "3276 [D loss: 0.121186, acc.: 96.88%] [G loss: 2.941097]\n",
      "3277 [D loss: 0.119598, acc.: 96.88%] [G loss: 2.923282]\n",
      "3278 [D loss: 0.124929, acc.: 96.88%] [G loss: 2.984819]\n",
      "3279 [D loss: 0.118836, acc.: 96.88%] [G loss: 2.949110]\n",
      "3280 [D loss: 0.119966, acc.: 96.88%] [G loss: 2.942372]\n",
      "3281 [D loss: 0.118023, acc.: 96.88%] [G loss: 2.961300]\n",
      "3282 [D loss: 0.123944, acc.: 96.88%] [G loss: 2.886014]\n",
      "3283 [D loss: 0.114348, acc.: 96.88%] [G loss: 3.000946]\n",
      "3284 [D loss: 0.117830, acc.: 96.88%] [G loss: 2.968024]\n",
      "3285 [D loss: 0.124479, acc.: 96.88%] [G loss: 2.891721]\n",
      "3286 [D loss: 0.119126, acc.: 96.88%] [G loss: 2.947899]\n",
      "3287 [D loss: 0.115789, acc.: 96.88%] [G loss: 3.019420]\n",
      "3288 [D loss: 0.123323, acc.: 96.88%] [G loss: 2.926881]\n",
      "3289 [D loss: 0.119210, acc.: 96.88%] [G loss: 2.966387]\n",
      "3290 [D loss: 0.124787, acc.: 96.88%] [G loss: 2.895760]\n",
      "3291 [D loss: 0.128087, acc.: 96.88%] [G loss: 2.874905]\n",
      "3292 [D loss: 0.126184, acc.: 96.88%] [G loss: 2.875223]\n",
      "3293 [D loss: 0.113845, acc.: 96.88%] [G loss: 2.919243]\n",
      "3294 [D loss: 0.118636, acc.: 96.88%] [G loss: 2.977344]\n",
      "3295 [D loss: 0.124003, acc.: 96.88%] [G loss: 2.865463]\n",
      "3296 [D loss: 0.114936, acc.: 96.88%] [G loss: 2.909326]\n",
      "3297 [D loss: 0.127910, acc.: 96.88%] [G loss: 2.884063]\n",
      "3298 [D loss: 0.125251, acc.: 96.88%] [G loss: 2.857013]\n",
      "3299 [D loss: 0.122908, acc.: 96.88%] [G loss: 2.927379]\n",
      "3300 [D loss: 0.121166, acc.: 96.88%] [G loss: 2.930053]\n",
      "generated_data\n",
      "3301 [D loss: 0.124313, acc.: 96.88%] [G loss: 2.918496]\n",
      "3302 [D loss: 0.115041, acc.: 96.88%] [G loss: 2.893311]\n",
      "3303 [D loss: 0.119163, acc.: 96.88%] [G loss: 2.892932]\n",
      "3304 [D loss: 0.125335, acc.: 96.88%] [G loss: 2.923190]\n",
      "3305 [D loss: 0.118311, acc.: 96.88%] [G loss: 3.023020]\n",
      "3306 [D loss: 0.112049, acc.: 96.88%] [G loss: 2.888744]\n",
      "3307 [D loss: 0.125283, acc.: 96.88%] [G loss: 2.865970]\n",
      "3308 [D loss: 0.124507, acc.: 96.88%] [G loss: 2.863681]\n",
      "3309 [D loss: 0.119555, acc.: 96.88%] [G loss: 2.959306]\n",
      "3310 [D loss: 0.122172, acc.: 96.88%] [G loss: 3.021544]\n",
      "3311 [D loss: 0.125862, acc.: 96.88%] [G loss: 2.952591]\n",
      "3312 [D loss: 0.117316, acc.: 96.88%] [G loss: 2.921920]\n",
      "3313 [D loss: 0.123329, acc.: 96.88%] [G loss: 3.050468]\n",
      "3314 [D loss: 0.126611, acc.: 96.88%] [G loss: 2.912630]\n",
      "3315 [D loss: 0.120867, acc.: 96.88%] [G loss: 2.889328]\n",
      "3316 [D loss: 0.117054, acc.: 96.88%] [G loss: 2.849384]\n",
      "3317 [D loss: 0.116256, acc.: 96.88%] [G loss: 2.987083]\n",
      "3318 [D loss: 0.125430, acc.: 96.88%] [G loss: 2.886122]\n",
      "3319 [D loss: 0.115241, acc.: 96.88%] [G loss: 2.876430]\n",
      "3320 [D loss: 0.124023, acc.: 96.88%] [G loss: 2.839824]\n",
      "3321 [D loss: 0.120159, acc.: 96.88%] [G loss: 2.903849]\n",
      "3322 [D loss: 0.123907, acc.: 96.88%] [G loss: 2.953333]\n",
      "3323 [D loss: 0.121524, acc.: 96.88%] [G loss: 2.939725]\n",
      "3324 [D loss: 0.127322, acc.: 96.88%] [G loss: 2.901158]\n",
      "3325 [D loss: 0.119633, acc.: 96.88%] [G loss: 2.894174]\n",
      "3326 [D loss: 0.127206, acc.: 96.88%] [G loss: 2.986115]\n",
      "3327 [D loss: 0.123297, acc.: 96.88%] [G loss: 2.970574]\n",
      "3328 [D loss: 0.118430, acc.: 96.88%] [G loss: 2.971684]\n",
      "3329 [D loss: 0.122965, acc.: 96.88%] [G loss: 2.925607]\n",
      "3330 [D loss: 0.117033, acc.: 96.88%] [G loss: 2.944513]\n",
      "3331 [D loss: 0.123738, acc.: 96.88%] [G loss: 2.941106]\n",
      "3332 [D loss: 0.119240, acc.: 96.88%] [G loss: 3.015702]\n",
      "3333 [D loss: 0.122031, acc.: 96.88%] [G loss: 2.897871]\n",
      "3334 [D loss: 0.122262, acc.: 96.88%] [G loss: 2.912855]\n",
      "3335 [D loss: 0.120938, acc.: 96.88%] [G loss: 2.912580]\n",
      "3336 [D loss: 0.121611, acc.: 96.88%] [G loss: 2.893415]\n",
      "3337 [D loss: 0.115971, acc.: 96.88%] [G loss: 2.911572]\n",
      "3338 [D loss: 0.122285, acc.: 96.88%] [G loss: 3.082721]\n",
      "3339 [D loss: 0.123329, acc.: 96.88%] [G loss: 2.946096]\n",
      "3340 [D loss: 0.115380, acc.: 96.88%] [G loss: 2.994275]\n",
      "3341 [D loss: 0.121033, acc.: 96.88%] [G loss: 2.971182]\n",
      "3342 [D loss: 0.124631, acc.: 96.88%] [G loss: 2.935706]\n",
      "3343 [D loss: 0.116855, acc.: 96.88%] [G loss: 2.948479]\n",
      "3344 [D loss: 0.121321, acc.: 96.88%] [G loss: 2.925958]\n",
      "3345 [D loss: 0.125204, acc.: 96.88%] [G loss: 2.960796]\n",
      "3346 [D loss: 0.119147, acc.: 96.88%] [G loss: 2.991777]\n",
      "3347 [D loss: 0.126936, acc.: 96.88%] [G loss: 2.893656]\n",
      "3348 [D loss: 0.120272, acc.: 96.88%] [G loss: 2.868196]\n",
      "3349 [D loss: 0.124879, acc.: 96.88%] [G loss: 3.020432]\n",
      "3350 [D loss: 0.126035, acc.: 96.88%] [G loss: 2.990378]\n",
      "3351 [D loss: 0.129133, acc.: 96.88%] [G loss: 2.921058]\n",
      "3352 [D loss: 0.121178, acc.: 96.88%] [G loss: 2.901462]\n",
      "3353 [D loss: 0.117342, acc.: 96.88%] [G loss: 2.922188]\n",
      "3354 [D loss: 0.119266, acc.: 96.88%] [G loss: 2.898736]\n",
      "3355 [D loss: 0.121180, acc.: 96.88%] [G loss: 2.969284]\n",
      "3356 [D loss: 0.121976, acc.: 96.88%] [G loss: 2.884197]\n",
      "3357 [D loss: 0.120088, acc.: 96.88%] [G loss: 2.965417]\n",
      "3358 [D loss: 0.130491, acc.: 96.88%] [G loss: 2.932377]\n",
      "3359 [D loss: 0.119597, acc.: 96.88%] [G loss: 2.893164]\n",
      "3360 [D loss: 0.125759, acc.: 96.88%] [G loss: 2.894055]\n",
      "3361 [D loss: 0.119279, acc.: 96.88%] [G loss: 2.950608]\n",
      "3362 [D loss: 0.121941, acc.: 96.88%] [G loss: 2.971780]\n",
      "3363 [D loss: 0.119474, acc.: 96.88%] [G loss: 2.970239]\n",
      "3364 [D loss: 0.117886, acc.: 96.88%] [G loss: 2.987038]\n",
      "3365 [D loss: 0.121566, acc.: 96.88%] [G loss: 2.913133]\n",
      "3366 [D loss: 0.120905, acc.: 96.88%] [G loss: 2.872779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3367 [D loss: 0.125552, acc.: 96.88%] [G loss: 3.003829]\n",
      "3368 [D loss: 0.122111, acc.: 96.88%] [G loss: 2.867743]\n",
      "3369 [D loss: 0.120283, acc.: 96.88%] [G loss: 2.936752]\n",
      "3370 [D loss: 0.124417, acc.: 96.88%] [G loss: 2.974148]\n",
      "3371 [D loss: 0.116876, acc.: 96.88%] [G loss: 2.927640]\n",
      "3372 [D loss: 0.120588, acc.: 96.88%] [G loss: 2.910453]\n",
      "3373 [D loss: 0.120225, acc.: 96.88%] [G loss: 2.885635]\n",
      "3374 [D loss: 0.116244, acc.: 96.88%] [G loss: 2.937656]\n",
      "3375 [D loss: 0.120501, acc.: 96.88%] [G loss: 2.858644]\n",
      "3376 [D loss: 0.118687, acc.: 96.88%] [G loss: 3.014655]\n",
      "3377 [D loss: 0.122440, acc.: 96.88%] [G loss: 2.970950]\n",
      "3378 [D loss: 0.118264, acc.: 96.88%] [G loss: 2.951058]\n",
      "3379 [D loss: 0.124229, acc.: 96.88%] [G loss: 2.952898]\n",
      "3380 [D loss: 0.115608, acc.: 96.88%] [G loss: 3.024961]\n",
      "3381 [D loss: 0.117237, acc.: 96.88%] [G loss: 3.093981]\n",
      "3382 [D loss: 0.120808, acc.: 96.88%] [G loss: 2.981016]\n",
      "3383 [D loss: 0.116673, acc.: 96.88%] [G loss: 3.008531]\n",
      "3384 [D loss: 0.127238, acc.: 96.88%] [G loss: 2.922852]\n",
      "3385 [D loss: 0.115864, acc.: 96.88%] [G loss: 2.999239]\n",
      "3386 [D loss: 0.120620, acc.: 96.88%] [G loss: 2.925322]\n",
      "3387 [D loss: 0.115631, acc.: 96.88%] [G loss: 2.944464]\n",
      "3388 [D loss: 0.115084, acc.: 96.88%] [G loss: 2.912977]\n",
      "3389 [D loss: 0.124331, acc.: 96.88%] [G loss: 2.901399]\n",
      "3390 [D loss: 0.120982, acc.: 96.88%] [G loss: 2.878379]\n",
      "3391 [D loss: 0.119126, acc.: 96.88%] [G loss: 2.899745]\n",
      "3392 [D loss: 0.117705, acc.: 96.88%] [G loss: 2.863759]\n",
      "3393 [D loss: 0.122756, acc.: 96.88%] [G loss: 2.924589]\n",
      "3394 [D loss: 0.123097, acc.: 96.88%] [G loss: 2.969088]\n",
      "3395 [D loss: 0.111233, acc.: 96.88%] [G loss: 2.988112]\n",
      "3396 [D loss: 0.119468, acc.: 96.88%] [G loss: 3.004972]\n",
      "3397 [D loss: 0.124364, acc.: 96.88%] [G loss: 2.950601]\n",
      "3398 [D loss: 0.126506, acc.: 96.88%] [G loss: 2.964573]\n",
      "3399 [D loss: 0.120188, acc.: 96.88%] [G loss: 2.951880]\n",
      "3400 [D loss: 0.124474, acc.: 96.88%] [G loss: 2.923605]\n",
      "generated_data\n",
      "3401 [D loss: 0.126755, acc.: 96.88%] [G loss: 2.932636]\n",
      "3402 [D loss: 0.122842, acc.: 96.88%] [G loss: 2.890282]\n",
      "3403 [D loss: 0.119245, acc.: 96.88%] [G loss: 2.916019]\n",
      "3404 [D loss: 0.123982, acc.: 96.88%] [G loss: 2.968937]\n",
      "3405 [D loss: 0.122501, acc.: 96.88%] [G loss: 2.970319]\n",
      "3406 [D loss: 0.123063, acc.: 96.88%] [G loss: 2.992478]\n",
      "3407 [D loss: 0.121252, acc.: 96.88%] [G loss: 2.853506]\n",
      "3408 [D loss: 0.122793, acc.: 96.88%] [G loss: 2.856175]\n",
      "3409 [D loss: 0.122442, acc.: 96.88%] [G loss: 2.928018]\n",
      "3410 [D loss: 0.124614, acc.: 96.88%] [G loss: 2.890334]\n",
      "3411 [D loss: 0.124799, acc.: 96.88%] [G loss: 2.879137]\n",
      "3412 [D loss: 0.122301, acc.: 96.88%] [G loss: 2.873401]\n",
      "3413 [D loss: 0.117920, acc.: 96.88%] [G loss: 2.883729]\n",
      "3414 [D loss: 0.120677, acc.: 96.88%] [G loss: 2.937494]\n",
      "3415 [D loss: 0.119658, acc.: 96.88%] [G loss: 2.979581]\n",
      "3416 [D loss: 0.127048, acc.: 96.88%] [G loss: 3.028761]\n",
      "3417 [D loss: 0.123252, acc.: 96.88%] [G loss: 2.869398]\n",
      "3418 [D loss: 0.123250, acc.: 96.88%] [G loss: 2.932183]\n",
      "3419 [D loss: 0.120220, acc.: 96.88%] [G loss: 2.887112]\n",
      "3420 [D loss: 0.124648, acc.: 96.88%] [G loss: 2.914912]\n",
      "3421 [D loss: 0.116422, acc.: 96.88%] [G loss: 2.927901]\n",
      "3422 [D loss: 0.123670, acc.: 96.88%] [G loss: 3.001289]\n",
      "3423 [D loss: 0.122378, acc.: 96.88%] [G loss: 2.958028]\n",
      "3424 [D loss: 0.126558, acc.: 96.88%] [G loss: 2.883636]\n",
      "3425 [D loss: 0.117963, acc.: 96.88%] [G loss: 2.965641]\n",
      "3426 [D loss: 0.116989, acc.: 96.88%] [G loss: 2.982629]\n",
      "3427 [D loss: 0.112987, acc.: 96.88%] [G loss: 3.078172]\n",
      "3428 [D loss: 0.118508, acc.: 96.88%] [G loss: 2.944927]\n",
      "3429 [D loss: 0.124132, acc.: 96.88%] [G loss: 2.886404]\n",
      "3430 [D loss: 0.114213, acc.: 96.88%] [G loss: 3.032656]\n",
      "3431 [D loss: 0.121319, acc.: 96.88%] [G loss: 2.986389]\n",
      "3432 [D loss: 0.122789, acc.: 96.88%] [G loss: 3.018896]\n",
      "3433 [D loss: 0.122441, acc.: 96.88%] [G loss: 2.964057]\n",
      "3434 [D loss: 0.119263, acc.: 96.88%] [G loss: 2.977741]\n",
      "3435 [D loss: 0.127365, acc.: 96.88%] [G loss: 2.855589]\n",
      "3436 [D loss: 0.116549, acc.: 96.88%] [G loss: 2.914476]\n",
      "3437 [D loss: 0.123156, acc.: 96.88%] [G loss: 2.949355]\n",
      "3438 [D loss: 0.116840, acc.: 96.88%] [G loss: 2.949376]\n",
      "3439 [D loss: 0.126130, acc.: 96.88%] [G loss: 2.879797]\n",
      "3440 [D loss: 0.125909, acc.: 96.88%] [G loss: 2.902636]\n",
      "3441 [D loss: 0.123429, acc.: 96.88%] [G loss: 3.001003]\n",
      "3442 [D loss: 0.119118, acc.: 96.88%] [G loss: 2.887810]\n",
      "3443 [D loss: 0.123475, acc.: 96.88%] [G loss: 2.902532]\n",
      "3444 [D loss: 0.124082, acc.: 96.88%] [G loss: 2.944826]\n",
      "3445 [D loss: 0.113593, acc.: 96.88%] [G loss: 2.955145]\n",
      "3446 [D loss: 0.117100, acc.: 96.88%] [G loss: 2.941375]\n",
      "3447 [D loss: 0.122809, acc.: 96.88%] [G loss: 2.863457]\n",
      "3448 [D loss: 0.122894, acc.: 96.88%] [G loss: 2.964898]\n",
      "3449 [D loss: 0.116822, acc.: 96.88%] [G loss: 2.893738]\n",
      "3450 [D loss: 0.119758, acc.: 96.88%] [G loss: 2.964884]\n",
      "3451 [D loss: 0.117884, acc.: 96.88%] [G loss: 2.933300]\n",
      "3452 [D loss: 0.121701, acc.: 96.88%] [G loss: 3.007397]\n",
      "3453 [D loss: 0.127185, acc.: 96.88%] [G loss: 2.922363]\n",
      "3454 [D loss: 0.124145, acc.: 96.88%] [G loss: 2.874430]\n",
      "3455 [D loss: 0.117974, acc.: 96.88%] [G loss: 2.892441]\n",
      "3456 [D loss: 0.118754, acc.: 96.88%] [G loss: 3.034783]\n",
      "3457 [D loss: 0.125533, acc.: 96.88%] [G loss: 2.872642]\n",
      "3458 [D loss: 0.123490, acc.: 96.88%] [G loss: 2.852112]\n",
      "3459 [D loss: 0.121449, acc.: 96.88%] [G loss: 2.888521]\n",
      "3460 [D loss: 0.128654, acc.: 96.88%] [G loss: 2.866968]\n",
      "3461 [D loss: 0.124581, acc.: 96.88%] [G loss: 2.890322]\n",
      "3462 [D loss: 0.116508, acc.: 96.88%] [G loss: 2.864658]\n",
      "3463 [D loss: 0.117650, acc.: 96.88%] [G loss: 2.871130]\n",
      "3464 [D loss: 0.120632, acc.: 96.88%] [G loss: 2.867777]\n",
      "3465 [D loss: 0.123908, acc.: 96.88%] [G loss: 2.871919]\n",
      "3466 [D loss: 0.125214, acc.: 96.88%] [G loss: 2.913210]\n",
      "3467 [D loss: 0.120626, acc.: 96.88%] [G loss: 2.916101]\n",
      "3468 [D loss: 0.126949, acc.: 96.88%] [G loss: 2.945591]\n",
      "3469 [D loss: 0.113048, acc.: 96.88%] [G loss: 2.963233]\n",
      "3470 [D loss: 0.121111, acc.: 96.88%] [G loss: 2.943548]\n",
      "3471 [D loss: 0.116399, acc.: 96.88%] [G loss: 2.973558]\n",
      "3472 [D loss: 0.121655, acc.: 96.88%] [G loss: 2.977303]\n",
      "3473 [D loss: 0.117579, acc.: 96.88%] [G loss: 2.911817]\n",
      "3474 [D loss: 0.124374, acc.: 96.88%] [G loss: 2.987336]\n",
      "3475 [D loss: 0.113460, acc.: 96.88%] [G loss: 2.934311]\n",
      "3476 [D loss: 0.119716, acc.: 96.88%] [G loss: 2.903409]\n",
      "3477 [D loss: 0.126618, acc.: 96.88%] [G loss: 2.950674]\n",
      "3478 [D loss: 0.118966, acc.: 96.88%] [G loss: 2.909389]\n",
      "3479 [D loss: 0.122271, acc.: 96.88%] [G loss: 3.033783]\n",
      "3480 [D loss: 0.120483, acc.: 96.88%] [G loss: 2.932280]\n",
      "3481 [D loss: 0.122833, acc.: 96.88%] [G loss: 2.983297]\n",
      "3482 [D loss: 0.120220, acc.: 96.88%] [G loss: 2.889182]\n",
      "3483 [D loss: 0.119784, acc.: 96.88%] [G loss: 2.864116]\n",
      "3484 [D loss: 0.123442, acc.: 96.88%] [G loss: 2.879823]\n",
      "3485 [D loss: 0.126589, acc.: 96.88%] [G loss: 2.878802]\n",
      "3486 [D loss: 0.118678, acc.: 96.88%] [G loss: 3.064997]\n",
      "3487 [D loss: 0.122101, acc.: 96.88%] [G loss: 3.045931]\n",
      "3488 [D loss: 0.114495, acc.: 96.88%] [G loss: 2.914568]\n",
      "3489 [D loss: 0.120289, acc.: 96.88%] [G loss: 3.001231]\n",
      "3490 [D loss: 0.122500, acc.: 96.88%] [G loss: 2.912597]\n",
      "3491 [D loss: 0.118648, acc.: 96.88%] [G loss: 2.941881]\n",
      "3492 [D loss: 0.128362, acc.: 96.88%] [G loss: 3.024645]\n",
      "3493 [D loss: 0.115862, acc.: 96.88%] [G loss: 2.949663]\n",
      "3494 [D loss: 0.119989, acc.: 96.88%] [G loss: 2.977969]\n",
      "3495 [D loss: 0.117620, acc.: 96.88%] [G loss: 2.953743]\n",
      "3496 [D loss: 0.123159, acc.: 96.88%] [G loss: 2.982255]\n",
      "3497 [D loss: 0.116101, acc.: 96.88%] [G loss: 2.891575]\n",
      "3498 [D loss: 0.121301, acc.: 96.88%] [G loss: 2.954391]\n",
      "3499 [D loss: 0.119201, acc.: 96.88%] [G loss: 2.956650]\n",
      "3500 [D loss: 0.119733, acc.: 96.88%] [G loss: 3.013298]\n",
      "generated_data\n",
      "3501 [D loss: 0.113908, acc.: 96.88%] [G loss: 2.950900]\n",
      "3502 [D loss: 0.123130, acc.: 96.88%] [G loss: 3.011444]\n",
      "3503 [D loss: 0.124236, acc.: 96.88%] [G loss: 2.950629]\n",
      "3504 [D loss: 0.117444, acc.: 96.88%] [G loss: 2.811679]\n",
      "3505 [D loss: 0.126363, acc.: 96.88%] [G loss: 2.976630]\n",
      "3506 [D loss: 0.118154, acc.: 96.88%] [G loss: 2.931251]\n",
      "3507 [D loss: 0.126005, acc.: 96.88%] [G loss: 3.061244]\n",
      "3508 [D loss: 0.118317, acc.: 96.88%] [G loss: 2.994714]\n",
      "3509 [D loss: 0.120825, acc.: 96.88%] [G loss: 2.930023]\n",
      "3510 [D loss: 0.117567, acc.: 96.88%] [G loss: 2.908200]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3511 [D loss: 0.114318, acc.: 96.88%] [G loss: 2.879026]\n",
      "3512 [D loss: 0.123257, acc.: 96.88%] [G loss: 2.978739]\n",
      "3513 [D loss: 0.124903, acc.: 96.88%] [G loss: 2.986068]\n",
      "3514 [D loss: 0.121775, acc.: 96.88%] [G loss: 3.001177]\n",
      "3515 [D loss: 0.120294, acc.: 96.88%] [G loss: 2.891511]\n",
      "3516 [D loss: 0.119490, acc.: 96.88%] [G loss: 2.956265]\n",
      "3517 [D loss: 0.128553, acc.: 96.88%] [G loss: 2.924282]\n",
      "3518 [D loss: 0.118828, acc.: 96.88%] [G loss: 2.854048]\n",
      "3519 [D loss: 0.119738, acc.: 96.88%] [G loss: 2.938922]\n",
      "3520 [D loss: 0.117155, acc.: 96.88%] [G loss: 2.976790]\n",
      "3521 [D loss: 0.115563, acc.: 96.88%] [G loss: 2.874993]\n",
      "3522 [D loss: 0.119499, acc.: 96.88%] [G loss: 2.865110]\n",
      "3523 [D loss: 0.122345, acc.: 96.88%] [G loss: 2.978766]\n",
      "3524 [D loss: 0.125807, acc.: 96.88%] [G loss: 2.929393]\n",
      "3525 [D loss: 0.127846, acc.: 96.88%] [G loss: 2.951152]\n",
      "3526 [D loss: 0.124299, acc.: 96.88%] [G loss: 2.843863]\n",
      "3527 [D loss: 0.117558, acc.: 96.88%] [G loss: 2.929172]\n",
      "3528 [D loss: 0.125553, acc.: 96.88%] [G loss: 2.913960]\n",
      "3529 [D loss: 0.118762, acc.: 96.88%] [G loss: 2.981402]\n",
      "3530 [D loss: 0.120075, acc.: 96.88%] [G loss: 2.884089]\n",
      "3531 [D loss: 0.124128, acc.: 96.88%] [G loss: 2.876753]\n",
      "3532 [D loss: 0.122682, acc.: 96.88%] [G loss: 2.840159]\n",
      "3533 [D loss: 0.116742, acc.: 96.88%] [G loss: 2.927738]\n",
      "3534 [D loss: 0.115680, acc.: 96.88%] [G loss: 3.045639]\n",
      "3535 [D loss: 0.125489, acc.: 96.88%] [G loss: 2.941866]\n",
      "3536 [D loss: 0.128032, acc.: 96.88%] [G loss: 2.878335]\n",
      "3537 [D loss: 0.120220, acc.: 96.88%] [G loss: 2.897408]\n",
      "3538 [D loss: 0.126252, acc.: 96.88%] [G loss: 2.838411]\n",
      "3539 [D loss: 0.121558, acc.: 96.88%] [G loss: 2.796430]\n",
      "3540 [D loss: 0.120609, acc.: 96.88%] [G loss: 2.824188]\n",
      "3541 [D loss: 0.121584, acc.: 96.88%] [G loss: 2.985122]\n",
      "3542 [D loss: 0.121920, acc.: 96.88%] [G loss: 3.018162]\n",
      "3543 [D loss: 0.117934, acc.: 96.88%] [G loss: 2.952806]\n",
      "3544 [D loss: 0.118816, acc.: 96.88%] [G loss: 2.943921]\n",
      "3545 [D loss: 0.119207, acc.: 96.88%] [G loss: 2.975182]\n",
      "3546 [D loss: 0.119514, acc.: 96.88%] [G loss: 2.910204]\n",
      "3547 [D loss: 0.121389, acc.: 96.88%] [G loss: 2.943056]\n",
      "3548 [D loss: 0.121693, acc.: 96.88%] [G loss: 2.892937]\n",
      "3549 [D loss: 0.120792, acc.: 96.88%] [G loss: 2.917553]\n",
      "3550 [D loss: 0.119750, acc.: 96.88%] [G loss: 3.046327]\n",
      "3551 [D loss: 0.124304, acc.: 96.88%] [G loss: 2.948635]\n",
      "3552 [D loss: 0.127635, acc.: 96.88%] [G loss: 2.928236]\n",
      "3553 [D loss: 0.117178, acc.: 96.88%] [G loss: 2.882745]\n",
      "3554 [D loss: 0.123205, acc.: 96.88%] [G loss: 2.867178]\n",
      "3555 [D loss: 0.117349, acc.: 96.88%] [G loss: 2.956828]\n",
      "3556 [D loss: 0.120044, acc.: 96.88%] [G loss: 2.977073]\n",
      "3557 [D loss: 0.120887, acc.: 96.88%] [G loss: 2.903199]\n",
      "3558 [D loss: 0.127274, acc.: 96.88%] [G loss: 2.916737]\n",
      "3559 [D loss: 0.126142, acc.: 96.88%] [G loss: 2.808135]\n",
      "3560 [D loss: 0.118331, acc.: 96.88%] [G loss: 2.888073]\n",
      "3561 [D loss: 0.120530, acc.: 96.88%] [G loss: 2.852380]\n",
      "3562 [D loss: 0.116993, acc.: 96.88%] [G loss: 2.958006]\n",
      "3563 [D loss: 0.125596, acc.: 96.88%] [G loss: 3.029435]\n",
      "3564 [D loss: 0.123908, acc.: 96.88%] [G loss: 2.906899]\n",
      "3565 [D loss: 0.124562, acc.: 96.88%] [G loss: 2.916702]\n",
      "3566 [D loss: 0.120598, acc.: 96.88%] [G loss: 2.880604]\n",
      "3567 [D loss: 0.123281, acc.: 96.88%] [G loss: 2.984049]\n",
      "3568 [D loss: 0.120093, acc.: 96.88%] [G loss: 2.894375]\n",
      "3569 [D loss: 0.127646, acc.: 96.88%] [G loss: 2.882476]\n",
      "3570 [D loss: 0.123370, acc.: 96.88%] [G loss: 2.906944]\n",
      "3571 [D loss: 0.117793, acc.: 96.88%] [G loss: 2.891229]\n",
      "3572 [D loss: 0.115785, acc.: 96.88%] [G loss: 2.873379]\n",
      "3573 [D loss: 0.120781, acc.: 96.88%] [G loss: 2.857876]\n",
      "3574 [D loss: 0.123272, acc.: 96.88%] [G loss: 2.844677]\n",
      "3575 [D loss: 0.123948, acc.: 96.88%] [G loss: 2.849195]\n",
      "3576 [D loss: 0.127161, acc.: 96.88%] [G loss: 2.673606]\n",
      "3577 [D loss: 0.123042, acc.: 96.88%] [G loss: 2.856492]\n",
      "3578 [D loss: 0.117718, acc.: 96.88%] [G loss: 2.873885]\n",
      "3579 [D loss: 0.121473, acc.: 96.88%] [G loss: 2.923477]\n",
      "3580 [D loss: 0.117770, acc.: 96.88%] [G loss: 2.933933]\n",
      "3581 [D loss: 0.122817, acc.: 96.88%] [G loss: 2.986385]\n",
      "3582 [D loss: 0.126313, acc.: 96.88%] [G loss: 2.849686]\n",
      "3583 [D loss: 0.123574, acc.: 96.88%] [G loss: 2.927363]\n",
      "3584 [D loss: 0.123326, acc.: 96.88%] [G loss: 2.870281]\n",
      "3585 [D loss: 0.124417, acc.: 96.88%] [G loss: 2.814824]\n",
      "3586 [D loss: 0.117614, acc.: 96.88%] [G loss: 2.947945]\n",
      "3587 [D loss: 0.125556, acc.: 96.88%] [G loss: 2.946206]\n",
      "3588 [D loss: 0.125501, acc.: 96.88%] [G loss: 2.950518]\n",
      "3589 [D loss: 0.125470, acc.: 96.88%] [G loss: 2.944844]\n",
      "3590 [D loss: 0.126133, acc.: 96.88%] [G loss: 2.835231]\n",
      "3591 [D loss: 0.127850, acc.: 96.88%] [G loss: 2.918334]\n",
      "3592 [D loss: 0.122922, acc.: 96.88%] [G loss: 2.880195]\n",
      "3593 [D loss: 0.120593, acc.: 96.88%] [G loss: 2.963177]\n",
      "3594 [D loss: 0.117800, acc.: 96.88%] [G loss: 2.941911]\n",
      "3595 [D loss: 0.109300, acc.: 96.88%] [G loss: 2.991511]\n",
      "3596 [D loss: 0.117749, acc.: 96.88%] [G loss: 2.950361]\n",
      "3597 [D loss: 0.128166, acc.: 96.88%] [G loss: 2.850985]\n",
      "3598 [D loss: 0.117954, acc.: 96.88%] [G loss: 2.927859]\n",
      "3599 [D loss: 0.124087, acc.: 96.88%] [G loss: 3.043250]\n",
      "3600 [D loss: 0.121678, acc.: 96.88%] [G loss: 2.972285]\n",
      "generated_data\n",
      "3601 [D loss: 0.125091, acc.: 96.88%] [G loss: 3.049273]\n",
      "3602 [D loss: 0.120919, acc.: 96.88%] [G loss: 2.933632]\n",
      "3603 [D loss: 0.117510, acc.: 96.88%] [G loss: 2.922958]\n",
      "3604 [D loss: 0.116916, acc.: 96.88%] [G loss: 3.047537]\n",
      "3605 [D loss: 0.119903, acc.: 96.88%] [G loss: 2.906673]\n",
      "3606 [D loss: 0.123837, acc.: 96.88%] [G loss: 2.978155]\n",
      "3607 [D loss: 0.117447, acc.: 96.88%] [G loss: 3.001292]\n",
      "3608 [D loss: 0.127026, acc.: 96.88%] [G loss: 3.011320]\n",
      "3609 [D loss: 0.120287, acc.: 96.88%] [G loss: 2.960054]\n",
      "3610 [D loss: 0.123022, acc.: 96.88%] [G loss: 2.871584]\n",
      "3611 [D loss: 0.124207, acc.: 96.88%] [G loss: 2.898366]\n",
      "3612 [D loss: 0.123338, acc.: 96.88%] [G loss: 2.991320]\n",
      "3613 [D loss: 0.119964, acc.: 96.88%] [G loss: 2.981545]\n",
      "3614 [D loss: 0.122103, acc.: 96.88%] [G loss: 2.924262]\n",
      "3615 [D loss: 0.126021, acc.: 96.88%] [G loss: 2.935342]\n",
      "3616 [D loss: 0.122808, acc.: 96.88%] [G loss: 2.972137]\n",
      "3617 [D loss: 0.125698, acc.: 96.88%] [G loss: 2.904490]\n",
      "3618 [D loss: 0.124543, acc.: 96.88%] [G loss: 2.810036]\n",
      "3619 [D loss: 0.118560, acc.: 96.88%] [G loss: 2.842269]\n",
      "3620 [D loss: 0.122514, acc.: 96.88%] [G loss: 2.889319]\n",
      "3621 [D loss: 0.119847, acc.: 96.88%] [G loss: 2.869188]\n",
      "3622 [D loss: 0.121757, acc.: 96.88%] [G loss: 2.919520]\n",
      "3623 [D loss: 0.117786, acc.: 96.88%] [G loss: 2.832815]\n",
      "3624 [D loss: 0.122136, acc.: 96.88%] [G loss: 3.000716]\n",
      "3625 [D loss: 0.125020, acc.: 96.88%] [G loss: 2.879540]\n",
      "3626 [D loss: 0.120487, acc.: 96.88%] [G loss: 2.895148]\n",
      "3627 [D loss: 0.117017, acc.: 96.88%] [G loss: 2.860424]\n",
      "3628 [D loss: 0.124231, acc.: 96.88%] [G loss: 2.897234]\n",
      "3629 [D loss: 0.121666, acc.: 96.88%] [G loss: 2.905185]\n",
      "3630 [D loss: 0.119025, acc.: 96.88%] [G loss: 2.900928]\n",
      "3631 [D loss: 0.123082, acc.: 96.88%] [G loss: 2.904389]\n",
      "3632 [D loss: 0.121625, acc.: 96.88%] [G loss: 2.947827]\n",
      "3633 [D loss: 0.127755, acc.: 96.88%] [G loss: 2.877856]\n",
      "3634 [D loss: 0.124157, acc.: 96.88%] [G loss: 2.932377]\n",
      "3635 [D loss: 0.122623, acc.: 96.88%] [G loss: 2.933356]\n",
      "3636 [D loss: 0.126473, acc.: 96.88%] [G loss: 2.890229]\n",
      "3637 [D loss: 0.115138, acc.: 96.88%] [G loss: 2.909340]\n",
      "3638 [D loss: 0.122682, acc.: 96.88%] [G loss: 2.941071]\n",
      "3639 [D loss: 0.123053, acc.: 96.88%] [G loss: 2.874358]\n",
      "3640 [D loss: 0.123048, acc.: 96.88%] [G loss: 2.908644]\n",
      "3641 [D loss: 0.123710, acc.: 96.88%] [G loss: 2.952157]\n",
      "3642 [D loss: 0.123132, acc.: 96.88%] [G loss: 3.021013]\n",
      "3643 [D loss: 0.115351, acc.: 96.88%] [G loss: 2.972239]\n",
      "3644 [D loss: 0.126024, acc.: 96.88%] [G loss: 2.943606]\n",
      "3645 [D loss: 0.125482, acc.: 96.88%] [G loss: 2.946190]\n",
      "3646 [D loss: 0.123738, acc.: 96.88%] [G loss: 2.931365]\n",
      "3647 [D loss: 0.124806, acc.: 96.88%] [G loss: 2.960418]\n",
      "3648 [D loss: 0.126515, acc.: 96.88%] [G loss: 2.832436]\n",
      "3649 [D loss: 0.115465, acc.: 96.88%] [G loss: 2.913633]\n",
      "3650 [D loss: 0.119507, acc.: 96.88%] [G loss: 2.903192]\n",
      "3651 [D loss: 0.122635, acc.: 96.88%] [G loss: 2.928625]\n",
      "3652 [D loss: 0.118879, acc.: 96.88%] [G loss: 2.969685]\n",
      "3653 [D loss: 0.121714, acc.: 96.88%] [G loss: 2.929374]\n",
      "3654 [D loss: 0.119726, acc.: 96.88%] [G loss: 2.924836]\n",
      "3655 [D loss: 0.122729, acc.: 96.88%] [G loss: 2.952607]\n",
      "3656 [D loss: 0.121018, acc.: 96.88%] [G loss: 2.901504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3657 [D loss: 0.118924, acc.: 96.88%] [G loss: 3.031751]\n",
      "3658 [D loss: 0.124797, acc.: 96.88%] [G loss: 3.051782]\n",
      "3659 [D loss: 0.120617, acc.: 96.88%] [G loss: 2.944661]\n",
      "3660 [D loss: 0.115767, acc.: 96.88%] [G loss: 2.974262]\n",
      "3661 [D loss: 0.122476, acc.: 96.88%] [G loss: 2.916633]\n",
      "3662 [D loss: 0.117724, acc.: 96.88%] [G loss: 2.989869]\n",
      "3663 [D loss: 0.127313, acc.: 96.88%] [G loss: 2.982771]\n",
      "3664 [D loss: 0.120933, acc.: 96.88%] [G loss: 2.889914]\n",
      "3665 [D loss: 0.118142, acc.: 96.88%] [G loss: 2.869221]\n",
      "3666 [D loss: 0.124473, acc.: 96.88%] [G loss: 2.926984]\n",
      "3667 [D loss: 0.129672, acc.: 96.88%] [G loss: 2.833779]\n",
      "3668 [D loss: 0.116313, acc.: 96.88%] [G loss: 2.927942]\n",
      "3669 [D loss: 0.114497, acc.: 96.88%] [G loss: 2.957169]\n",
      "3670 [D loss: 0.121946, acc.: 96.88%] [G loss: 2.946246]\n",
      "3671 [D loss: 0.114274, acc.: 96.88%] [G loss: 2.952757]\n",
      "3672 [D loss: 0.113519, acc.: 96.88%] [G loss: 2.946619]\n",
      "3673 [D loss: 0.119957, acc.: 96.88%] [G loss: 2.922042]\n",
      "3674 [D loss: 0.122072, acc.: 96.88%] [G loss: 3.111495]\n",
      "3675 [D loss: 0.124885, acc.: 96.88%] [G loss: 2.930284]\n",
      "3676 [D loss: 0.121440, acc.: 96.88%] [G loss: 2.906719]\n",
      "3677 [D loss: 0.119802, acc.: 96.88%] [G loss: 2.961020]\n",
      "3678 [D loss: 0.119951, acc.: 96.88%] [G loss: 2.981867]\n",
      "3679 [D loss: 0.118123, acc.: 96.88%] [G loss: 2.874228]\n",
      "3680 [D loss: 0.126074, acc.: 96.88%] [G loss: 2.891444]\n",
      "3681 [D loss: 0.122218, acc.: 96.88%] [G loss: 2.888047]\n",
      "3682 [D loss: 0.119088, acc.: 96.88%] [G loss: 3.035193]\n",
      "3683 [D loss: 0.114814, acc.: 96.88%] [G loss: 2.904978]\n",
      "3684 [D loss: 0.125579, acc.: 96.88%] [G loss: 2.950449]\n",
      "3685 [D loss: 0.125587, acc.: 96.88%] [G loss: 2.908236]\n",
      "3686 [D loss: 0.119307, acc.: 96.88%] [G loss: 2.921703]\n",
      "3687 [D loss: 0.121749, acc.: 96.88%] [G loss: 3.113209]\n",
      "3688 [D loss: 0.119867, acc.: 96.88%] [G loss: 2.897792]\n",
      "3689 [D loss: 0.118064, acc.: 96.88%] [G loss: 3.071353]\n",
      "3690 [D loss: 0.123828, acc.: 96.88%] [G loss: 3.009173]\n",
      "3691 [D loss: 0.121517, acc.: 96.88%] [G loss: 2.925090]\n",
      "3692 [D loss: 0.120132, acc.: 96.88%] [G loss: 2.917573]\n",
      "3693 [D loss: 0.123722, acc.: 96.88%] [G loss: 2.953825]\n",
      "3694 [D loss: 0.122156, acc.: 96.88%] [G loss: 2.858553]\n",
      "3695 [D loss: 0.122592, acc.: 96.88%] [G loss: 2.860891]\n",
      "3696 [D loss: 0.117423, acc.: 96.88%] [G loss: 2.961507]\n",
      "3697 [D loss: 0.118260, acc.: 96.88%] [G loss: 2.940460]\n",
      "3698 [D loss: 0.123474, acc.: 96.88%] [G loss: 2.853814]\n",
      "3699 [D loss: 0.118044, acc.: 96.88%] [G loss: 3.035664]\n",
      "3700 [D loss: 0.114861, acc.: 96.88%] [G loss: 3.006571]\n",
      "generated_data\n",
      "3701 [D loss: 0.117247, acc.: 96.88%] [G loss: 3.112401]\n",
      "3702 [D loss: 0.124305, acc.: 96.88%] [G loss: 3.035833]\n",
      "3703 [D loss: 0.121704, acc.: 96.88%] [G loss: 2.922340]\n",
      "3704 [D loss: 0.127777, acc.: 96.88%] [G loss: 2.873550]\n",
      "3705 [D loss: 0.117525, acc.: 96.88%] [G loss: 2.907619]\n",
      "3706 [D loss: 0.117848, acc.: 96.88%] [G loss: 2.988642]\n",
      "3707 [D loss: 0.116272, acc.: 96.88%] [G loss: 2.888758]\n",
      "3708 [D loss: 0.122904, acc.: 96.88%] [G loss: 2.918157]\n",
      "3709 [D loss: 0.122388, acc.: 96.88%] [G loss: 2.915425]\n",
      "3710 [D loss: 0.122255, acc.: 96.88%] [G loss: 2.964816]\n",
      "3711 [D loss: 0.120902, acc.: 96.88%] [G loss: 2.932146]\n",
      "3712 [D loss: 0.120121, acc.: 96.88%] [G loss: 2.869037]\n",
      "3713 [D loss: 0.119883, acc.: 96.88%] [G loss: 2.883779]\n",
      "3714 [D loss: 0.125077, acc.: 96.88%] [G loss: 2.917665]\n",
      "3715 [D loss: 0.122381, acc.: 96.88%] [G loss: 2.955962]\n",
      "3716 [D loss: 0.123408, acc.: 96.88%] [G loss: 2.913998]\n",
      "3717 [D loss: 0.128685, acc.: 96.88%] [G loss: 2.875267]\n",
      "3718 [D loss: 0.124946, acc.: 96.88%] [G loss: 2.812577]\n",
      "3719 [D loss: 0.124828, acc.: 96.88%] [G loss: 2.852215]\n",
      "3720 [D loss: 0.127565, acc.: 96.88%] [G loss: 2.881582]\n",
      "3721 [D loss: 0.121582, acc.: 96.88%] [G loss: 2.858342]\n",
      "3722 [D loss: 0.124451, acc.: 96.88%] [G loss: 2.951365]\n",
      "3723 [D loss: 0.121717, acc.: 96.88%] [G loss: 2.866142]\n",
      "3724 [D loss: 0.125703, acc.: 96.88%] [G loss: 2.872631]\n",
      "3725 [D loss: 0.127056, acc.: 96.88%] [G loss: 2.860717]\n",
      "3726 [D loss: 0.118438, acc.: 96.88%] [G loss: 2.943083]\n",
      "3727 [D loss: 0.120446, acc.: 96.88%] [G loss: 3.017642]\n",
      "3728 [D loss: 0.117435, acc.: 96.88%] [G loss: 2.935313]\n",
      "3729 [D loss: 0.120125, acc.: 96.88%] [G loss: 2.931189]\n",
      "3730 [D loss: 0.122801, acc.: 96.88%] [G loss: 2.824678]\n",
      "3731 [D loss: 0.112478, acc.: 96.88%] [G loss: 2.959189]\n",
      "3732 [D loss: 0.122140, acc.: 96.88%] [G loss: 2.930877]\n",
      "3733 [D loss: 0.117102, acc.: 96.88%] [G loss: 2.998400]\n",
      "3734 [D loss: 0.119834, acc.: 96.88%] [G loss: 3.075777]\n",
      "3735 [D loss: 0.115861, acc.: 96.88%] [G loss: 3.015704]\n",
      "3736 [D loss: 0.117980, acc.: 96.88%] [G loss: 3.031208]\n",
      "3737 [D loss: 0.114846, acc.: 96.88%] [G loss: 3.005099]\n",
      "3738 [D loss: 0.121739, acc.: 96.88%] [G loss: 3.026107]\n",
      "3739 [D loss: 0.121965, acc.: 96.88%] [G loss: 2.971127]\n",
      "3740 [D loss: 0.118065, acc.: 96.88%] [G loss: 2.895376]\n",
      "3741 [D loss: 0.118928, acc.: 96.88%] [G loss: 2.942182]\n",
      "3742 [D loss: 0.121615, acc.: 96.88%] [G loss: 2.967537]\n",
      "3743 [D loss: 0.120551, acc.: 96.88%] [G loss: 2.918740]\n",
      "3744 [D loss: 0.122514, acc.: 96.88%] [G loss: 2.866793]\n",
      "3745 [D loss: 0.119748, acc.: 96.88%] [G loss: 3.025844]\n",
      "3746 [D loss: 0.117751, acc.: 96.88%] [G loss: 3.035295]\n",
      "3747 [D loss: 0.118150, acc.: 96.88%] [G loss: 2.971952]\n",
      "3748 [D loss: 0.123681, acc.: 96.88%] [G loss: 2.882719]\n",
      "3749 [D loss: 0.111056, acc.: 96.88%] [G loss: 2.973502]\n",
      "3750 [D loss: 0.119024, acc.: 96.88%] [G loss: 2.966577]\n",
      "3751 [D loss: 0.121537, acc.: 96.88%] [G loss: 2.938120]\n",
      "3752 [D loss: 0.124199, acc.: 96.88%] [G loss: 2.887384]\n",
      "3753 [D loss: 0.125365, acc.: 96.88%] [G loss: 2.865391]\n",
      "3754 [D loss: 0.118592, acc.: 96.88%] [G loss: 2.881665]\n",
      "3755 [D loss: 0.122078, acc.: 96.88%] [G loss: 2.846688]\n",
      "3756 [D loss: 0.120729, acc.: 96.88%] [G loss: 2.930409]\n",
      "3757 [D loss: 0.127260, acc.: 96.88%] [G loss: 2.877711]\n",
      "3758 [D loss: 0.124397, acc.: 96.88%] [G loss: 2.855243]\n",
      "3759 [D loss: 0.123250, acc.: 96.88%] [G loss: 2.875957]\n",
      "3760 [D loss: 0.122483, acc.: 96.88%] [G loss: 2.889481]\n",
      "3761 [D loss: 0.121568, acc.: 96.88%] [G loss: 2.940586]\n",
      "3762 [D loss: 0.115249, acc.: 96.88%] [G loss: 2.902494]\n",
      "3763 [D loss: 0.120719, acc.: 96.88%] [G loss: 2.954953]\n",
      "3764 [D loss: 0.119212, acc.: 96.88%] [G loss: 2.932674]\n",
      "3765 [D loss: 0.122433, acc.: 96.88%] [G loss: 3.027096]\n",
      "3766 [D loss: 0.117804, acc.: 96.88%] [G loss: 2.913111]\n",
      "3767 [D loss: 0.120213, acc.: 96.88%] [G loss: 2.919060]\n",
      "3768 [D loss: 0.130988, acc.: 96.88%] [G loss: 2.928026]\n",
      "3769 [D loss: 0.120443, acc.: 96.88%] [G loss: 2.907427]\n",
      "3770 [D loss: 0.125087, acc.: 96.88%] [G loss: 2.892489]\n",
      "3771 [D loss: 0.122244, acc.: 96.88%] [G loss: 2.993502]\n",
      "3772 [D loss: 0.119521, acc.: 96.88%] [G loss: 2.918976]\n",
      "3773 [D loss: 0.111827, acc.: 96.88%] [G loss: 2.960728]\n",
      "3774 [D loss: 0.125674, acc.: 96.88%] [G loss: 2.873143]\n",
      "3775 [D loss: 0.122394, acc.: 96.88%] [G loss: 2.876481]\n",
      "3776 [D loss: 0.122450, acc.: 96.88%] [G loss: 3.057424]\n",
      "3777 [D loss: 0.123747, acc.: 96.88%] [G loss: 2.947567]\n",
      "3778 [D loss: 0.122547, acc.: 96.88%] [G loss: 2.852973]\n",
      "3779 [D loss: 0.122742, acc.: 96.88%] [G loss: 2.910908]\n",
      "3780 [D loss: 0.112441, acc.: 96.88%] [G loss: 2.916858]\n",
      "3781 [D loss: 0.125248, acc.: 96.88%] [G loss: 2.892908]\n",
      "3782 [D loss: 0.120899, acc.: 96.88%] [G loss: 2.979423]\n",
      "3783 [D loss: 0.115087, acc.: 96.88%] [G loss: 2.971465]\n",
      "3784 [D loss: 0.118948, acc.: 96.88%] [G loss: 2.907888]\n",
      "3785 [D loss: 0.127554, acc.: 96.88%] [G loss: 2.878505]\n",
      "3786 [D loss: 0.123421, acc.: 96.88%] [G loss: 2.873915]\n",
      "3787 [D loss: 0.118270, acc.: 96.88%] [G loss: 2.978050]\n",
      "3788 [D loss: 0.124612, acc.: 96.88%] [G loss: 2.881214]\n",
      "3789 [D loss: 0.122200, acc.: 96.88%] [G loss: 2.953114]\n",
      "3790 [D loss: 0.123583, acc.: 96.88%] [G loss: 2.824515]\n",
      "3791 [D loss: 0.121136, acc.: 96.88%] [G loss: 2.907391]\n",
      "3792 [D loss: 0.123427, acc.: 96.88%] [G loss: 2.879122]\n",
      "3793 [D loss: 0.123460, acc.: 96.88%] [G loss: 2.827172]\n",
      "3794 [D loss: 0.119994, acc.: 96.88%] [G loss: 2.898499]\n",
      "3795 [D loss: 0.120627, acc.: 96.88%] [G loss: 2.831619]\n",
      "3796 [D loss: 0.120605, acc.: 96.88%] [G loss: 2.910918]\n",
      "3797 [D loss: 0.115111, acc.: 96.88%] [G loss: 2.921347]\n",
      "3798 [D loss: 0.120551, acc.: 96.88%] [G loss: 2.872664]\n",
      "3799 [D loss: 0.121962, acc.: 96.88%] [G loss: 3.056471]\n",
      "3800 [D loss: 0.117964, acc.: 96.88%] [G loss: 2.961251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_data\n",
      "3801 [D loss: 0.122103, acc.: 96.88%] [G loss: 2.852753]\n",
      "3802 [D loss: 0.125102, acc.: 96.88%] [G loss: 2.927309]\n",
      "3803 [D loss: 0.118690, acc.: 96.88%] [G loss: 2.924878]\n",
      "3804 [D loss: 0.116221, acc.: 96.88%] [G loss: 2.912899]\n",
      "3805 [D loss: 0.123944, acc.: 96.88%] [G loss: 2.884078]\n",
      "3806 [D loss: 0.123821, acc.: 96.88%] [G loss: 2.949887]\n",
      "3807 [D loss: 0.120327, acc.: 96.88%] [G loss: 2.996830]\n",
      "3808 [D loss: 0.109659, acc.: 96.88%] [G loss: 2.972240]\n",
      "3809 [D loss: 0.115201, acc.: 96.88%] [G loss: 2.987120]\n",
      "3810 [D loss: 0.120252, acc.: 96.88%] [G loss: 2.884549]\n",
      "3811 [D loss: 0.115630, acc.: 96.88%] [G loss: 2.970895]\n",
      "3812 [D loss: 0.122086, acc.: 96.88%] [G loss: 2.973405]\n",
      "3813 [D loss: 0.122673, acc.: 96.88%] [G loss: 2.902112]\n",
      "3814 [D loss: 0.122348, acc.: 96.88%] [G loss: 2.947282]\n",
      "3815 [D loss: 0.126803, acc.: 96.88%] [G loss: 2.915940]\n",
      "3816 [D loss: 0.128221, acc.: 96.88%] [G loss: 2.903138]\n",
      "3817 [D loss: 0.127082, acc.: 96.88%] [G loss: 2.851543]\n",
      "3818 [D loss: 0.115669, acc.: 96.88%] [G loss: 2.956502]\n",
      "3819 [D loss: 0.125902, acc.: 96.88%] [G loss: 2.959461]\n",
      "3820 [D loss: 0.127134, acc.: 96.88%] [G loss: 2.905241]\n",
      "3821 [D loss: 0.118277, acc.: 96.88%] [G loss: 2.951812]\n",
      "3822 [D loss: 0.126377, acc.: 96.88%] [G loss: 2.890709]\n",
      "3823 [D loss: 0.122493, acc.: 96.88%] [G loss: 2.865521]\n",
      "3824 [D loss: 0.121031, acc.: 96.88%] [G loss: 2.872492]\n",
      "3825 [D loss: 0.125439, acc.: 96.88%] [G loss: 2.937508]\n",
      "3826 [D loss: 0.121552, acc.: 96.88%] [G loss: 2.875011]\n",
      "3827 [D loss: 0.124871, acc.: 96.88%] [G loss: 2.879494]\n",
      "3828 [D loss: 0.121401, acc.: 96.88%] [G loss: 2.935596]\n",
      "3829 [D loss: 0.121302, acc.: 96.88%] [G loss: 2.934130]\n",
      "3830 [D loss: 0.117111, acc.: 96.88%] [G loss: 2.950036]\n",
      "3831 [D loss: 0.115834, acc.: 96.88%] [G loss: 2.957933]\n",
      "3832 [D loss: 0.120560, acc.: 96.88%] [G loss: 3.036259]\n",
      "3833 [D loss: 0.122558, acc.: 96.88%] [G loss: 2.940821]\n",
      "3834 [D loss: 0.127489, acc.: 96.88%] [G loss: 2.977342]\n",
      "3835 [D loss: 0.121594, acc.: 96.88%] [G loss: 2.936051]\n",
      "3836 [D loss: 0.123752, acc.: 96.88%] [G loss: 3.004002]\n",
      "3837 [D loss: 0.123954, acc.: 96.88%] [G loss: 2.921271]\n",
      "3838 [D loss: 0.118989, acc.: 96.88%] [G loss: 2.938628]\n",
      "3839 [D loss: 0.120916, acc.: 96.88%] [G loss: 2.827263]\n",
      "3840 [D loss: 0.120825, acc.: 96.88%] [G loss: 2.935953]\n",
      "3841 [D loss: 0.122475, acc.: 96.88%] [G loss: 2.933341]\n",
      "3842 [D loss: 0.121278, acc.: 96.88%] [G loss: 2.906651]\n",
      "3843 [D loss: 0.120434, acc.: 96.88%] [G loss: 2.962254]\n",
      "3844 [D loss: 0.124552, acc.: 96.88%] [G loss: 2.907315]\n",
      "3845 [D loss: 0.116760, acc.: 96.88%] [G loss: 2.860128]\n",
      "3846 [D loss: 0.124093, acc.: 96.88%] [G loss: 3.059783]\n",
      "3847 [D loss: 0.122690, acc.: 96.88%] [G loss: 2.960323]\n",
      "3848 [D loss: 0.118470, acc.: 96.88%] [G loss: 3.008368]\n",
      "3849 [D loss: 0.116385, acc.: 96.88%] [G loss: 3.036225]\n",
      "3850 [D loss: 0.112769, acc.: 96.88%] [G loss: 2.974496]\n",
      "3851 [D loss: 0.123302, acc.: 96.88%] [G loss: 3.106919]\n",
      "3852 [D loss: 0.117245, acc.: 96.88%] [G loss: 2.957754]\n",
      "3853 [D loss: 0.121554, acc.: 96.88%] [G loss: 2.880800]\n",
      "3854 [D loss: 0.122665, acc.: 96.88%] [G loss: 2.877066]\n",
      "3855 [D loss: 0.128155, acc.: 96.88%] [G loss: 2.846023]\n",
      "3856 [D loss: 0.123100, acc.: 96.88%] [G loss: 2.953066]\n",
      "3857 [D loss: 0.119148, acc.: 96.88%] [G loss: 2.927164]\n",
      "3858 [D loss: 0.116317, acc.: 96.88%] [G loss: 2.956981]\n",
      "3859 [D loss: 0.121629, acc.: 96.88%] [G loss: 2.879783]\n",
      "3860 [D loss: 0.124624, acc.: 96.88%] [G loss: 2.899655]\n",
      "3861 [D loss: 0.121539, acc.: 96.88%] [G loss: 2.935523]\n",
      "3862 [D loss: 0.119840, acc.: 96.88%] [G loss: 2.846927]\n",
      "3863 [D loss: 0.115683, acc.: 96.88%] [G loss: 2.882891]\n",
      "3864 [D loss: 0.124995, acc.: 96.88%] [G loss: 2.951887]\n",
      "3865 [D loss: 0.117325, acc.: 96.88%] [G loss: 2.920112]\n",
      "3866 [D loss: 0.123822, acc.: 96.88%] [G loss: 2.942659]\n",
      "3867 [D loss: 0.119206, acc.: 96.88%] [G loss: 3.115928]\n",
      "3868 [D loss: 0.122200, acc.: 96.88%] [G loss: 2.963726]\n",
      "3869 [D loss: 0.119309, acc.: 96.88%] [G loss: 2.925585]\n",
      "3870 [D loss: 0.121624, acc.: 96.88%] [G loss: 2.898980]\n",
      "3871 [D loss: 0.121473, acc.: 96.88%] [G loss: 2.918916]\n",
      "3872 [D loss: 0.121126, acc.: 96.88%] [G loss: 3.025095]\n",
      "3873 [D loss: 0.109968, acc.: 96.88%] [G loss: 3.094110]\n",
      "3874 [D loss: 0.122358, acc.: 96.88%] [G loss: 3.048479]\n",
      "3875 [D loss: 0.124914, acc.: 96.88%] [G loss: 2.935488]\n",
      "3876 [D loss: 0.122368, acc.: 96.88%] [G loss: 2.856050]\n",
      "3877 [D loss: 0.123349, acc.: 96.88%] [G loss: 2.877454]\n",
      "3878 [D loss: 0.122359, acc.: 96.88%] [G loss: 2.959132]\n",
      "3879 [D loss: 0.120632, acc.: 96.88%] [G loss: 2.899166]\n",
      "3880 [D loss: 0.121505, acc.: 96.88%] [G loss: 2.859741]\n",
      "3881 [D loss: 0.121202, acc.: 96.88%] [G loss: 2.970760]\n",
      "3882 [D loss: 0.123915, acc.: 96.88%] [G loss: 2.869883]\n",
      "3883 [D loss: 0.117718, acc.: 96.88%] [G loss: 2.905732]\n",
      "3884 [D loss: 0.114498, acc.: 96.88%] [G loss: 2.951105]\n",
      "3885 [D loss: 0.121144, acc.: 96.88%] [G loss: 2.890947]\n",
      "3886 [D loss: 0.122961, acc.: 96.88%] [G loss: 2.991346]\n",
      "3887 [D loss: 0.114861, acc.: 96.88%] [G loss: 2.937814]\n",
      "3888 [D loss: 0.120667, acc.: 96.88%] [G loss: 2.905220]\n",
      "3889 [D loss: 0.120379, acc.: 96.88%] [G loss: 2.898669]\n",
      "3890 [D loss: 0.124581, acc.: 96.88%] [G loss: 2.958112]\n",
      "3891 [D loss: 0.122435, acc.: 96.88%] [G loss: 2.862878]\n",
      "3892 [D loss: 0.123476, acc.: 96.88%] [G loss: 2.886691]\n",
      "3893 [D loss: 0.119421, acc.: 96.88%] [G loss: 2.911537]\n",
      "3894 [D loss: 0.120369, acc.: 96.88%] [G loss: 2.914018]\n",
      "3895 [D loss: 0.119765, acc.: 96.88%] [G loss: 2.908092]\n",
      "3896 [D loss: 0.119075, acc.: 96.88%] [G loss: 2.892045]\n",
      "3897 [D loss: 0.120951, acc.: 96.88%] [G loss: 2.975725]\n",
      "3898 [D loss: 0.126136, acc.: 96.88%] [G loss: 2.907017]\n",
      "3899 [D loss: 0.117322, acc.: 96.88%] [G loss: 2.904160]\n",
      "3900 [D loss: 0.115793, acc.: 96.88%] [G loss: 2.912169]\n",
      "generated_data\n",
      "3901 [D loss: 0.123464, acc.: 96.88%] [G loss: 2.899202]\n",
      "3902 [D loss: 0.121094, acc.: 96.88%] [G loss: 2.942911]\n",
      "3903 [D loss: 0.118758, acc.: 96.88%] [G loss: 2.982656]\n",
      "3904 [D loss: 0.125648, acc.: 96.88%] [G loss: 2.943974]\n",
      "3905 [D loss: 0.112989, acc.: 96.88%] [G loss: 2.955508]\n",
      "3906 [D loss: 0.127707, acc.: 96.88%] [G loss: 2.979767]\n",
      "3907 [D loss: 0.115293, acc.: 96.88%] [G loss: 2.998978]\n",
      "3908 [D loss: 0.124315, acc.: 96.88%] [G loss: 2.927843]\n",
      "3909 [D loss: 0.122412, acc.: 96.88%] [G loss: 2.878888]\n",
      "3910 [D loss: 0.124217, acc.: 96.88%] [G loss: 2.847448]\n",
      "3911 [D loss: 0.121487, acc.: 96.88%] [G loss: 2.864270]\n",
      "3912 [D loss: 0.115059, acc.: 96.88%] [G loss: 2.936124]\n",
      "3913 [D loss: 0.120958, acc.: 96.88%] [G loss: 2.944062]\n",
      "3914 [D loss: 0.117296, acc.: 96.88%] [G loss: 2.994586]\n",
      "3915 [D loss: 0.121471, acc.: 96.88%] [G loss: 3.041840]\n",
      "3916 [D loss: 0.115791, acc.: 96.88%] [G loss: 3.109095]\n",
      "3917 [D loss: 0.120234, acc.: 96.88%] [G loss: 2.960343]\n",
      "3918 [D loss: 0.119982, acc.: 96.88%] [G loss: 2.965480]\n",
      "3919 [D loss: 0.122089, acc.: 96.88%] [G loss: 3.048275]\n",
      "3920 [D loss: 0.119158, acc.: 96.88%] [G loss: 2.907002]\n",
      "3921 [D loss: 0.120429, acc.: 96.88%] [G loss: 2.888783]\n",
      "3922 [D loss: 0.120468, acc.: 96.88%] [G loss: 2.869315]\n",
      "3923 [D loss: 0.116024, acc.: 96.88%] [G loss: 2.947277]\n",
      "3924 [D loss: 0.117604, acc.: 96.88%] [G loss: 2.937281]\n",
      "3925 [D loss: 0.119972, acc.: 96.88%] [G loss: 2.923161]\n",
      "3926 [D loss: 0.119027, acc.: 96.88%] [G loss: 2.847952]\n",
      "3927 [D loss: 0.119803, acc.: 96.88%] [G loss: 3.007905]\n",
      "3928 [D loss: 0.124118, acc.: 96.88%] [G loss: 2.956625]\n",
      "3929 [D loss: 0.117245, acc.: 96.88%] [G loss: 3.048199]\n",
      "3930 [D loss: 0.117146, acc.: 96.88%] [G loss: 2.914586]\n",
      "3931 [D loss: 0.121537, acc.: 96.88%] [G loss: 2.903292]\n",
      "3932 [D loss: 0.121482, acc.: 96.88%] [G loss: 2.921752]\n",
      "3933 [D loss: 0.126611, acc.: 96.88%] [G loss: 3.080378]\n",
      "3934 [D loss: 0.119924, acc.: 96.88%] [G loss: 2.932077]\n",
      "3935 [D loss: 0.123974, acc.: 96.88%] [G loss: 2.918418]\n",
      "3936 [D loss: 0.124379, acc.: 96.88%] [G loss: 2.887968]\n",
      "3937 [D loss: 0.127225, acc.: 96.88%] [G loss: 2.939272]\n",
      "3938 [D loss: 0.118897, acc.: 96.88%] [G loss: 2.904291]\n",
      "3939 [D loss: 0.119435, acc.: 96.88%] [G loss: 2.916103]\n",
      "3940 [D loss: 0.122815, acc.: 96.88%] [G loss: 2.908119]\n",
      "3941 [D loss: 0.121971, acc.: 96.88%] [G loss: 2.884351]\n",
      "3942 [D loss: 0.124057, acc.: 96.88%] [G loss: 2.890878]\n",
      "3943 [D loss: 0.124548, acc.: 96.88%] [G loss: 3.035794]\n",
      "3944 [D loss: 0.117455, acc.: 96.88%] [G loss: 2.868380]\n",
      "3945 [D loss: 0.120638, acc.: 96.88%] [G loss: 2.878785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3946 [D loss: 0.120028, acc.: 96.88%] [G loss: 2.978252]\n",
      "3947 [D loss: 0.120644, acc.: 96.88%] [G loss: 2.912741]\n",
      "3948 [D loss: 0.119914, acc.: 96.88%] [G loss: 2.907489]\n",
      "3949 [D loss: 0.119644, acc.: 96.88%] [G loss: 2.961771]\n",
      "3950 [D loss: 0.122517, acc.: 96.88%] [G loss: 2.869123]\n",
      "3951 [D loss: 0.121598, acc.: 96.88%] [G loss: 2.938265]\n",
      "3952 [D loss: 0.123368, acc.: 96.88%] [G loss: 2.865059]\n",
      "3953 [D loss: 0.118320, acc.: 96.88%] [G loss: 2.992121]\n",
      "3954 [D loss: 0.118672, acc.: 96.88%] [G loss: 2.939485]\n",
      "3955 [D loss: 0.112424, acc.: 96.88%] [G loss: 2.974297]\n",
      "3956 [D loss: 0.126143, acc.: 96.88%] [G loss: 2.972974]\n",
      "3957 [D loss: 0.115064, acc.: 96.88%] [G loss: 2.997767]\n",
      "3958 [D loss: 0.120830, acc.: 96.88%] [G loss: 2.944120]\n",
      "3959 [D loss: 0.119611, acc.: 96.88%] [G loss: 3.005887]\n",
      "3960 [D loss: 0.125572, acc.: 96.88%] [G loss: 2.909958]\n",
      "3961 [D loss: 0.120445, acc.: 96.88%] [G loss: 2.994031]\n",
      "3962 [D loss: 0.123996, acc.: 96.88%] [G loss: 2.891973]\n",
      "3963 [D loss: 0.125840, acc.: 96.88%] [G loss: 2.928205]\n",
      "3964 [D loss: 0.121897, acc.: 96.88%] [G loss: 2.985229]\n",
      "3965 [D loss: 0.119279, acc.: 96.88%] [G loss: 2.949404]\n",
      "3966 [D loss: 0.124616, acc.: 96.88%] [G loss: 2.870077]\n",
      "3967 [D loss: 0.124089, acc.: 96.88%] [G loss: 2.891639]\n",
      "3968 [D loss: 0.122692, acc.: 96.88%] [G loss: 3.037716]\n",
      "3969 [D loss: 0.120096, acc.: 96.88%] [G loss: 2.969800]\n",
      "3970 [D loss: 0.124968, acc.: 96.88%] [G loss: 2.890013]\n",
      "3971 [D loss: 0.124213, acc.: 96.88%] [G loss: 2.819324]\n",
      "3972 [D loss: 0.119127, acc.: 96.88%] [G loss: 2.815402]\n",
      "3973 [D loss: 0.119873, acc.: 96.88%] [G loss: 2.871942]\n",
      "3974 [D loss: 0.121363, acc.: 96.88%] [G loss: 2.881069]\n",
      "3975 [D loss: 0.122420, acc.: 96.88%] [G loss: 3.036176]\n",
      "3976 [D loss: 0.120487, acc.: 96.88%] [G loss: 3.005124]\n",
      "3977 [D loss: 0.121699, acc.: 96.88%] [G loss: 2.948081]\n",
      "3978 [D loss: 0.121157, acc.: 96.88%] [G loss: 2.991313]\n",
      "3979 [D loss: 0.122424, acc.: 96.88%] [G loss: 2.960567]\n",
      "3980 [D loss: 0.126197, acc.: 96.88%] [G loss: 2.910868]\n",
      "3981 [D loss: 0.118106, acc.: 96.88%] [G loss: 2.918520]\n",
      "3982 [D loss: 0.120548, acc.: 96.88%] [G loss: 2.991587]\n",
      "3983 [D loss: 0.123113, acc.: 96.88%] [G loss: 2.926523]\n",
      "3984 [D loss: 0.123682, acc.: 96.88%] [G loss: 3.009780]\n",
      "3985 [D loss: 0.124589, acc.: 96.88%] [G loss: 2.981637]\n",
      "3986 [D loss: 0.116766, acc.: 96.88%] [G loss: 2.930106]\n",
      "3987 [D loss: 0.123482, acc.: 96.88%] [G loss: 2.925121]\n",
      "3988 [D loss: 0.114712, acc.: 96.88%] [G loss: 2.904280]\n",
      "3989 [D loss: 0.118226, acc.: 96.88%] [G loss: 2.945761]\n",
      "3990 [D loss: 0.122539, acc.: 96.88%] [G loss: 2.907799]\n",
      "3991 [D loss: 0.119841, acc.: 96.88%] [G loss: 2.919780]\n",
      "3992 [D loss: 0.115458, acc.: 96.88%] [G loss: 2.958771]\n",
      "3993 [D loss: 0.126425, acc.: 96.88%] [G loss: 2.917382]\n",
      "3994 [D loss: 0.122354, acc.: 96.88%] [G loss: 2.905401]\n",
      "3995 [D loss: 0.119088, acc.: 96.88%] [G loss: 2.904951]\n",
      "3996 [D loss: 0.115486, acc.: 96.88%] [G loss: 2.906967]\n",
      "3997 [D loss: 0.116852, acc.: 96.88%] [G loss: 2.946939]\n",
      "3998 [D loss: 0.124786, acc.: 96.88%] [G loss: 2.937040]\n",
      "3999 [D loss: 0.120118, acc.: 96.88%] [G loss: 2.947936]\n",
      "4000 [D loss: 0.128062, acc.: 96.88%] [G loss: 2.918215]\n",
      "generated_data\n",
      "4001 [D loss: 0.125376, acc.: 96.88%] [G loss: 2.928044]\n",
      "4002 [D loss: 0.121059, acc.: 96.88%] [G loss: 2.943379]\n",
      "4003 [D loss: 0.117255, acc.: 96.88%] [G loss: 2.905180]\n",
      "4004 [D loss: 0.124432, acc.: 96.88%] [G loss: 2.966953]\n",
      "4005 [D loss: 0.125296, acc.: 96.88%] [G loss: 2.795389]\n",
      "4006 [D loss: 0.117352, acc.: 96.88%] [G loss: 2.997128]\n",
      "4007 [D loss: 0.116514, acc.: 96.88%] [G loss: 3.016802]\n",
      "4008 [D loss: 0.118470, acc.: 96.88%] [G loss: 2.917801]\n",
      "4009 [D loss: 0.125029, acc.: 96.88%] [G loss: 2.883397]\n",
      "4010 [D loss: 0.115320, acc.: 96.88%] [G loss: 2.911621]\n",
      "4011 [D loss: 0.119518, acc.: 96.88%] [G loss: 3.016189]\n",
      "4012 [D loss: 0.118201, acc.: 96.88%] [G loss: 2.941112]\n",
      "4013 [D loss: 0.126262, acc.: 96.88%] [G loss: 2.866698]\n",
      "4014 [D loss: 0.127779, acc.: 96.88%] [G loss: 2.862509]\n",
      "4015 [D loss: 0.123347, acc.: 96.88%] [G loss: 2.918061]\n",
      "4016 [D loss: 0.114324, acc.: 96.88%] [G loss: 2.884549]\n",
      "4017 [D loss: 0.123347, acc.: 96.88%] [G loss: 2.925924]\n",
      "4018 [D loss: 0.121272, acc.: 96.88%] [G loss: 2.916326]\n",
      "4019 [D loss: 0.120855, acc.: 96.88%] [G loss: 2.919269]\n",
      "4020 [D loss: 0.121177, acc.: 96.88%] [G loss: 2.891090]\n",
      "4021 [D loss: 0.124654, acc.: 96.88%] [G loss: 2.864197]\n",
      "4022 [D loss: 0.126922, acc.: 96.88%] [G loss: 2.862164]\n",
      "4023 [D loss: 0.119850, acc.: 96.88%] [G loss: 2.843748]\n",
      "4024 [D loss: 0.120823, acc.: 96.88%] [G loss: 2.888195]\n",
      "4025 [D loss: 0.116644, acc.: 96.88%] [G loss: 2.881668]\n",
      "4026 [D loss: 0.122488, acc.: 96.88%] [G loss: 2.859404]\n",
      "4027 [D loss: 0.119850, acc.: 96.88%] [G loss: 2.993412]\n",
      "4028 [D loss: 0.116557, acc.: 96.88%] [G loss: 2.946667]\n",
      "4029 [D loss: 0.120873, acc.: 96.88%] [G loss: 2.921093]\n",
      "4030 [D loss: 0.122927, acc.: 96.88%] [G loss: 2.918672]\n",
      "4031 [D loss: 0.121851, acc.: 96.88%] [G loss: 2.871789]\n",
      "4032 [D loss: 0.121439, acc.: 96.88%] [G loss: 2.900669]\n",
      "4033 [D loss: 0.123585, acc.: 96.88%] [G loss: 2.875093]\n",
      "4034 [D loss: 0.126454, acc.: 96.88%] [G loss: 3.050710]\n",
      "4035 [D loss: 0.120708, acc.: 96.88%] [G loss: 2.981852]\n",
      "4036 [D loss: 0.120379, acc.: 96.88%] [G loss: 2.865605]\n",
      "4037 [D loss: 0.118129, acc.: 96.88%] [G loss: 2.826279]\n",
      "4038 [D loss: 0.121821, acc.: 96.88%] [G loss: 2.893327]\n",
      "4039 [D loss: 0.116994, acc.: 96.88%] [G loss: 2.965057]\n",
      "4040 [D loss: 0.120130, acc.: 96.88%] [G loss: 2.843466]\n",
      "4041 [D loss: 0.119799, acc.: 96.88%] [G loss: 2.902408]\n",
      "4042 [D loss: 0.113040, acc.: 96.88%] [G loss: 3.062340]\n",
      "4043 [D loss: 0.124043, acc.: 96.88%] [G loss: 2.821674]\n",
      "4044 [D loss: 0.125281, acc.: 96.88%] [G loss: 2.915962]\n",
      "4045 [D loss: 0.119209, acc.: 96.88%] [G loss: 2.900291]\n",
      "4046 [D loss: 0.123327, acc.: 96.88%] [G loss: 2.975493]\n",
      "4047 [D loss: 0.116269, acc.: 96.88%] [G loss: 3.007052]\n",
      "4048 [D loss: 0.119136, acc.: 96.88%] [G loss: 2.884418]\n",
      "4049 [D loss: 0.119079, acc.: 96.88%] [G loss: 2.888485]\n",
      "4050 [D loss: 0.118459, acc.: 96.88%] [G loss: 2.911150]\n",
      "4051 [D loss: 0.116040, acc.: 96.88%] [G loss: 2.994893]\n",
      "4052 [D loss: 0.120910, acc.: 96.88%] [G loss: 2.966475]\n",
      "4053 [D loss: 0.120612, acc.: 96.88%] [G loss: 3.117179]\n",
      "4054 [D loss: 0.124142, acc.: 96.88%] [G loss: 2.964976]\n",
      "4055 [D loss: 0.118325, acc.: 96.88%] [G loss: 2.939852]\n",
      "4056 [D loss: 0.117943, acc.: 96.88%] [G loss: 2.933353]\n",
      "4057 [D loss: 0.118510, acc.: 96.88%] [G loss: 2.974028]\n",
      "4058 [D loss: 0.125018, acc.: 96.88%] [G loss: 2.945062]\n",
      "4059 [D loss: 0.126575, acc.: 96.88%] [G loss: 2.885619]\n",
      "4060 [D loss: 0.118014, acc.: 96.88%] [G loss: 2.908927]\n",
      "4061 [D loss: 0.122235, acc.: 96.88%] [G loss: 3.060036]\n",
      "4062 [D loss: 0.120499, acc.: 96.88%] [G loss: 2.884109]\n",
      "4063 [D loss: 0.120487, acc.: 96.88%] [G loss: 2.994688]\n",
      "4064 [D loss: 0.114218, acc.: 96.88%] [G loss: 2.939887]\n",
      "4065 [D loss: 0.118705, acc.: 96.88%] [G loss: 2.928434]\n",
      "4066 [D loss: 0.118354, acc.: 96.88%] [G loss: 3.050708]\n",
      "4067 [D loss: 0.119655, acc.: 96.88%] [G loss: 2.881998]\n",
      "4068 [D loss: 0.125743, acc.: 96.88%] [G loss: 2.889203]\n",
      "4069 [D loss: 0.123161, acc.: 96.88%] [G loss: 2.835978]\n",
      "4070 [D loss: 0.123834, acc.: 96.88%] [G loss: 2.876694]\n",
      "4071 [D loss: 0.118236, acc.: 96.88%] [G loss: 2.913442]\n",
      "4072 [D loss: 0.127323, acc.: 96.88%] [G loss: 2.959563]\n",
      "4073 [D loss: 0.122111, acc.: 96.88%] [G loss: 2.897254]\n",
      "4074 [D loss: 0.123669, acc.: 96.88%] [G loss: 2.882593]\n",
      "4075 [D loss: 0.114036, acc.: 96.88%] [G loss: 2.915687]\n",
      "4076 [D loss: 0.111034, acc.: 96.88%] [G loss: 2.905696]\n",
      "4077 [D loss: 0.123283, acc.: 96.88%] [G loss: 2.864833]\n",
      "4078 [D loss: 0.122090, acc.: 96.88%] [G loss: 2.952457]\n",
      "4079 [D loss: 0.114737, acc.: 96.88%] [G loss: 2.950182]\n",
      "4080 [D loss: 0.128040, acc.: 96.88%] [G loss: 2.944297]\n",
      "4081 [D loss: 0.123069, acc.: 96.88%] [G loss: 3.017258]\n",
      "4082 [D loss: 0.120367, acc.: 96.88%] [G loss: 3.007012]\n",
      "4083 [D loss: 0.121875, acc.: 96.88%] [G loss: 2.921001]\n",
      "4084 [D loss: 0.130221, acc.: 96.88%] [G loss: 2.990963]\n",
      "4085 [D loss: 0.130101, acc.: 96.88%] [G loss: 3.000242]\n",
      "4086 [D loss: 0.117763, acc.: 96.88%] [G loss: 3.149832]\n",
      "4087 [D loss: 0.114901, acc.: 96.88%] [G loss: 3.047363]\n",
      "4088 [D loss: 0.113398, acc.: 96.88%] [G loss: 3.050297]\n",
      "4089 [D loss: 0.116801, acc.: 96.88%] [G loss: 2.989128]\n",
      "4090 [D loss: 0.121293, acc.: 96.88%] [G loss: 2.923957]\n",
      "4091 [D loss: 0.121602, acc.: 96.88%] [G loss: 2.879088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4092 [D loss: 0.122263, acc.: 96.88%] [G loss: 3.049570]\n",
      "4093 [D loss: 0.116678, acc.: 96.88%] [G loss: 3.053209]\n",
      "4094 [D loss: 0.122494, acc.: 96.88%] [G loss: 2.947493]\n",
      "4095 [D loss: 0.122571, acc.: 96.88%] [G loss: 2.873970]\n",
      "4096 [D loss: 0.121634, acc.: 96.88%] [G loss: 2.875297]\n",
      "4097 [D loss: 0.125611, acc.: 96.88%] [G loss: 3.128578]\n",
      "4098 [D loss: 0.122022, acc.: 96.88%] [G loss: 3.077726]\n",
      "4099 [D loss: 0.114514, acc.: 96.88%] [G loss: 3.012667]\n",
      "4100 [D loss: 0.117611, acc.: 96.88%] [G loss: 3.003591]\n",
      "generated_data\n",
      "4101 [D loss: 0.122872, acc.: 96.88%] [G loss: 2.928720]\n",
      "4102 [D loss: 0.121473, acc.: 96.88%] [G loss: 2.868452]\n",
      "4103 [D loss: 0.121003, acc.: 96.88%] [G loss: 2.954977]\n",
      "4104 [D loss: 0.129016, acc.: 96.88%] [G loss: 2.946628]\n",
      "4105 [D loss: 0.124586, acc.: 96.88%] [G loss: 3.004949]\n",
      "4106 [D loss: 0.117744, acc.: 96.88%] [G loss: 2.933190]\n",
      "4107 [D loss: 0.113720, acc.: 96.88%] [G loss: 2.877656]\n",
      "4108 [D loss: 0.119716, acc.: 96.88%] [G loss: 2.910838]\n",
      "4109 [D loss: 0.119537, acc.: 96.88%] [G loss: 2.984244]\n",
      "4110 [D loss: 0.119086, acc.: 96.88%] [G loss: 2.890029]\n",
      "4111 [D loss: 0.126283, acc.: 96.88%] [G loss: 2.864935]\n",
      "4112 [D loss: 0.116144, acc.: 96.88%] [G loss: 2.854197]\n",
      "4113 [D loss: 0.116601, acc.: 96.88%] [G loss: 2.843935]\n",
      "4114 [D loss: 0.120493, acc.: 96.88%] [G loss: 2.884583]\n",
      "4115 [D loss: 0.114243, acc.: 96.88%] [G loss: 2.889898]\n",
      "4116 [D loss: 0.121897, acc.: 96.88%] [G loss: 2.842694]\n",
      "4117 [D loss: 0.119474, acc.: 96.88%] [G loss: 2.982765]\n",
      "4118 [D loss: 0.120757, acc.: 96.88%] [G loss: 2.853125]\n",
      "4119 [D loss: 0.123698, acc.: 96.88%] [G loss: 2.920095]\n",
      "4120 [D loss: 0.118269, acc.: 96.88%] [G loss: 3.002084]\n",
      "4121 [D loss: 0.119414, acc.: 96.88%] [G loss: 3.036389]\n",
      "4122 [D loss: 0.118459, acc.: 96.88%] [G loss: 3.031240]\n",
      "4123 [D loss: 0.121517, acc.: 96.88%] [G loss: 2.856428]\n",
      "4124 [D loss: 0.127054, acc.: 96.88%] [G loss: 2.926882]\n",
      "4125 [D loss: 0.123166, acc.: 96.88%] [G loss: 2.862790]\n",
      "4126 [D loss: 0.121695, acc.: 96.88%] [G loss: 2.944227]\n",
      "4127 [D loss: 0.118224, acc.: 96.88%] [G loss: 2.802632]\n",
      "4128 [D loss: 0.119811, acc.: 96.88%] [G loss: 2.953323]\n",
      "4129 [D loss: 0.120148, acc.: 96.88%] [G loss: 2.971300]\n",
      "4130 [D loss: 0.122858, acc.: 96.88%] [G loss: 2.962963]\n",
      "4131 [D loss: 0.119508, acc.: 96.88%] [G loss: 2.871720]\n",
      "4132 [D loss: 0.115557, acc.: 96.88%] [G loss: 3.091231]\n",
      "4133 [D loss: 0.124554, acc.: 96.88%] [G loss: 2.966781]\n",
      "4134 [D loss: 0.120119, acc.: 96.88%] [G loss: 2.900327]\n",
      "4135 [D loss: 0.119894, acc.: 96.88%] [G loss: 2.967053]\n",
      "4136 [D loss: 0.120706, acc.: 96.88%] [G loss: 2.974848]\n",
      "4137 [D loss: 0.121272, acc.: 96.88%] [G loss: 2.962571]\n",
      "4138 [D loss: 0.123344, acc.: 96.88%] [G loss: 3.109865]\n",
      "4139 [D loss: 0.122935, acc.: 96.88%] [G loss: 3.041096]\n",
      "4140 [D loss: 0.120331, acc.: 96.88%] [G loss: 2.995850]\n",
      "4141 [D loss: 0.134834, acc.: 96.88%] [G loss: 2.895079]\n",
      "4142 [D loss: 0.121015, acc.: 96.88%] [G loss: 2.808635]\n",
      "4143 [D loss: 0.124139, acc.: 96.88%] [G loss: 2.943221]\n",
      "4144 [D loss: 0.116274, acc.: 96.88%] [G loss: 2.975945]\n",
      "4145 [D loss: 0.120833, acc.: 96.88%] [G loss: 3.015569]\n",
      "4146 [D loss: 0.118697, acc.: 96.88%] [G loss: 2.935055]\n",
      "4147 [D loss: 0.120970, acc.: 96.88%] [G loss: 2.904696]\n",
      "4148 [D loss: 0.123861, acc.: 96.88%] [G loss: 2.962522]\n",
      "4149 [D loss: 0.122663, acc.: 96.88%] [G loss: 3.048715]\n",
      "4150 [D loss: 0.124093, acc.: 96.88%] [G loss: 2.891615]\n",
      "4151 [D loss: 0.112528, acc.: 96.88%] [G loss: 2.863071]\n",
      "4152 [D loss: 0.119214, acc.: 96.88%] [G loss: 3.008447]\n",
      "4153 [D loss: 0.118311, acc.: 96.88%] [G loss: 2.982906]\n",
      "4154 [D loss: 0.120968, acc.: 96.88%] [G loss: 2.968700]\n",
      "4155 [D loss: 0.130627, acc.: 96.88%] [G loss: 2.864638]\n",
      "4156 [D loss: 0.121495, acc.: 96.88%] [G loss: 2.830574]\n",
      "4157 [D loss: 0.128813, acc.: 96.88%] [G loss: 2.809159]\n",
      "4158 [D loss: 0.120674, acc.: 96.88%] [G loss: 2.831669]\n",
      "4159 [D loss: 0.128279, acc.: 96.88%] [G loss: 2.797405]\n",
      "4160 [D loss: 0.121899, acc.: 96.88%] [G loss: 2.830368]\n",
      "4161 [D loss: 0.121233, acc.: 96.88%] [G loss: 3.017392]\n",
      "4162 [D loss: 0.124973, acc.: 96.88%] [G loss: 2.878743]\n",
      "4163 [D loss: 0.118240, acc.: 96.88%] [G loss: 3.012033]\n",
      "4164 [D loss: 0.123189, acc.: 96.88%] [G loss: 2.933331]\n",
      "4165 [D loss: 0.118072, acc.: 96.88%] [G loss: 2.882699]\n",
      "4166 [D loss: 0.124709, acc.: 96.88%] [G loss: 2.924360]\n",
      "4167 [D loss: 0.124843, acc.: 96.88%] [G loss: 2.879850]\n",
      "4168 [D loss: 0.119997, acc.: 96.88%] [G loss: 2.884873]\n",
      "4169 [D loss: 0.124904, acc.: 96.88%] [G loss: 2.881307]\n",
      "4170 [D loss: 0.120100, acc.: 96.88%] [G loss: 2.854904]\n",
      "4171 [D loss: 0.119433, acc.: 96.88%] [G loss: 2.865832]\n",
      "4172 [D loss: 0.126495, acc.: 96.88%] [G loss: 2.916409]\n",
      "4173 [D loss: 0.120080, acc.: 96.88%] [G loss: 2.920196]\n",
      "4174 [D loss: 0.120014, acc.: 96.88%] [G loss: 3.006513]\n",
      "4175 [D loss: 0.110927, acc.: 96.88%] [G loss: 2.926878]\n",
      "4176 [D loss: 0.114526, acc.: 96.88%] [G loss: 2.937296]\n",
      "4177 [D loss: 0.125970, acc.: 96.88%] [G loss: 2.894011]\n",
      "4178 [D loss: 0.119659, acc.: 96.88%] [G loss: 3.000313]\n",
      "4179 [D loss: 0.112522, acc.: 96.88%] [G loss: 2.996754]\n",
      "4180 [D loss: 0.119816, acc.: 96.88%] [G loss: 3.033997]\n",
      "4181 [D loss: 0.119560, acc.: 96.88%] [G loss: 2.974768]\n",
      "4182 [D loss: 0.120388, acc.: 96.88%] [G loss: 3.041787]\n",
      "4183 [D loss: 0.115767, acc.: 96.88%] [G loss: 2.881806]\n",
      "4184 [D loss: 0.125565, acc.: 96.88%] [G loss: 2.927440]\n",
      "4185 [D loss: 0.118952, acc.: 96.88%] [G loss: 3.052970]\n",
      "4186 [D loss: 0.125046, acc.: 96.88%] [G loss: 2.886288]\n",
      "4187 [D loss: 0.115493, acc.: 96.88%] [G loss: 2.992654]\n",
      "4188 [D loss: 0.118757, acc.: 96.88%] [G loss: 2.992656]\n",
      "4189 [D loss: 0.127456, acc.: 96.88%] [G loss: 2.836747]\n",
      "4190 [D loss: 0.116373, acc.: 96.88%] [G loss: 2.940638]\n",
      "4191 [D loss: 0.122450, acc.: 96.88%] [G loss: 2.983130]\n",
      "4192 [D loss: 0.116305, acc.: 96.88%] [G loss: 2.919646]\n",
      "4193 [D loss: 0.117441, acc.: 96.88%] [G loss: 2.911628]\n",
      "4194 [D loss: 0.123944, acc.: 96.88%] [G loss: 2.931628]\n",
      "4195 [D loss: 0.118891, acc.: 96.88%] [G loss: 2.972126]\n",
      "4196 [D loss: 0.117177, acc.: 96.88%] [G loss: 2.928954]\n",
      "4197 [D loss: 0.117476, acc.: 96.88%] [G loss: 2.920870]\n",
      "4198 [D loss: 0.120372, acc.: 96.88%] [G loss: 2.812018]\n",
      "4199 [D loss: 0.121518, acc.: 96.88%] [G loss: 2.840292]\n",
      "4200 [D loss: 0.118283, acc.: 96.88%] [G loss: 2.946722]\n",
      "generated_data\n",
      "4201 [D loss: 0.121008, acc.: 96.88%] [G loss: 2.907126]\n",
      "4202 [D loss: 0.116476, acc.: 96.88%] [G loss: 3.039406]\n",
      "4203 [D loss: 0.125091, acc.: 96.88%] [G loss: 2.927023]\n",
      "4204 [D loss: 0.119523, acc.: 96.88%] [G loss: 2.967347]\n",
      "4205 [D loss: 0.119238, acc.: 96.88%] [G loss: 2.942658]\n",
      "4206 [D loss: 0.118206, acc.: 96.88%] [G loss: 2.966982]\n",
      "4207 [D loss: 0.118542, acc.: 96.88%] [G loss: 2.945501]\n",
      "4208 [D loss: 0.122279, acc.: 96.88%] [G loss: 2.962660]\n",
      "4209 [D loss: 0.112983, acc.: 96.88%] [G loss: 2.938667]\n",
      "4210 [D loss: 0.120421, acc.: 96.88%] [G loss: 2.950938]\n",
      "4211 [D loss: 0.126012, acc.: 96.88%] [G loss: 2.904899]\n",
      "4212 [D loss: 0.126539, acc.: 96.88%] [G loss: 2.866452]\n",
      "4213 [D loss: 0.124233, acc.: 96.88%] [G loss: 2.853815]\n",
      "4214 [D loss: 0.121453, acc.: 96.88%] [G loss: 2.845655]\n",
      "4215 [D loss: 0.119703, acc.: 96.88%] [G loss: 2.824519]\n",
      "4216 [D loss: 0.120820, acc.: 96.88%] [G loss: 2.883611]\n",
      "4217 [D loss: 0.117606, acc.: 96.88%] [G loss: 2.994352]\n",
      "4218 [D loss: 0.121020, acc.: 96.88%] [G loss: 2.967879]\n",
      "4219 [D loss: 0.125989, acc.: 96.88%] [G loss: 2.883370]\n",
      "4220 [D loss: 0.124290, acc.: 96.88%] [G loss: 2.856317]\n",
      "4221 [D loss: 0.119451, acc.: 96.88%] [G loss: 2.918555]\n",
      "4222 [D loss: 0.119974, acc.: 96.88%] [G loss: 2.906861]\n",
      "4223 [D loss: 0.119617, acc.: 96.88%] [G loss: 2.964664]\n",
      "4224 [D loss: 0.122575, acc.: 96.88%] [G loss: 2.900993]\n",
      "4225 [D loss: 0.120920, acc.: 96.88%] [G loss: 2.932637]\n",
      "4226 [D loss: 0.121558, acc.: 96.88%] [G loss: 2.950078]\n",
      "4227 [D loss: 0.120302, acc.: 96.88%] [G loss: 3.100783]\n",
      "4228 [D loss: 0.114845, acc.: 96.88%] [G loss: 3.044171]\n",
      "4229 [D loss: 0.115209, acc.: 96.88%] [G loss: 2.969769]\n",
      "4230 [D loss: 0.120503, acc.: 96.88%] [G loss: 2.957819]\n",
      "4231 [D loss: 0.119804, acc.: 96.88%] [G loss: 3.095265]\n",
      "4232 [D loss: 0.122402, acc.: 96.88%] [G loss: 2.972169]\n",
      "4233 [D loss: 0.119290, acc.: 96.88%] [G loss: 3.003201]\n",
      "4234 [D loss: 0.123196, acc.: 96.88%] [G loss: 2.894554]\n",
      "4235 [D loss: 0.115654, acc.: 96.88%] [G loss: 2.906310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4236 [D loss: 0.119741, acc.: 96.88%] [G loss: 2.864926]\n",
      "4237 [D loss: 0.123805, acc.: 96.88%] [G loss: 2.903719]\n",
      "4238 [D loss: 0.116297, acc.: 96.88%] [G loss: 2.921408]\n",
      "4239 [D loss: 0.118963, acc.: 96.88%] [G loss: 2.985203]\n",
      "4240 [D loss: 0.120026, acc.: 96.88%] [G loss: 2.875461]\n",
      "4241 [D loss: 0.119835, acc.: 96.88%] [G loss: 2.898467]\n",
      "4242 [D loss: 0.116632, acc.: 96.88%] [G loss: 2.930930]\n",
      "4243 [D loss: 0.118315, acc.: 96.88%] [G loss: 2.931674]\n",
      "4244 [D loss: 0.123035, acc.: 96.88%] [G loss: 2.861599]\n",
      "4245 [D loss: 0.126177, acc.: 96.88%] [G loss: 2.845488]\n",
      "4246 [D loss: 0.118823, acc.: 96.88%] [G loss: 2.878024]\n",
      "4247 [D loss: 0.115735, acc.: 96.88%] [G loss: 2.872703]\n",
      "4248 [D loss: 0.120038, acc.: 96.88%] [G loss: 2.977700]\n",
      "4249 [D loss: 0.111515, acc.: 96.88%] [G loss: 2.996952]\n",
      "4250 [D loss: 0.123760, acc.: 96.88%] [G loss: 3.007753]\n",
      "4251 [D loss: 0.123622, acc.: 96.88%] [G loss: 3.053039]\n",
      "4252 [D loss: 0.116985, acc.: 96.88%] [G loss: 2.921427]\n",
      "4253 [D loss: 0.124670, acc.: 96.88%] [G loss: 2.815568]\n",
      "4254 [D loss: 0.121839, acc.: 96.88%] [G loss: 2.941645]\n",
      "4255 [D loss: 0.123512, acc.: 96.88%] [G loss: 3.115843]\n",
      "4256 [D loss: 0.124429, acc.: 96.88%] [G loss: 2.943645]\n",
      "4257 [D loss: 0.124220, acc.: 96.88%] [G loss: 2.857525]\n",
      "4258 [D loss: 0.121399, acc.: 96.88%] [G loss: 2.944093]\n",
      "4259 [D loss: 0.120759, acc.: 96.88%] [G loss: 2.929348]\n",
      "4260 [D loss: 0.117065, acc.: 96.88%] [G loss: 2.978772]\n",
      "4261 [D loss: 0.118999, acc.: 96.88%] [G loss: 2.939091]\n",
      "4262 [D loss: 0.119054, acc.: 96.88%] [G loss: 2.936040]\n",
      "4263 [D loss: 0.116736, acc.: 96.88%] [G loss: 3.130006]\n",
      "4264 [D loss: 0.127123, acc.: 96.88%] [G loss: 2.848403]\n",
      "4265 [D loss: 0.118144, acc.: 96.88%] [G loss: 2.963150]\n",
      "4266 [D loss: 0.122211, acc.: 96.88%] [G loss: 2.990716]\n",
      "4267 [D loss: 0.121397, acc.: 96.88%] [G loss: 2.972347]\n",
      "4268 [D loss: 0.116932, acc.: 96.88%] [G loss: 2.948389]\n",
      "4269 [D loss: 0.110793, acc.: 96.88%] [G loss: 2.940819]\n",
      "4270 [D loss: 0.121167, acc.: 96.88%] [G loss: 2.825900]\n",
      "4271 [D loss: 0.121479, acc.: 96.88%] [G loss: 2.878263]\n",
      "4272 [D loss: 0.122457, acc.: 96.88%] [G loss: 2.923016]\n",
      "4273 [D loss: 0.124369, acc.: 96.88%] [G loss: 2.806073]\n",
      "4274 [D loss: 0.115581, acc.: 96.88%] [G loss: 3.002004]\n",
      "4275 [D loss: 0.122873, acc.: 96.88%] [G loss: 2.882095]\n",
      "4276 [D loss: 0.121005, acc.: 96.88%] [G loss: 2.986723]\n",
      "4277 [D loss: 0.120353, acc.: 96.88%] [G loss: 2.984241]\n",
      "4278 [D loss: 0.115136, acc.: 96.88%] [G loss: 2.854420]\n",
      "4279 [D loss: 0.114936, acc.: 96.88%] [G loss: 3.026793]\n",
      "4280 [D loss: 0.124775, acc.: 96.88%] [G loss: 3.060853]\n",
      "4281 [D loss: 0.123359, acc.: 96.88%] [G loss: 2.927567]\n",
      "4282 [D loss: 0.122031, acc.: 96.88%] [G loss: 2.935808]\n",
      "4283 [D loss: 0.117107, acc.: 96.88%] [G loss: 2.886485]\n",
      "4284 [D loss: 0.122330, acc.: 96.88%] [G loss: 2.918449]\n",
      "4285 [D loss: 0.121130, acc.: 96.88%] [G loss: 2.943385]\n",
      "4286 [D loss: 0.119619, acc.: 96.88%] [G loss: 3.025045]\n",
      "4287 [D loss: 0.117744, acc.: 96.88%] [G loss: 3.057582]\n",
      "4288 [D loss: 0.112091, acc.: 96.88%] [G loss: 3.079386]\n",
      "4289 [D loss: 0.118900, acc.: 96.88%] [G loss: 2.975607]\n",
      "4290 [D loss: 0.126655, acc.: 96.88%] [G loss: 2.758611]\n",
      "4291 [D loss: 0.118833, acc.: 96.88%] [G loss: 2.861605]\n",
      "4292 [D loss: 0.122123, acc.: 96.88%] [G loss: 2.872788]\n",
      "4293 [D loss: 0.118274, acc.: 96.88%] [G loss: 3.029482]\n",
      "4294 [D loss: 0.119716, acc.: 96.88%] [G loss: 2.960243]\n",
      "4295 [D loss: 0.118462, acc.: 96.88%] [G loss: 2.901358]\n",
      "4296 [D loss: 0.123238, acc.: 96.88%] [G loss: 2.936601]\n",
      "4297 [D loss: 0.117320, acc.: 96.88%] [G loss: 2.963715]\n",
      "4298 [D loss: 0.122461, acc.: 96.88%] [G loss: 2.993093]\n",
      "4299 [D loss: 0.114399, acc.: 96.88%] [G loss: 3.005420]\n",
      "4300 [D loss: 0.122884, acc.: 96.88%] [G loss: 2.975712]\n",
      "generated_data\n",
      "4301 [D loss: 0.113979, acc.: 96.88%] [G loss: 3.000014]\n",
      "4302 [D loss: 0.120259, acc.: 96.88%] [G loss: 2.953654]\n",
      "4303 [D loss: 0.121702, acc.: 96.88%] [G loss: 2.928369]\n",
      "4304 [D loss: 0.119604, acc.: 96.88%] [G loss: 3.111638]\n",
      "4305 [D loss: 0.121205, acc.: 96.88%] [G loss: 3.006440]\n",
      "4306 [D loss: 0.121837, acc.: 96.88%] [G loss: 3.057569]\n",
      "4307 [D loss: 0.118685, acc.: 96.88%] [G loss: 2.933607]\n",
      "4308 [D loss: 0.122543, acc.: 96.88%] [G loss: 2.996732]\n",
      "4309 [D loss: 0.124384, acc.: 96.88%] [G loss: 2.988785]\n",
      "4310 [D loss: 0.116044, acc.: 96.88%] [G loss: 2.894760]\n",
      "4311 [D loss: 0.126773, acc.: 96.88%] [G loss: 2.928932]\n",
      "4312 [D loss: 0.119373, acc.: 96.88%] [G loss: 2.966652]\n",
      "4313 [D loss: 0.122268, acc.: 96.88%] [G loss: 2.885439]\n",
      "4314 [D loss: 0.121970, acc.: 96.88%] [G loss: 2.937801]\n",
      "4315 [D loss: 0.118173, acc.: 96.88%] [G loss: 2.902271]\n",
      "4316 [D loss: 0.123753, acc.: 96.88%] [G loss: 2.947810]\n",
      "4317 [D loss: 0.114202, acc.: 96.88%] [G loss: 3.041549]\n",
      "4318 [D loss: 0.121618, acc.: 96.88%] [G loss: 2.978974]\n",
      "4319 [D loss: 0.119246, acc.: 96.88%] [G loss: 3.042569]\n",
      "4320 [D loss: 0.123849, acc.: 96.88%] [G loss: 2.864439]\n",
      "4321 [D loss: 0.121006, acc.: 96.88%] [G loss: 2.946846]\n",
      "4322 [D loss: 0.128727, acc.: 96.88%] [G loss: 2.986653]\n",
      "4323 [D loss: 0.116396, acc.: 96.88%] [G loss: 2.830697]\n",
      "4324 [D loss: 0.116868, acc.: 96.88%] [G loss: 2.930118]\n",
      "4325 [D loss: 0.125178, acc.: 96.88%] [G loss: 2.869207]\n",
      "4326 [D loss: 0.118856, acc.: 96.88%] [G loss: 2.992971]\n",
      "4327 [D loss: 0.125034, acc.: 96.88%] [G loss: 2.873010]\n",
      "4328 [D loss: 0.122896, acc.: 96.88%] [G loss: 2.866140]\n",
      "4329 [D loss: 0.125451, acc.: 96.88%] [G loss: 2.889045]\n",
      "4330 [D loss: 0.117253, acc.: 96.88%] [G loss: 2.918258]\n",
      "4331 [D loss: 0.126057, acc.: 96.88%] [G loss: 2.857508]\n",
      "4332 [D loss: 0.121332, acc.: 96.88%] [G loss: 2.873130]\n",
      "4333 [D loss: 0.118693, acc.: 96.88%] [G loss: 2.912630]\n",
      "4334 [D loss: 0.119861, acc.: 96.88%] [G loss: 2.882675]\n",
      "4335 [D loss: 0.121658, acc.: 96.88%] [G loss: 2.931367]\n",
      "4336 [D loss: 0.114008, acc.: 96.88%] [G loss: 2.935143]\n",
      "4337 [D loss: 0.120450, acc.: 96.88%] [G loss: 2.949188]\n",
      "4338 [D loss: 0.120843, acc.: 96.88%] [G loss: 2.900289]\n",
      "4339 [D loss: 0.118248, acc.: 96.88%] [G loss: 2.881055]\n",
      "4340 [D loss: 0.117535, acc.: 96.88%] [G loss: 2.953605]\n",
      "4341 [D loss: 0.123674, acc.: 96.88%] [G loss: 2.973656]\n",
      "4342 [D loss: 0.118265, acc.: 96.88%] [G loss: 2.941386]\n",
      "4343 [D loss: 0.122972, acc.: 96.88%] [G loss: 3.030692]\n",
      "4344 [D loss: 0.124250, acc.: 96.88%] [G loss: 2.837539]\n",
      "4345 [D loss: 0.124673, acc.: 96.88%] [G loss: 2.953720]\n",
      "4346 [D loss: 0.126458, acc.: 96.88%] [G loss: 2.873981]\n",
      "4347 [D loss: 0.124305, acc.: 96.88%] [G loss: 2.794475]\n",
      "4348 [D loss: 0.122601, acc.: 96.88%] [G loss: 2.868703]\n",
      "4349 [D loss: 0.116280, acc.: 96.88%] [G loss: 2.957085]\n",
      "4350 [D loss: 0.118285, acc.: 96.88%] [G loss: 2.874668]\n",
      "4351 [D loss: 0.124284, acc.: 96.88%] [G loss: 2.983067]\n",
      "4352 [D loss: 0.122154, acc.: 96.88%] [G loss: 3.002512]\n",
      "4353 [D loss: 0.125740, acc.: 96.88%] [G loss: 2.877193]\n",
      "4354 [D loss: 0.122613, acc.: 96.88%] [G loss: 2.941273]\n",
      "4355 [D loss: 0.114507, acc.: 96.88%] [G loss: 2.945729]\n",
      "4356 [D loss: 0.123842, acc.: 96.88%] [G loss: 2.942773]\n",
      "4357 [D loss: 0.127395, acc.: 96.88%] [G loss: 2.948462]\n",
      "4358 [D loss: 0.122188, acc.: 96.88%] [G loss: 2.850449]\n",
      "4359 [D loss: 0.118456, acc.: 96.88%] [G loss: 3.018234]\n",
      "4360 [D loss: 0.118465, acc.: 96.88%] [G loss: 2.905533]\n",
      "4361 [D loss: 0.122027, acc.: 96.88%] [G loss: 3.039444]\n",
      "4362 [D loss: 0.127782, acc.: 96.88%] [G loss: 2.934183]\n",
      "4363 [D loss: 0.116443, acc.: 96.88%] [G loss: 2.883122]\n",
      "4364 [D loss: 0.118959, acc.: 96.88%] [G loss: 2.879441]\n",
      "4365 [D loss: 0.121734, acc.: 96.88%] [G loss: 2.904164]\n",
      "4366 [D loss: 0.122621, acc.: 96.88%] [G loss: 2.961092]\n",
      "4367 [D loss: 0.122453, acc.: 96.88%] [G loss: 2.879787]\n",
      "4368 [D loss: 0.120252, acc.: 96.88%] [G loss: 2.859054]\n",
      "4369 [D loss: 0.117214, acc.: 96.88%] [G loss: 2.841716]\n",
      "4370 [D loss: 0.122765, acc.: 96.88%] [G loss: 2.920303]\n",
      "4371 [D loss: 0.124914, acc.: 96.88%] [G loss: 2.878519]\n",
      "4372 [D loss: 0.123356, acc.: 96.88%] [G loss: 2.894449]\n",
      "4373 [D loss: 0.124926, acc.: 96.88%] [G loss: 2.913318]\n",
      "4374 [D loss: 0.121309, acc.: 96.88%] [G loss: 2.988898]\n",
      "4375 [D loss: 0.126484, acc.: 96.88%] [G loss: 2.944292]\n",
      "4376 [D loss: 0.117385, acc.: 96.88%] [G loss: 2.863044]\n",
      "4377 [D loss: 0.126653, acc.: 96.88%] [G loss: 2.977262]\n",
      "4378 [D loss: 0.119315, acc.: 96.88%] [G loss: 2.914536]\n",
      "4379 [D loss: 0.117631, acc.: 96.88%] [G loss: 2.989440]\n",
      "4380 [D loss: 0.115976, acc.: 96.88%] [G loss: 2.911144]\n",
      "4381 [D loss: 0.121297, acc.: 96.88%] [G loss: 2.842530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4382 [D loss: 0.116449, acc.: 96.88%] [G loss: 2.946000]\n",
      "4383 [D loss: 0.119003, acc.: 96.88%] [G loss: 2.941424]\n",
      "4384 [D loss: 0.120319, acc.: 96.88%] [G loss: 2.876752]\n",
      "4385 [D loss: 0.121118, acc.: 96.88%] [G loss: 2.879665]\n",
      "4386 [D loss: 0.121706, acc.: 96.88%] [G loss: 2.847134]\n",
      "4387 [D loss: 0.119025, acc.: 96.88%] [G loss: 2.805512]\n",
      "4388 [D loss: 0.120104, acc.: 96.88%] [G loss: 2.881227]\n",
      "4389 [D loss: 0.125135, acc.: 96.88%] [G loss: 2.837170]\n",
      "4390 [D loss: 0.113240, acc.: 96.88%] [G loss: 2.877494]\n",
      "4391 [D loss: 0.117958, acc.: 96.88%] [G loss: 2.906402]\n",
      "4392 [D loss: 0.126528, acc.: 96.88%] [G loss: 2.994648]\n",
      "4393 [D loss: 0.119604, acc.: 96.88%] [G loss: 2.874727]\n",
      "4394 [D loss: 0.118589, acc.: 96.88%] [G loss: 2.940772]\n",
      "4395 [D loss: 0.124085, acc.: 96.88%] [G loss: 2.911188]\n",
      "4396 [D loss: 0.117908, acc.: 96.88%] [G loss: 2.928689]\n",
      "4397 [D loss: 0.125440, acc.: 96.88%] [G loss: 2.884525]\n",
      "4398 [D loss: 0.120536, acc.: 96.88%] [G loss: 2.890136]\n",
      "4399 [D loss: 0.115260, acc.: 96.88%] [G loss: 2.933076]\n",
      "4400 [D loss: 0.117542, acc.: 96.88%] [G loss: 2.932153]\n",
      "generated_data\n",
      "4401 [D loss: 0.120917, acc.: 96.88%] [G loss: 2.872720]\n",
      "4402 [D loss: 0.123654, acc.: 96.88%] [G loss: 2.974534]\n",
      "4403 [D loss: 0.123625, acc.: 96.88%] [G loss: 2.854294]\n",
      "4404 [D loss: 0.124054, acc.: 96.88%] [G loss: 2.922285]\n",
      "4405 [D loss: 0.123549, acc.: 96.88%] [G loss: 2.847729]\n",
      "4406 [D loss: 0.121031, acc.: 96.88%] [G loss: 2.872737]\n",
      "4407 [D loss: 0.118939, acc.: 96.88%] [G loss: 2.901205]\n",
      "4408 [D loss: 0.119349, acc.: 96.88%] [G loss: 2.937366]\n",
      "4409 [D loss: 0.114053, acc.: 96.88%] [G loss: 3.012790]\n",
      "4410 [D loss: 0.124032, acc.: 96.88%] [G loss: 2.895081]\n",
      "4411 [D loss: 0.113139, acc.: 96.88%] [G loss: 2.917984]\n",
      "4412 [D loss: 0.115306, acc.: 96.88%] [G loss: 2.946391]\n",
      "4413 [D loss: 0.121542, acc.: 96.88%] [G loss: 2.897783]\n",
      "4414 [D loss: 0.120929, acc.: 96.88%] [G loss: 2.990228]\n",
      "4415 [D loss: 0.120129, acc.: 96.88%] [G loss: 2.908591]\n",
      "4416 [D loss: 0.116684, acc.: 96.88%] [G loss: 2.892551]\n",
      "4417 [D loss: 0.120244, acc.: 96.88%] [G loss: 3.012828]\n",
      "4418 [D loss: 0.127143, acc.: 96.88%] [G loss: 2.871106]\n",
      "4419 [D loss: 0.125528, acc.: 96.88%] [G loss: 2.987533]\n",
      "4420 [D loss: 0.126777, acc.: 96.88%] [G loss: 2.802373]\n",
      "4421 [D loss: 0.116902, acc.: 96.88%] [G loss: 2.867584]\n",
      "4422 [D loss: 0.118505, acc.: 96.88%] [G loss: 2.938642]\n",
      "4423 [D loss: 0.117409, acc.: 96.88%] [G loss: 2.905516]\n",
      "4424 [D loss: 0.121114, acc.: 96.88%] [G loss: 2.920986]\n",
      "4425 [D loss: 0.113654, acc.: 96.88%] [G loss: 2.901287]\n",
      "4426 [D loss: 0.116347, acc.: 96.88%] [G loss: 2.966664]\n",
      "4427 [D loss: 0.118686, acc.: 96.88%] [G loss: 2.966189]\n",
      "4428 [D loss: 0.119936, acc.: 96.88%] [G loss: 2.906785]\n",
      "4429 [D loss: 0.121532, acc.: 96.88%] [G loss: 3.056902]\n",
      "4430 [D loss: 0.119206, acc.: 96.88%] [G loss: 2.901076]\n",
      "4431 [D loss: 0.118176, acc.: 96.88%] [G loss: 2.964553]\n",
      "4432 [D loss: 0.118604, acc.: 96.88%] [G loss: 2.944939]\n",
      "4433 [D loss: 0.120845, acc.: 96.88%] [G loss: 2.981243]\n",
      "4434 [D loss: 0.123842, acc.: 96.88%] [G loss: 2.914782]\n",
      "4435 [D loss: 0.126103, acc.: 96.88%] [G loss: 2.881902]\n",
      "4436 [D loss: 0.121614, acc.: 96.88%] [G loss: 2.838307]\n",
      "4437 [D loss: 0.124560, acc.: 96.88%] [G loss: 3.014344]\n",
      "4438 [D loss: 0.120772, acc.: 96.88%] [G loss: 2.989153]\n",
      "4439 [D loss: 0.114848, acc.: 96.88%] [G loss: 2.962663]\n",
      "4440 [D loss: 0.119465, acc.: 96.88%] [G loss: 2.913354]\n",
      "4441 [D loss: 0.118441, acc.: 96.88%] [G loss: 3.028472]\n",
      "4442 [D loss: 0.118590, acc.: 96.88%] [G loss: 2.976273]\n",
      "4443 [D loss: 0.126227, acc.: 96.88%] [G loss: 2.869370]\n",
      "4444 [D loss: 0.125460, acc.: 96.88%] [G loss: 2.848430]\n",
      "4445 [D loss: 0.119602, acc.: 96.88%] [G loss: 2.883317]\n",
      "4446 [D loss: 0.120832, acc.: 96.88%] [G loss: 2.887204]\n",
      "4447 [D loss: 0.123260, acc.: 96.88%] [G loss: 3.011609]\n",
      "4448 [D loss: 0.119821, acc.: 96.88%] [G loss: 2.878716]\n",
      "4449 [D loss: 0.120235, acc.: 96.88%] [G loss: 2.876027]\n",
      "4450 [D loss: 0.114522, acc.: 96.88%] [G loss: 2.978478]\n",
      "4451 [D loss: 0.120136, acc.: 96.88%] [G loss: 2.926075]\n",
      "4452 [D loss: 0.126208, acc.: 96.88%] [G loss: 2.868502]\n",
      "4453 [D loss: 0.121094, acc.: 96.88%] [G loss: 2.883536]\n",
      "4454 [D loss: 0.121954, acc.: 96.88%] [G loss: 3.008009]\n",
      "4455 [D loss: 0.122623, acc.: 96.88%] [G loss: 2.902257]\n",
      "4456 [D loss: 0.121308, acc.: 96.88%] [G loss: 2.931084]\n",
      "4457 [D loss: 0.119001, acc.: 96.88%] [G loss: 2.849874]\n",
      "4458 [D loss: 0.117927, acc.: 96.88%] [G loss: 2.954551]\n",
      "4459 [D loss: 0.121283, acc.: 96.88%] [G loss: 2.881987]\n",
      "4460 [D loss: 0.114068, acc.: 96.88%] [G loss: 2.959530]\n",
      "4461 [D loss: 0.119607, acc.: 96.88%] [G loss: 2.889673]\n",
      "4462 [D loss: 0.123068, acc.: 96.88%] [G loss: 3.003540]\n",
      "4463 [D loss: 0.123125, acc.: 96.88%] [G loss: 2.919999]\n",
      "4464 [D loss: 0.119224, acc.: 96.88%] [G loss: 2.924755]\n",
      "4465 [D loss: 0.118751, acc.: 96.88%] [G loss: 2.995813]\n",
      "4466 [D loss: 0.126297, acc.: 96.88%] [G loss: 3.046002]\n",
      "4467 [D loss: 0.120738, acc.: 96.88%] [G loss: 2.943401]\n",
      "4468 [D loss: 0.118623, acc.: 96.88%] [G loss: 2.938897]\n",
      "4469 [D loss: 0.116732, acc.: 96.88%] [G loss: 3.002203]\n",
      "4470 [D loss: 0.126357, acc.: 96.88%] [G loss: 2.939778]\n",
      "4471 [D loss: 0.123232, acc.: 96.88%] [G loss: 2.878584]\n",
      "4472 [D loss: 0.120707, acc.: 96.88%] [G loss: 2.884443]\n",
      "4473 [D loss: 0.127322, acc.: 96.88%] [G loss: 2.942432]\n",
      "4474 [D loss: 0.123054, acc.: 96.88%] [G loss: 2.871921]\n",
      "4475 [D loss: 0.113291, acc.: 96.88%] [G loss: 2.911358]\n",
      "4476 [D loss: 0.124054, acc.: 96.88%] [G loss: 3.009622]\n",
      "4477 [D loss: 0.125302, acc.: 96.88%] [G loss: 2.903867]\n",
      "4478 [D loss: 0.120595, acc.: 96.88%] [G loss: 2.980539]\n",
      "4479 [D loss: 0.117687, acc.: 96.88%] [G loss: 2.862486]\n",
      "4480 [D loss: 0.123953, acc.: 96.88%] [G loss: 2.889295]\n",
      "4481 [D loss: 0.112646, acc.: 96.88%] [G loss: 2.914641]\n",
      "4482 [D loss: 0.122837, acc.: 96.88%] [G loss: 2.975511]\n",
      "4483 [D loss: 0.125922, acc.: 96.88%] [G loss: 2.810339]\n",
      "4484 [D loss: 0.123477, acc.: 96.88%] [G loss: 2.855604]\n",
      "4485 [D loss: 0.122167, acc.: 96.88%] [G loss: 2.974573]\n",
      "4486 [D loss: 0.125054, acc.: 96.88%] [G loss: 2.973190]\n",
      "4487 [D loss: 0.120036, acc.: 96.88%] [G loss: 2.992799]\n",
      "4488 [D loss: 0.121342, acc.: 96.88%] [G loss: 2.916063]\n",
      "4489 [D loss: 0.114886, acc.: 96.88%] [G loss: 2.937085]\n",
      "4490 [D loss: 0.129145, acc.: 96.88%] [G loss: 2.933178]\n",
      "4491 [D loss: 0.121461, acc.: 96.88%] [G loss: 2.800049]\n",
      "4492 [D loss: 0.119500, acc.: 96.88%] [G loss: 2.872636]\n",
      "4493 [D loss: 0.114617, acc.: 96.88%] [G loss: 2.957856]\n",
      "4494 [D loss: 0.125533, acc.: 96.88%] [G loss: 3.097483]\n",
      "4495 [D loss: 0.123142, acc.: 96.88%] [G loss: 2.952765]\n",
      "4496 [D loss: 0.120998, acc.: 96.88%] [G loss: 2.936204]\n",
      "4497 [D loss: 0.123503, acc.: 96.88%] [G loss: 3.104167]\n",
      "4498 [D loss: 0.122282, acc.: 96.88%] [G loss: 2.959197]\n",
      "4499 [D loss: 0.120298, acc.: 96.88%] [G loss: 3.033807]\n",
      "4500 [D loss: 0.123116, acc.: 96.88%] [G loss: 2.841774]\n",
      "generated_data\n",
      "4501 [D loss: 0.119030, acc.: 96.88%] [G loss: 2.965698]\n",
      "4502 [D loss: 0.116095, acc.: 96.88%] [G loss: 2.869093]\n",
      "4503 [D loss: 0.118055, acc.: 96.88%] [G loss: 2.930892]\n",
      "4504 [D loss: 0.120754, acc.: 96.88%] [G loss: 2.974460]\n",
      "4505 [D loss: 0.125162, acc.: 96.88%] [G loss: 2.893782]\n",
      "4506 [D loss: 0.117594, acc.: 96.88%] [G loss: 2.947117]\n",
      "4507 [D loss: 0.120937, acc.: 96.88%] [G loss: 2.927539]\n",
      "4508 [D loss: 0.120216, acc.: 96.88%] [G loss: 2.972555]\n",
      "4509 [D loss: 0.121714, acc.: 96.88%] [G loss: 3.004531]\n",
      "4510 [D loss: 0.120200, acc.: 96.88%] [G loss: 2.957191]\n",
      "4511 [D loss: 0.116069, acc.: 96.88%] [G loss: 3.149391]\n",
      "4512 [D loss: 0.126883, acc.: 96.88%] [G loss: 2.916577]\n",
      "4513 [D loss: 0.120216, acc.: 96.88%] [G loss: 2.914347]\n",
      "4514 [D loss: 0.123309, acc.: 96.88%] [G loss: 2.872322]\n",
      "4515 [D loss: 0.122642, acc.: 96.88%] [G loss: 2.939569]\n",
      "4516 [D loss: 0.120439, acc.: 96.88%] [G loss: 2.930734]\n",
      "4517 [D loss: 0.126932, acc.: 96.88%] [G loss: 2.909751]\n",
      "4518 [D loss: 0.121353, acc.: 96.88%] [G loss: 2.819085]\n",
      "4519 [D loss: 0.122910, acc.: 96.88%] [G loss: 3.023681]\n",
      "4520 [D loss: 0.116111, acc.: 96.88%] [G loss: 2.985321]\n",
      "4521 [D loss: 0.119937, acc.: 96.88%] [G loss: 3.058129]\n",
      "4522 [D loss: 0.119991, acc.: 96.88%] [G loss: 2.862605]\n",
      "4523 [D loss: 0.124351, acc.: 96.88%] [G loss: 2.853056]\n",
      "4524 [D loss: 0.121379, acc.: 96.88%] [G loss: 2.951869]\n",
      "4525 [D loss: 0.119560, acc.: 96.88%] [G loss: 2.949850]\n",
      "4526 [D loss: 0.120226, acc.: 96.88%] [G loss: 2.856612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4527 [D loss: 0.122893, acc.: 96.88%] [G loss: 2.871978]\n",
      "4528 [D loss: 0.116915, acc.: 96.88%] [G loss: 2.860055]\n",
      "4529 [D loss: 0.121688, acc.: 96.88%] [G loss: 2.861772]\n",
      "4530 [D loss: 0.122532, acc.: 96.88%] [G loss: 2.849386]\n",
      "4531 [D loss: 0.121249, acc.: 96.88%] [G loss: 2.863306]\n",
      "4532 [D loss: 0.126082, acc.: 96.88%] [G loss: 2.905951]\n",
      "4533 [D loss: 0.128508, acc.: 96.88%] [G loss: 2.881195]\n",
      "4534 [D loss: 0.120995, acc.: 96.88%] [G loss: 2.821455]\n",
      "4535 [D loss: 0.120398, acc.: 96.88%] [G loss: 2.933319]\n",
      "4536 [D loss: 0.122680, acc.: 96.88%] [G loss: 2.834874]\n",
      "4537 [D loss: 0.122368, acc.: 96.88%] [G loss: 2.818716]\n",
      "4538 [D loss: 0.118036, acc.: 96.88%] [G loss: 2.880285]\n",
      "4539 [D loss: 0.120568, acc.: 96.88%] [G loss: 2.940652]\n",
      "4540 [D loss: 0.117404, acc.: 96.88%] [G loss: 2.930257]\n",
      "4541 [D loss: 0.115473, acc.: 96.88%] [G loss: 2.945211]\n",
      "4542 [D loss: 0.117929, acc.: 96.88%] [G loss: 2.909750]\n",
      "4543 [D loss: 0.121212, acc.: 96.88%] [G loss: 2.847821]\n",
      "4544 [D loss: 0.118400, acc.: 96.88%] [G loss: 2.890021]\n",
      "4545 [D loss: 0.118092, acc.: 96.88%] [G loss: 3.110969]\n",
      "4546 [D loss: 0.118462, acc.: 96.88%] [G loss: 3.010369]\n",
      "4547 [D loss: 0.121215, acc.: 96.88%] [G loss: 2.933940]\n",
      "4548 [D loss: 0.114196, acc.: 96.88%] [G loss: 2.912392]\n",
      "4549 [D loss: 0.116622, acc.: 96.88%] [G loss: 2.860201]\n",
      "4550 [D loss: 0.121758, acc.: 96.88%] [G loss: 2.977587]\n",
      "4551 [D loss: 0.118912, acc.: 96.88%] [G loss: 2.866267]\n",
      "4552 [D loss: 0.124243, acc.: 96.88%] [G loss: 2.827626]\n",
      "4553 [D loss: 0.120268, acc.: 96.88%] [G loss: 2.862256]\n",
      "4554 [D loss: 0.125419, acc.: 96.88%] [G loss: 2.880272]\n",
      "4555 [D loss: 0.119962, acc.: 96.88%] [G loss: 2.925021]\n",
      "4556 [D loss: 0.119673, acc.: 96.88%] [G loss: 3.016396]\n",
      "4557 [D loss: 0.123073, acc.: 96.88%] [G loss: 3.004990]\n",
      "4558 [D loss: 0.122922, acc.: 96.88%] [G loss: 2.871588]\n",
      "4559 [D loss: 0.124208, acc.: 96.88%] [G loss: 2.804024]\n",
      "4560 [D loss: 0.120916, acc.: 96.88%] [G loss: 2.857019]\n",
      "4561 [D loss: 0.123147, acc.: 96.88%] [G loss: 2.844416]\n",
      "4562 [D loss: 0.122814, acc.: 96.88%] [G loss: 3.060258]\n",
      "4563 [D loss: 0.117151, acc.: 96.88%] [G loss: 2.829910]\n",
      "4564 [D loss: 0.119750, acc.: 96.88%] [G loss: 2.924387]\n",
      "4565 [D loss: 0.125916, acc.: 96.88%] [G loss: 2.895843]\n",
      "4566 [D loss: 0.124366, acc.: 96.88%] [G loss: 2.891635]\n",
      "4567 [D loss: 0.120031, acc.: 96.88%] [G loss: 2.828018]\n",
      "4568 [D loss: 0.120153, acc.: 96.88%] [G loss: 2.815112]\n",
      "4569 [D loss: 0.122858, acc.: 96.88%] [G loss: 2.905184]\n",
      "4570 [D loss: 0.115473, acc.: 96.88%] [G loss: 2.998816]\n",
      "4571 [D loss: 0.116169, acc.: 96.88%] [G loss: 2.898724]\n",
      "4572 [D loss: 0.118224, acc.: 96.88%] [G loss: 2.978001]\n",
      "4573 [D loss: 0.115601, acc.: 96.88%] [G loss: 3.083293]\n",
      "4574 [D loss: 0.114578, acc.: 96.88%] [G loss: 3.120401]\n",
      "4575 [D loss: 0.128066, acc.: 96.88%] [G loss: 2.936002]\n",
      "4576 [D loss: 0.121827, acc.: 96.88%] [G loss: 2.877234]\n",
      "4577 [D loss: 0.118643, acc.: 96.88%] [G loss: 2.833259]\n",
      "4578 [D loss: 0.118940, acc.: 96.88%] [G loss: 2.923343]\n",
      "4579 [D loss: 0.118748, acc.: 96.88%] [G loss: 2.978567]\n",
      "4580 [D loss: 0.117355, acc.: 96.88%] [G loss: 2.865990]\n",
      "4581 [D loss: 0.120722, acc.: 96.88%] [G loss: 2.940473]\n",
      "4582 [D loss: 0.123125, acc.: 96.88%] [G loss: 2.887179]\n",
      "4583 [D loss: 0.124210, acc.: 96.88%] [G loss: 2.893896]\n",
      "4584 [D loss: 0.116803, acc.: 96.88%] [G loss: 2.924691]\n",
      "4585 [D loss: 0.116048, acc.: 96.88%] [G loss: 2.923366]\n",
      "4586 [D loss: 0.119546, acc.: 96.88%] [G loss: 2.967822]\n",
      "4587 [D loss: 0.122286, acc.: 96.88%] [G loss: 2.877143]\n",
      "4588 [D loss: 0.121744, acc.: 96.88%] [G loss: 2.843658]\n",
      "4589 [D loss: 0.119826, acc.: 96.88%] [G loss: 3.029495]\n",
      "4590 [D loss: 0.115421, acc.: 96.88%] [G loss: 2.937814]\n",
      "4591 [D loss: 0.121797, acc.: 96.88%] [G loss: 2.938845]\n",
      "4592 [D loss: 0.115026, acc.: 96.88%] [G loss: 2.891564]\n",
      "4593 [D loss: 0.126430, acc.: 96.88%] [G loss: 2.963634]\n",
      "4594 [D loss: 0.127139, acc.: 96.88%] [G loss: 2.945059]\n",
      "4595 [D loss: 0.122539, acc.: 96.88%] [G loss: 2.911689]\n",
      "4596 [D loss: 0.119881, acc.: 96.88%] [G loss: 2.864493]\n",
      "4597 [D loss: 0.122201, acc.: 96.88%] [G loss: 2.921624]\n",
      "4598 [D loss: 0.122239, acc.: 96.88%] [G loss: 2.866270]\n",
      "4599 [D loss: 0.122135, acc.: 96.88%] [G loss: 2.846237]\n",
      "4600 [D loss: 0.118711, acc.: 96.88%] [G loss: 2.765978]\n",
      "generated_data\n",
      "4601 [D loss: 0.120397, acc.: 96.88%] [G loss: 2.964205]\n",
      "4602 [D loss: 0.124681, acc.: 96.88%] [G loss: 2.873814]\n",
      "4603 [D loss: 0.126374, acc.: 96.88%] [G loss: 2.851472]\n",
      "4604 [D loss: 0.122996, acc.: 96.88%] [G loss: 2.871654]\n",
      "4605 [D loss: 0.123695, acc.: 96.88%] [G loss: 2.973753]\n",
      "4606 [D loss: 0.119611, acc.: 96.88%] [G loss: 2.893160]\n",
      "4607 [D loss: 0.124170, acc.: 96.88%] [G loss: 2.933639]\n",
      "4608 [D loss: 0.116168, acc.: 96.88%] [G loss: 2.963008]\n",
      "4609 [D loss: 0.125822, acc.: 96.88%] [G loss: 2.877396]\n",
      "4610 [D loss: 0.126324, acc.: 96.88%] [G loss: 2.896979]\n",
      "4611 [D loss: 0.118513, acc.: 96.88%] [G loss: 2.924265]\n",
      "4612 [D loss: 0.118701, acc.: 96.88%] [G loss: 2.892890]\n",
      "4613 [D loss: 0.119403, acc.: 96.88%] [G loss: 3.099874]\n",
      "4614 [D loss: 0.120810, acc.: 96.88%] [G loss: 2.956581]\n",
      "4615 [D loss: 0.118364, acc.: 96.88%] [G loss: 2.992525]\n",
      "4616 [D loss: 0.125540, acc.: 96.88%] [G loss: 2.933022]\n",
      "4617 [D loss: 0.122524, acc.: 96.88%] [G loss: 2.926461]\n",
      "4618 [D loss: 0.112767, acc.: 96.88%] [G loss: 2.966634]\n",
      "4619 [D loss: 0.119173, acc.: 96.88%] [G loss: 2.913726]\n",
      "4620 [D loss: 0.126256, acc.: 96.88%] [G loss: 2.914663]\n",
      "4621 [D loss: 0.111609, acc.: 96.88%] [G loss: 2.956701]\n",
      "4622 [D loss: 0.119553, acc.: 96.88%] [G loss: 2.888503]\n",
      "4623 [D loss: 0.121410, acc.: 96.88%] [G loss: 2.948923]\n",
      "4624 [D loss: 0.121332, acc.: 96.88%] [G loss: 2.911172]\n",
      "4625 [D loss: 0.125203, acc.: 96.88%] [G loss: 2.904869]\n",
      "4626 [D loss: 0.116174, acc.: 96.88%] [G loss: 2.816687]\n",
      "4627 [D loss: 0.120429, acc.: 96.88%] [G loss: 2.879795]\n",
      "4628 [D loss: 0.121499, acc.: 96.88%] [G loss: 2.927016]\n",
      "4629 [D loss: 0.121621, acc.: 96.88%] [G loss: 3.011812]\n",
      "4630 [D loss: 0.117912, acc.: 96.88%] [G loss: 2.980008]\n",
      "4631 [D loss: 0.128344, acc.: 96.88%] [G loss: 2.936863]\n",
      "4632 [D loss: 0.126024, acc.: 96.88%] [G loss: 2.820232]\n",
      "4633 [D loss: 0.119416, acc.: 96.88%] [G loss: 2.936960]\n",
      "4634 [D loss: 0.121833, acc.: 96.88%] [G loss: 2.987869]\n",
      "4635 [D loss: 0.125978, acc.: 96.88%] [G loss: 2.838812]\n",
      "4636 [D loss: 0.121510, acc.: 96.88%] [G loss: 3.088571]\n",
      "4637 [D loss: 0.115678, acc.: 96.88%] [G loss: 2.944744]\n",
      "4638 [D loss: 0.125619, acc.: 96.88%] [G loss: 2.941412]\n",
      "4639 [D loss: 0.121166, acc.: 96.88%] [G loss: 2.877045]\n",
      "4640 [D loss: 0.123059, acc.: 96.88%] [G loss: 2.914807]\n",
      "4641 [D loss: 0.121034, acc.: 96.88%] [G loss: 2.870667]\n",
      "4642 [D loss: 0.121555, acc.: 96.88%] [G loss: 2.874648]\n",
      "4643 [D loss: 0.122030, acc.: 96.88%] [G loss: 2.889465]\n",
      "4644 [D loss: 0.117757, acc.: 96.88%] [G loss: 2.911706]\n",
      "4645 [D loss: 0.122129, acc.: 96.88%] [G loss: 2.853171]\n",
      "4646 [D loss: 0.120793, acc.: 96.88%] [G loss: 3.055482]\n",
      "4647 [D loss: 0.120264, acc.: 96.88%] [G loss: 2.950240]\n",
      "4648 [D loss: 0.124643, acc.: 96.88%] [G loss: 2.996880]\n",
      "4649 [D loss: 0.119535, acc.: 96.88%] [G loss: 2.908705]\n",
      "4650 [D loss: 0.120577, acc.: 96.88%] [G loss: 2.868694]\n",
      "4651 [D loss: 0.127192, acc.: 96.88%] [G loss: 2.897247]\n",
      "4652 [D loss: 0.116713, acc.: 96.88%] [G loss: 2.975190]\n",
      "4653 [D loss: 0.124949, acc.: 96.88%] [G loss: 3.009641]\n",
      "4654 [D loss: 0.121834, acc.: 96.88%] [G loss: 3.048575]\n",
      "4655 [D loss: 0.115967, acc.: 96.88%] [G loss: 3.016589]\n",
      "4656 [D loss: 0.123658, acc.: 96.88%] [G loss: 2.947683]\n",
      "4657 [D loss: 0.123894, acc.: 96.88%] [G loss: 2.983174]\n",
      "4658 [D loss: 0.115049, acc.: 96.88%] [G loss: 3.102721]\n",
      "4659 [D loss: 0.123362, acc.: 96.88%] [G loss: 2.907101]\n",
      "4660 [D loss: 0.119717, acc.: 96.88%] [G loss: 2.987286]\n",
      "4661 [D loss: 0.114980, acc.: 96.88%] [G loss: 3.067035]\n",
      "4662 [D loss: 0.117292, acc.: 96.88%] [G loss: 2.943504]\n",
      "4663 [D loss: 0.123882, acc.: 96.88%] [G loss: 2.775240]\n",
      "4664 [D loss: 0.120926, acc.: 96.88%] [G loss: 2.993812]\n",
      "4665 [D loss: 0.121120, acc.: 96.88%] [G loss: 2.912093]\n",
      "4666 [D loss: 0.113333, acc.: 96.88%] [G loss: 2.975993]\n",
      "4667 [D loss: 0.117006, acc.: 96.88%] [G loss: 3.050178]\n",
      "4668 [D loss: 0.119499, acc.: 96.88%] [G loss: 2.937555]\n",
      "4669 [D loss: 0.120599, acc.: 96.88%] [G loss: 2.844088]\n",
      "4670 [D loss: 0.123271, acc.: 96.88%] [G loss: 2.906749]\n",
      "4671 [D loss: 0.119085, acc.: 96.88%] [G loss: 2.948131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672 [D loss: 0.117468, acc.: 96.88%] [G loss: 2.902859]\n",
      "4673 [D loss: 0.113891, acc.: 96.88%] [G loss: 2.915250]\n",
      "4674 [D loss: 0.122118, acc.: 96.88%] [G loss: 2.947284]\n",
      "4675 [D loss: 0.117321, acc.: 96.88%] [G loss: 3.017076]\n",
      "4676 [D loss: 0.118203, acc.: 96.88%] [G loss: 2.943895]\n",
      "4677 [D loss: 0.120764, acc.: 96.88%] [G loss: 2.977435]\n",
      "4678 [D loss: 0.121425, acc.: 96.88%] [G loss: 2.925507]\n",
      "4679 [D loss: 0.118781, acc.: 96.88%] [G loss: 2.966102]\n",
      "4680 [D loss: 0.116626, acc.: 96.88%] [G loss: 2.877939]\n",
      "4681 [D loss: 0.119392, acc.: 96.88%] [G loss: 2.867357]\n",
      "4682 [D loss: 0.118629, acc.: 96.88%] [G loss: 2.891912]\n",
      "4683 [D loss: 0.123353, acc.: 96.88%] [G loss: 2.878712]\n",
      "4684 [D loss: 0.119277, acc.: 96.88%] [G loss: 3.008096]\n",
      "4685 [D loss: 0.123954, acc.: 96.88%] [G loss: 2.826265]\n",
      "4686 [D loss: 0.122756, acc.: 96.88%] [G loss: 2.888152]\n",
      "4687 [D loss: 0.117073, acc.: 96.88%] [G loss: 2.899063]\n",
      "4688 [D loss: 0.121458, acc.: 96.88%] [G loss: 2.926732]\n",
      "4689 [D loss: 0.124120, acc.: 96.88%] [G loss: 2.939675]\n",
      "4690 [D loss: 0.118958, acc.: 96.88%] [G loss: 2.794877]\n",
      "4691 [D loss: 0.120565, acc.: 96.88%] [G loss: 2.879333]\n",
      "4692 [D loss: 0.123351, acc.: 96.88%] [G loss: 2.841986]\n",
      "4693 [D loss: 0.122317, acc.: 96.88%] [G loss: 2.887065]\n",
      "4694 [D loss: 0.119790, acc.: 96.88%] [G loss: 2.814457]\n",
      "4695 [D loss: 0.119300, acc.: 96.88%] [G loss: 2.868589]\n",
      "4696 [D loss: 0.117038, acc.: 96.88%] [G loss: 2.984387]\n",
      "4697 [D loss: 0.117403, acc.: 96.88%] [G loss: 2.860138]\n",
      "4698 [D loss: 0.115735, acc.: 96.88%] [G loss: 2.959588]\n",
      "4699 [D loss: 0.117332, acc.: 96.88%] [G loss: 2.944852]\n",
      "4700 [D loss: 0.125178, acc.: 96.88%] [G loss: 2.905082]\n",
      "generated_data\n",
      "4701 [D loss: 0.123269, acc.: 96.88%] [G loss: 2.989066]\n",
      "4702 [D loss: 0.119453, acc.: 96.88%] [G loss: 2.820900]\n",
      "4703 [D loss: 0.125752, acc.: 96.88%] [G loss: 2.796053]\n",
      "4704 [D loss: 0.118261, acc.: 96.88%] [G loss: 2.872513]\n",
      "4705 [D loss: 0.115222, acc.: 96.88%] [G loss: 2.939079]\n",
      "4706 [D loss: 0.120229, acc.: 96.88%] [G loss: 2.938674]\n",
      "4707 [D loss: 0.119325, acc.: 96.88%] [G loss: 2.884074]\n",
      "4708 [D loss: 0.125656, acc.: 96.88%] [G loss: 2.982766]\n",
      "4709 [D loss: 0.114051, acc.: 96.88%] [G loss: 2.957431]\n",
      "4710 [D loss: 0.118941, acc.: 96.88%] [G loss: 2.893415]\n",
      "4711 [D loss: 0.121050, acc.: 96.88%] [G loss: 2.946345]\n",
      "4712 [D loss: 0.118310, acc.: 96.88%] [G loss: 2.948039]\n",
      "4713 [D loss: 0.121527, acc.: 96.88%] [G loss: 2.884336]\n",
      "4714 [D loss: 0.123401, acc.: 96.88%] [G loss: 2.976375]\n",
      "4715 [D loss: 0.125916, acc.: 96.88%] [G loss: 2.942418]\n",
      "4716 [D loss: 0.121766, acc.: 96.88%] [G loss: 2.920681]\n",
      "4717 [D loss: 0.118813, acc.: 96.88%] [G loss: 2.904446]\n",
      "4718 [D loss: 0.123827, acc.: 96.88%] [G loss: 2.857379]\n",
      "4719 [D loss: 0.123006, acc.: 96.88%] [G loss: 2.839297]\n",
      "4720 [D loss: 0.121554, acc.: 96.88%] [G loss: 2.914751]\n",
      "4721 [D loss: 0.128007, acc.: 96.88%] [G loss: 2.865629]\n",
      "4722 [D loss: 0.119222, acc.: 96.88%] [G loss: 2.902373]\n",
      "4723 [D loss: 0.120495, acc.: 96.88%] [G loss: 2.903709]\n",
      "4724 [D loss: 0.119706, acc.: 96.88%] [G loss: 2.849626]\n",
      "4725 [D loss: 0.123619, acc.: 96.88%] [G loss: 2.943100]\n",
      "4726 [D loss: 0.121593, acc.: 96.88%] [G loss: 2.856740]\n",
      "4727 [D loss: 0.126743, acc.: 96.88%] [G loss: 2.869694]\n",
      "4728 [D loss: 0.120019, acc.: 96.88%] [G loss: 2.869871]\n",
      "4729 [D loss: 0.124968, acc.: 96.88%] [G loss: 2.847199]\n",
      "4730 [D loss: 0.113271, acc.: 96.88%] [G loss: 2.893675]\n",
      "4731 [D loss: 0.123646, acc.: 96.88%] [G loss: 2.953386]\n",
      "4732 [D loss: 0.113834, acc.: 96.88%] [G loss: 3.027586]\n",
      "4733 [D loss: 0.126104, acc.: 96.88%] [G loss: 2.852672]\n",
      "4734 [D loss: 0.123076, acc.: 96.88%] [G loss: 2.843677]\n",
      "4735 [D loss: 0.121591, acc.: 96.88%] [G loss: 3.026316]\n",
      "4736 [D loss: 0.118336, acc.: 96.88%] [G loss: 2.944844]\n",
      "4737 [D loss: 0.115093, acc.: 96.88%] [G loss: 2.936875]\n",
      "4738 [D loss: 0.118202, acc.: 96.88%] [G loss: 2.822517]\n",
      "4739 [D loss: 0.122345, acc.: 96.88%] [G loss: 2.861142]\n",
      "4740 [D loss: 0.124491, acc.: 96.88%] [G loss: 2.962781]\n",
      "4741 [D loss: 0.122608, acc.: 96.88%] [G loss: 2.962697]\n",
      "4742 [D loss: 0.124666, acc.: 96.88%] [G loss: 2.877825]\n",
      "4743 [D loss: 0.120904, acc.: 96.88%] [G loss: 2.853825]\n",
      "4744 [D loss: 0.120167, acc.: 96.88%] [G loss: 2.893149]\n",
      "4745 [D loss: 0.121307, acc.: 96.88%] [G loss: 2.944767]\n",
      "4746 [D loss: 0.117752, acc.: 96.88%] [G loss: 2.989267]\n",
      "4747 [D loss: 0.117801, acc.: 96.88%] [G loss: 2.924390]\n",
      "4748 [D loss: 0.120407, acc.: 96.88%] [G loss: 2.835512]\n",
      "4749 [D loss: 0.116747, acc.: 96.88%] [G loss: 2.950427]\n",
      "4750 [D loss: 0.122026, acc.: 96.88%] [G loss: 2.988052]\n",
      "4751 [D loss: 0.118996, acc.: 96.88%] [G loss: 2.881661]\n",
      "4752 [D loss: 0.114935, acc.: 96.88%] [G loss: 2.881087]\n",
      "4753 [D loss: 0.122064, acc.: 96.88%] [G loss: 3.010558]\n",
      "4754 [D loss: 0.120011, acc.: 96.88%] [G loss: 3.074979]\n",
      "4755 [D loss: 0.122141, acc.: 96.88%] [G loss: 2.963248]\n",
      "4756 [D loss: 0.118653, acc.: 96.88%] [G loss: 3.001412]\n",
      "4757 [D loss: 0.122298, acc.: 96.88%] [G loss: 3.005849]\n",
      "4758 [D loss: 0.125171, acc.: 96.88%] [G loss: 2.912110]\n",
      "4759 [D loss: 0.119382, acc.: 96.88%] [G loss: 2.924722]\n",
      "4760 [D loss: 0.116302, acc.: 96.88%] [G loss: 2.897274]\n",
      "4761 [D loss: 0.122659, acc.: 96.88%] [G loss: 2.908412]\n",
      "4762 [D loss: 0.118083, acc.: 96.88%] [G loss: 2.948377]\n",
      "4763 [D loss: 0.115623, acc.: 96.88%] [G loss: 3.069547]\n",
      "4764 [D loss: 0.123861, acc.: 96.88%] [G loss: 3.003399]\n",
      "4765 [D loss: 0.117973, acc.: 96.88%] [G loss: 2.952022]\n",
      "4766 [D loss: 0.122768, acc.: 96.88%] [G loss: 2.986076]\n",
      "4767 [D loss: 0.118224, acc.: 96.88%] [G loss: 2.920078]\n",
      "4768 [D loss: 0.115831, acc.: 96.88%] [G loss: 3.225032]\n",
      "4769 [D loss: 0.124195, acc.: 96.88%] [G loss: 2.999200]\n",
      "4770 [D loss: 0.115553, acc.: 96.88%] [G loss: 3.116029]\n",
      "4771 [D loss: 0.121590, acc.: 96.88%] [G loss: 3.106113]\n",
      "4772 [D loss: 0.118973, acc.: 96.88%] [G loss: 3.121749]\n",
      "4773 [D loss: 0.123347, acc.: 96.88%] [G loss: 2.954705]\n",
      "4774 [D loss: 0.121211, acc.: 96.88%] [G loss: 2.845158]\n",
      "4775 [D loss: 0.116669, acc.: 96.88%] [G loss: 3.018500]\n",
      "4776 [D loss: 0.119509, acc.: 96.88%] [G loss: 3.079881]\n",
      "4777 [D loss: 0.115627, acc.: 96.88%] [G loss: 3.117376]\n",
      "4778 [D loss: 0.119388, acc.: 96.88%] [G loss: 3.054956]\n",
      "4779 [D loss: 0.114727, acc.: 96.88%] [G loss: 2.961766]\n",
      "4780 [D loss: 0.123084, acc.: 96.88%] [G loss: 2.977461]\n",
      "4781 [D loss: 0.124308, acc.: 96.88%] [G loss: 3.046896]\n",
      "4782 [D loss: 0.117290, acc.: 96.88%] [G loss: 2.853629]\n",
      "4783 [D loss: 0.117807, acc.: 96.88%] [G loss: 2.894342]\n",
      "4784 [D loss: 0.118017, acc.: 96.88%] [G loss: 3.031837]\n",
      "4785 [D loss: 0.119326, acc.: 96.88%] [G loss: 2.925572]\n",
      "4786 [D loss: 0.125502, acc.: 96.88%] [G loss: 2.934787]\n",
      "4787 [D loss: 0.124040, acc.: 96.88%] [G loss: 2.903358]\n",
      "4788 [D loss: 0.120423, acc.: 96.88%] [G loss: 2.904256]\n",
      "4789 [D loss: 0.119427, acc.: 96.88%] [G loss: 2.826264]\n",
      "4790 [D loss: 0.118376, acc.: 96.88%] [G loss: 2.878928]\n",
      "4791 [D loss: 0.119292, acc.: 96.88%] [G loss: 3.061139]\n",
      "4792 [D loss: 0.120024, acc.: 96.88%] [G loss: 2.946494]\n",
      "4793 [D loss: 0.112977, acc.: 96.88%] [G loss: 3.114913]\n",
      "4794 [D loss: 0.117699, acc.: 96.88%] [G loss: 3.062320]\n",
      "4795 [D loss: 0.115777, acc.: 96.88%] [G loss: 2.913112]\n",
      "4796 [D loss: 0.115085, acc.: 96.88%] [G loss: 3.291144]\n",
      "4797 [D loss: 0.118375, acc.: 96.88%] [G loss: 2.854849]\n",
      "4798 [D loss: 0.118395, acc.: 96.88%] [G loss: 2.970796]\n",
      "4799 [D loss: 0.110675, acc.: 96.88%] [G loss: 3.099970]\n",
      "4800 [D loss: 0.122339, acc.: 96.88%] [G loss: 3.010564]\n",
      "generated_data\n",
      "4801 [D loss: 0.118904, acc.: 96.88%] [G loss: 2.787207]\n",
      "4802 [D loss: 0.116278, acc.: 96.88%] [G loss: 3.064621]\n",
      "4803 [D loss: 0.115038, acc.: 96.88%] [G loss: 2.968552]\n",
      "4804 [D loss: 0.120297, acc.: 96.88%] [G loss: 2.916784]\n",
      "4805 [D loss: 0.119121, acc.: 96.88%] [G loss: 2.979198]\n",
      "4806 [D loss: 0.120536, acc.: 96.88%] [G loss: 3.070675]\n",
      "4807 [D loss: 0.112820, acc.: 96.88%] [G loss: 2.942485]\n",
      "4808 [D loss: 0.117120, acc.: 96.88%] [G loss: 3.018230]\n",
      "4809 [D loss: 0.127318, acc.: 96.88%] [G loss: 2.816210]\n",
      "4810 [D loss: 0.119981, acc.: 96.88%] [G loss: 3.100384]\n",
      "4811 [D loss: 0.119024, acc.: 96.88%] [G loss: 2.917911]\n",
      "4812 [D loss: 0.120356, acc.: 96.88%] [G loss: 2.878900]\n",
      "4813 [D loss: 0.119929, acc.: 96.88%] [G loss: 3.014127]\n",
      "4814 [D loss: 0.116440, acc.: 96.88%] [G loss: 3.319608]\n",
      "4815 [D loss: 0.121016, acc.: 96.88%] [G loss: 2.909721]\n",
      "4816 [D loss: 0.118576, acc.: 96.88%] [G loss: 2.907431]\n",
      "4817 [D loss: 0.114144, acc.: 96.88%] [G loss: 2.953579]\n",
      "4818 [D loss: 0.115019, acc.: 96.88%] [G loss: 2.904971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4819 [D loss: 0.125135, acc.: 96.88%] [G loss: 2.988538]\n",
      "4820 [D loss: 0.120090, acc.: 96.88%] [G loss: 2.922822]\n",
      "4821 [D loss: 0.120975, acc.: 96.88%] [G loss: 3.052103]\n",
      "4822 [D loss: 0.120868, acc.: 96.88%] [G loss: 2.958972]\n",
      "4823 [D loss: 0.119958, acc.: 96.88%] [G loss: 2.801458]\n",
      "4824 [D loss: 0.118163, acc.: 96.88%] [G loss: 3.077306]\n",
      "4825 [D loss: 0.121904, acc.: 96.88%] [G loss: 2.957620]\n",
      "4826 [D loss: 0.116303, acc.: 96.88%] [G loss: 3.040063]\n",
      "4827 [D loss: 0.114594, acc.: 96.88%] [G loss: 2.950218]\n",
      "4828 [D loss: 0.117248, acc.: 96.88%] [G loss: 2.998423]\n",
      "4829 [D loss: 0.122823, acc.: 96.88%] [G loss: 2.947429]\n",
      "4830 [D loss: 0.113417, acc.: 96.88%] [G loss: 2.982575]\n",
      "4831 [D loss: 0.122380, acc.: 96.88%] [G loss: 2.965221]\n",
      "4832 [D loss: 0.117682, acc.: 96.88%] [G loss: 2.889663]\n",
      "4833 [D loss: 0.123178, acc.: 96.88%] [G loss: 3.079465]\n",
      "4834 [D loss: 0.123967, acc.: 96.88%] [G loss: 2.988585]\n",
      "4835 [D loss: 0.116316, acc.: 96.88%] [G loss: 3.015248]\n",
      "4836 [D loss: 0.117338, acc.: 96.88%] [G loss: 2.916144]\n",
      "4837 [D loss: 0.117552, acc.: 96.88%] [G loss: 2.896838]\n",
      "4838 [D loss: 0.119561, acc.: 96.88%] [G loss: 3.021684]\n",
      "4839 [D loss: 0.109822, acc.: 96.88%] [G loss: 3.193342]\n",
      "4840 [D loss: 0.113513, acc.: 96.88%] [G loss: 3.297632]\n",
      "4841 [D loss: 0.112218, acc.: 96.88%] [G loss: 3.203918]\n",
      "4842 [D loss: 0.111737, acc.: 96.88%] [G loss: 2.994838]\n",
      "4843 [D loss: 0.117241, acc.: 96.88%] [G loss: 2.915485]\n",
      "4844 [D loss: 0.113400, acc.: 96.88%] [G loss: 3.157026]\n",
      "4845 [D loss: 0.122619, acc.: 96.88%] [G loss: 3.016228]\n",
      "4846 [D loss: 0.110440, acc.: 96.88%] [G loss: 2.939196]\n",
      "4847 [D loss: 0.121159, acc.: 96.88%] [G loss: 3.040706]\n",
      "4848 [D loss: 0.120267, acc.: 96.88%] [G loss: 2.722074]\n",
      "4849 [D loss: 0.119535, acc.: 96.88%] [G loss: 2.949469]\n",
      "4850 [D loss: 0.118651, acc.: 96.88%] [G loss: 2.815416]\n",
      "4851 [D loss: 0.115250, acc.: 96.88%] [G loss: 2.976121]\n",
      "4852 [D loss: 0.125104, acc.: 96.88%] [G loss: 2.858198]\n",
      "4853 [D loss: 0.120331, acc.: 96.88%] [G loss: 2.927099]\n",
      "4854 [D loss: 0.118822, acc.: 96.88%] [G loss: 2.908222]\n",
      "4855 [D loss: 0.124200, acc.: 96.88%] [G loss: 2.901100]\n",
      "4856 [D loss: 0.117427, acc.: 96.88%] [G loss: 3.072274]\n",
      "4857 [D loss: 0.125914, acc.: 96.88%] [G loss: 2.929586]\n",
      "4858 [D loss: 0.113339, acc.: 96.88%] [G loss: 3.130043]\n",
      "4859 [D loss: 0.121215, acc.: 96.88%] [G loss: 2.992365]\n",
      "4860 [D loss: 0.124858, acc.: 96.88%] [G loss: 2.913010]\n",
      "4861 [D loss: 0.122097, acc.: 96.88%] [G loss: 2.976118]\n",
      "4862 [D loss: 0.123436, acc.: 96.88%] [G loss: 3.096807]\n",
      "4863 [D loss: 0.114601, acc.: 96.88%] [G loss: 3.016253]\n",
      "4864 [D loss: 0.120139, acc.: 96.88%] [G loss: 3.044785]\n",
      "4865 [D loss: 0.117947, acc.: 96.88%] [G loss: 2.926617]\n",
      "4866 [D loss: 0.114685, acc.: 96.88%] [G loss: 2.894921]\n",
      "4867 [D loss: 0.129960, acc.: 96.88%] [G loss: 2.761344]\n",
      "4868 [D loss: 0.124242, acc.: 96.88%] [G loss: 3.115135]\n",
      "4869 [D loss: 0.114653, acc.: 96.88%] [G loss: 3.093514]\n",
      "4870 [D loss: 0.122419, acc.: 96.88%] [G loss: 2.967348]\n",
      "4871 [D loss: 0.112892, acc.: 96.88%] [G loss: 3.226684]\n",
      "4872 [D loss: 0.113485, acc.: 96.88%] [G loss: 3.204174]\n",
      "4873 [D loss: 0.119710, acc.: 96.88%] [G loss: 3.129271]\n",
      "4874 [D loss: 0.113264, acc.: 96.88%] [G loss: 3.150183]\n",
      "4875 [D loss: 0.126460, acc.: 96.88%] [G loss: 2.946864]\n",
      "4876 [D loss: 0.115011, acc.: 96.88%] [G loss: 3.110811]\n",
      "4877 [D loss: 0.122182, acc.: 96.88%] [G loss: 3.072826]\n",
      "4878 [D loss: 0.104999, acc.: 96.88%] [G loss: 3.329292]\n",
      "4879 [D loss: 0.114466, acc.: 96.88%] [G loss: 3.151489]\n",
      "4880 [D loss: 0.120415, acc.: 96.88%] [G loss: 2.812739]\n",
      "4881 [D loss: 0.117879, acc.: 96.88%] [G loss: 3.041394]\n",
      "4882 [D loss: 0.119526, acc.: 96.88%] [G loss: 2.904515]\n",
      "4883 [D loss: 0.112664, acc.: 96.88%] [G loss: 3.040870]\n",
      "4884 [D loss: 0.115933, acc.: 96.88%] [G loss: 3.051373]\n",
      "4885 [D loss: 0.114013, acc.: 96.88%] [G loss: 2.882599]\n",
      "4886 [D loss: 0.125596, acc.: 96.88%] [G loss: 2.826060]\n",
      "4887 [D loss: 0.119098, acc.: 96.88%] [G loss: 3.008759]\n",
      "4888 [D loss: 0.118087, acc.: 96.88%] [G loss: 2.914270]\n",
      "4889 [D loss: 0.126047, acc.: 96.88%] [G loss: 3.035256]\n",
      "4890 [D loss: 0.118337, acc.: 96.88%] [G loss: 3.131323]\n",
      "4891 [D loss: 0.118424, acc.: 96.88%] [G loss: 2.871749]\n",
      "4892 [D loss: 0.118974, acc.: 96.88%] [G loss: 3.111169]\n",
      "4893 [D loss: 0.118752, acc.: 96.88%] [G loss: 3.022047]\n",
      "4894 [D loss: 0.119890, acc.: 96.88%] [G loss: 2.973691]\n",
      "4895 [D loss: 0.113680, acc.: 96.88%] [G loss: 3.136484]\n",
      "4896 [D loss: 0.110629, acc.: 96.88%] [G loss: 3.273949]\n",
      "4897 [D loss: 0.120903, acc.: 96.88%] [G loss: 2.896229]\n",
      "4898 [D loss: 0.117116, acc.: 96.88%] [G loss: 3.151947]\n",
      "4899 [D loss: 0.117882, acc.: 96.88%] [G loss: 3.078589]\n",
      "4900 [D loss: 0.118358, acc.: 96.88%] [G loss: 3.077000]\n",
      "generated_data\n",
      "4901 [D loss: 0.108323, acc.: 96.88%] [G loss: 3.090865]\n",
      "4902 [D loss: 0.119853, acc.: 96.88%] [G loss: 2.986565]\n",
      "4903 [D loss: 0.113847, acc.: 96.88%] [G loss: 3.332038]\n",
      "4904 [D loss: 0.120678, acc.: 96.88%] [G loss: 3.051361]\n",
      "4905 [D loss: 0.110102, acc.: 96.88%] [G loss: 3.265537]\n",
      "4906 [D loss: 0.110127, acc.: 96.88%] [G loss: 3.189925]\n",
      "4907 [D loss: 0.118057, acc.: 96.88%] [G loss: 2.969954]\n",
      "4908 [D loss: 0.120719, acc.: 96.88%] [G loss: 3.168275]\n",
      "4909 [D loss: 0.122132, acc.: 96.88%] [G loss: 3.498663]\n",
      "4910 [D loss: 0.116176, acc.: 96.88%] [G loss: 2.941904]\n",
      "4911 [D loss: 0.119462, acc.: 96.88%] [G loss: 3.229926]\n",
      "4912 [D loss: 0.111147, acc.: 96.88%] [G loss: 3.171134]\n",
      "4913 [D loss: 0.124020, acc.: 96.88%] [G loss: 2.779490]\n",
      "4914 [D loss: 0.125433, acc.: 96.88%] [G loss: 2.954121]\n",
      "4915 [D loss: 0.109072, acc.: 96.88%] [G loss: 3.695643]\n",
      "4916 [D loss: 0.134385, acc.: 96.88%] [G loss: 2.993135]\n",
      "4917 [D loss: 0.117268, acc.: 96.88%] [G loss: 3.103250]\n",
      "4918 [D loss: 0.118465, acc.: 96.88%] [G loss: 3.049111]\n",
      "4919 [D loss: 0.120044, acc.: 96.88%] [G loss: 2.783132]\n",
      "4920 [D loss: 0.116826, acc.: 96.88%] [G loss: 2.923719]\n",
      "4921 [D loss: 0.121202, acc.: 96.88%] [G loss: 3.047235]\n",
      "4922 [D loss: 0.116796, acc.: 96.88%] [G loss: 3.461756]\n",
      "4923 [D loss: 0.126356, acc.: 96.88%] [G loss: 3.349257]\n",
      "4924 [D loss: 0.111697, acc.: 96.88%] [G loss: 3.278165]\n",
      "4925 [D loss: 0.109576, acc.: 96.88%] [G loss: 3.222079]\n",
      "4926 [D loss: 0.126446, acc.: 96.88%] [G loss: 2.961941]\n",
      "4927 [D loss: 0.123511, acc.: 96.88%] [G loss: 2.861848]\n",
      "4928 [D loss: 0.113318, acc.: 96.88%] [G loss: 3.295007]\n",
      "4929 [D loss: 0.116595, acc.: 96.88%] [G loss: 3.109793]\n",
      "4930 [D loss: 0.119818, acc.: 96.88%] [G loss: 3.153872]\n",
      "4931 [D loss: 0.114731, acc.: 96.88%] [G loss: 3.014791]\n",
      "4932 [D loss: 0.116755, acc.: 96.88%] [G loss: 2.908518]\n",
      "4933 [D loss: 0.124275, acc.: 96.88%] [G loss: 2.866733]\n",
      "4934 [D loss: 0.121805, acc.: 96.88%] [G loss: 2.989702]\n",
      "4935 [D loss: 0.119799, acc.: 96.88%] [G loss: 2.903318]\n",
      "4936 [D loss: 0.120739, acc.: 96.88%] [G loss: 2.948454]\n",
      "4937 [D loss: 0.121592, acc.: 96.88%] [G loss: 2.946620]\n",
      "4938 [D loss: 0.113944, acc.: 96.88%] [G loss: 2.984967]\n",
      "4939 [D loss: 0.116148, acc.: 96.88%] [G loss: 3.181478]\n",
      "4940 [D loss: 0.120280, acc.: 96.88%] [G loss: 2.885602]\n",
      "4941 [D loss: 0.116461, acc.: 96.88%] [G loss: 3.158928]\n",
      "4942 [D loss: 0.116002, acc.: 96.88%] [G loss: 3.022147]\n",
      "4943 [D loss: 0.114690, acc.: 96.88%] [G loss: 2.983743]\n",
      "4944 [D loss: 0.121940, acc.: 96.88%] [G loss: 2.937625]\n",
      "4945 [D loss: 0.127108, acc.: 96.88%] [G loss: 2.925852]\n",
      "4946 [D loss: 0.117096, acc.: 96.88%] [G loss: 2.897614]\n",
      "4947 [D loss: 0.122187, acc.: 96.88%] [G loss: 2.961236]\n",
      "4948 [D loss: 0.116337, acc.: 96.88%] [G loss: 2.960136]\n",
      "4949 [D loss: 0.114643, acc.: 96.88%] [G loss: 2.840687]\n",
      "4950 [D loss: 0.118279, acc.: 96.88%] [G loss: 2.989937]\n",
      "4951 [D loss: 0.124527, acc.: 96.88%] [G loss: 2.905292]\n",
      "4952 [D loss: 0.116789, acc.: 96.88%] [G loss: 2.925021]\n",
      "4953 [D loss: 0.122806, acc.: 96.88%] [G loss: 2.836793]\n",
      "4954 [D loss: 0.123690, acc.: 96.88%] [G loss: 2.842405]\n",
      "4955 [D loss: 0.120863, acc.: 96.88%] [G loss: 2.848175]\n",
      "4956 [D loss: 0.115473, acc.: 96.88%] [G loss: 3.050544]\n",
      "4957 [D loss: 0.120123, acc.: 96.88%] [G loss: 2.945909]\n",
      "4958 [D loss: 0.120285, acc.: 96.88%] [G loss: 2.880226]\n",
      "4959 [D loss: 0.110338, acc.: 96.88%] [G loss: 2.967582]\n",
      "4960 [D loss: 0.120334, acc.: 96.88%] [G loss: 2.874276]\n",
      "4961 [D loss: 0.118986, acc.: 96.88%] [G loss: 3.133480]\n",
      "4962 [D loss: 0.121643, acc.: 96.88%] [G loss: 2.982814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4963 [D loss: 0.118337, acc.: 96.88%] [G loss: 2.872071]\n",
      "4964 [D loss: 0.117712, acc.: 96.88%] [G loss: 2.898411]\n",
      "4965 [D loss: 0.120070, acc.: 96.88%] [G loss: 3.088248]\n",
      "4966 [D loss: 0.125007, acc.: 96.88%] [G loss: 2.949359]\n",
      "4967 [D loss: 0.117020, acc.: 96.88%] [G loss: 2.933079]\n",
      "4968 [D loss: 0.121269, acc.: 96.88%] [G loss: 2.937835]\n",
      "4969 [D loss: 0.121432, acc.: 96.88%] [G loss: 2.891368]\n",
      "4970 [D loss: 0.117725, acc.: 96.88%] [G loss: 2.936648]\n",
      "4971 [D loss: 0.117196, acc.: 96.88%] [G loss: 3.022078]\n",
      "4972 [D loss: 0.121035, acc.: 96.88%] [G loss: 3.063306]\n",
      "4973 [D loss: 0.112803, acc.: 96.88%] [G loss: 3.060540]\n",
      "4974 [D loss: 0.115042, acc.: 96.88%] [G loss: 2.914592]\n",
      "4975 [D loss: 0.121086, acc.: 96.88%] [G loss: 3.053073]\n",
      "4976 [D loss: 0.118398, acc.: 96.88%] [G loss: 2.949723]\n",
      "4977 [D loss: 0.120439, acc.: 96.88%] [G loss: 2.964039]\n",
      "4978 [D loss: 0.119987, acc.: 96.88%] [G loss: 2.964246]\n",
      "4979 [D loss: 0.121572, acc.: 96.88%] [G loss: 2.963712]\n",
      "4980 [D loss: 0.126037, acc.: 96.88%] [G loss: 3.028396]\n",
      "4981 [D loss: 0.122561, acc.: 96.88%] [G loss: 3.041770]\n",
      "4982 [D loss: 0.117565, acc.: 96.88%] [G loss: 2.948536]\n",
      "4983 [D loss: 0.120987, acc.: 96.88%] [G loss: 3.017923]\n",
      "4984 [D loss: 0.125667, acc.: 96.88%] [G loss: 2.819567]\n",
      "4985 [D loss: 0.117888, acc.: 96.88%] [G loss: 3.102217]\n",
      "4986 [D loss: 0.114135, acc.: 96.88%] [G loss: 2.916180]\n",
      "4987 [D loss: 0.116608, acc.: 96.88%] [G loss: 3.127883]\n",
      "4988 [D loss: 0.118435, acc.: 96.88%] [G loss: 2.973865]\n",
      "4989 [D loss: 0.111800, acc.: 96.88%] [G loss: 2.958445]\n",
      "4990 [D loss: 0.122061, acc.: 96.88%] [G loss: 3.072577]\n",
      "4991 [D loss: 0.115635, acc.: 96.88%] [G loss: 3.071527]\n",
      "4992 [D loss: 0.114892, acc.: 96.88%] [G loss: 2.955218]\n",
      "4993 [D loss: 0.122621, acc.: 96.88%] [G loss: 3.036596]\n",
      "4994 [D loss: 0.113313, acc.: 96.88%] [G loss: 3.038417]\n",
      "4995 [D loss: 0.122199, acc.: 96.88%] [G loss: 2.976103]\n",
      "4996 [D loss: 0.112082, acc.: 96.88%] [G loss: 3.433943]\n",
      "4997 [D loss: 0.120340, acc.: 96.88%] [G loss: 3.242824]\n",
      "4998 [D loss: 0.118033, acc.: 96.88%] [G loss: 3.079132]\n",
      "4999 [D loss: 0.112307, acc.: 96.88%] [G loss: 3.329576]\n",
      "5000 [D loss: 0.119980, acc.: 96.88%] [G loss: 2.840901]\n",
      "generated_data\n"
     ]
    }
   ],
   "source": [
    "model = GAN\n",
    "\n",
    "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
    "synthesizer = model(gan_args)\n",
    "synthesizer.train(df, train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eeec2c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.save('model/gan/saved', 'generator_timestamps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "824f8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'GAN': ['GAN', False, synthesizer.generator]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d122a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "z = np.random.normal(size=(test_size, noise_dim))\n",
    "real = synthesizer.get_data_batch(train=df, batch_size=test_size, seed=seed)\n",
    "real_samples = pd.DataFrame(real, columns=data_cols)\n",
    "\n",
    "#Actual fraud data visualization\n",
    "model_steps = [ 0, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000]\n",
    "rows = len(model_steps)\n",
    "columns = 5\n",
    "\n",
    "axarr = [[]]*len(model_steps)\n",
    "\n",
    "for model_step_ix, model_step in enumerate(model_steps):        \n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    [model_name, with_class, generator_model] = models['GAN']\n",
    "\n",
    "    generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
    "\n",
    "\n",
    "\n",
    "    g_z = generator_model.predict(z)\n",
    "    gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "    gen_samples.to_csv('Generated_sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1547830",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_z=pw.inverse_transform(g_z)\n",
    "gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "gen_samples.to_csv('Generated_sample_inverse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8aaa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'model/'\n",
    "[model_name, with_class, generator_model] = models['GAN']\n",
    "model_steps = [ 0, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000]\n",
    "for model_step_ix, model_step in enumerate(model_steps): \n",
    "    generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
    "\n",
    "    z = np.random.normal(size=(test_size, noise_dim))\n",
    "    g_z = generator_model.predict(z)\n",
    "#g_z=pw.inverse_transform(g_z)\n",
    "gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "gen_samples.to_csv('Generated_sample1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d931c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
